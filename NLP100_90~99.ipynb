{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90.データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kftt-data-1.0/\n",
      "kftt-data-1.0/data/\n",
      "kftt-data-1.0/data/orig/\n",
      "kftt-data-1.0/data/orig/kyoto-tune.en\n",
      "kftt-data-1.0/data/orig/kyoto-dev.ja\n",
      "kftt-data-1.0/data/orig/kyoto-dev.en\n",
      "kftt-data-1.0/data/orig/kyoto-train.en\n",
      "kftt-data-1.0/data/orig/kyoto-tune.ja\n",
      "kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "kftt-data-1.0/data/orig/kyoto-test.ja\n",
      "kftt-data-1.0/data/orig/kyoto-test.en\n",
      "kftt-data-1.0/data/tok/\n",
      "kftt-data-1.0/data/tok/kyoto-tune.en\n",
      "kftt-data-1.0/data/tok/kyoto-dev.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.cln.en\n",
      "kftt-data-1.0/data/tok/kyoto-dev.en\n",
      "kftt-data-1.0/data/tok/kyoto-train.en\n",
      "kftt-data-1.0/data/tok/kyoto-tune.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.cln.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.ja\n",
      "kftt-data-1.0/data/tok/kyoto-test.ja\n",
      "kftt-data-1.0/data/tok/kyoto-test.en\n",
      "kftt-data-1.0/README.txt\n"
     ]
    }
   ],
   "source": [
    "!tar zxvf kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本 の 水墨 画 を 一変 さ せ た 。\n",
      "諱 は 「 等楊 （ とうよう ） 」 、 もしくは 「 拙宗 （ せっしゅう ） 」 と 号 し た 。\n",
      "備中 国 に 生まれ 、 京都 ・ 相国 寺 に 入 っ て から 周防 国 に 移 る 。\n"
     ]
    }
   ],
   "source": [
    "!head -3 kftt-data-1.0/data/tok/kyoto-train.cln.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-04 15:42:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-04 15:42:49 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer='space', bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='ja', target_lang='en', trainpref='kftt-data-1.0/data/tok/kyoto-train.cln', validpref='kftt-data-1.0/data/tok/kyoto-dev', testpref='kftt-data-1.0/data/tok/kyoto-test', align_suffix=None, destdir='data-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
      "2023-07-04 15:43:20 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 114288 types\n",
      "2023-07-04 15:44:15 | INFO | fairseq_cli.preprocess | [ja] kftt-data-1.0/data/tok/kyoto-train.cln.ja: 329882 sents, 6415013 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-04 15:44:15 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 114288 types\n",
      "2023-07-04 15:44:15 | INFO | fairseq_cli.preprocess | [ja] kftt-data-1.0/data/tok/kyoto-dev.ja: 1166 sents, 28010 tokens, 0.678% replaced (by <unk>)\n",
      "2023-07-04 15:44:15 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 114288 types\n",
      "2023-07-04 15:44:17 | INFO | fairseq_cli.preprocess | [ja] kftt-data-1.0/data/tok/kyoto-test.ja: 1160 sents, 29638 tokens, 0.79% replaced (by <unk>)\n",
      "2023-07-04 15:44:17 | INFO | fairseq_cli.preprocess | [en] Dictionary: 161664 types\n",
      "2023-07-04 15:44:58 | INFO | fairseq_cli.preprocess | [en] kftt-data-1.0/data/tok/kyoto-train.cln.en: 329882 sents, 6241368 tokens, 0.0% replaced (by <unk>)\n",
      "2023-07-04 15:44:58 | INFO | fairseq_cli.preprocess | [en] Dictionary: 161664 types\n",
      "2023-07-04 15:44:58 | INFO | fairseq_cli.preprocess | [en] kftt-data-1.0/data/tok/kyoto-dev.en: 1166 sents, 25475 tokens, 2.15% replaced (by <unk>)\n",
      "2023-07-04 15:44:58 | INFO | fairseq_cli.preprocess | [en] Dictionary: 161664 types\n",
      "2023-07-04 15:44:58 | INFO | fairseq_cli.preprocess | [en] kftt-data-1.0/data/tok/kyoto-test.en: 1160 sents, 27894 tokens, 1.98% replaced (by <unk>)\n",
      "2023-07-04 15:44:58 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-preprocess \\\n",
    "    --source-lang ja \\\n",
    "    --target-lang en \\\n",
    "    --trainpref kftt-data-1.0/data/tok/kyoto-train.cln \\\n",
    "    --validpref kftt-data-1.0/data/tok/kyoto-dev \\\n",
    "    --testpref kftt-data-1.0/data/tok/kyoto-test \\\n",
    "    --tokenizer space \\\n",
    "    --destdir data-bin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91.機械翻訳モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-04 15:53:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-04 15:53:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': 500, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=500, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=500, keep_interval_updates=500, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.2, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-04 15:53:34 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-04 15:53:34 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-04 15:53:37 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(114288, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(161664, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=161664, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-07-04 15:53:37 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2023-07-04 15:53:37 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2023-07-04 15:53:37 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-07-04 15:53:37 | INFO | fairseq_cli.train | num. shared model params: 268,197,888 (num. trained: 268,197,888)\n",
      "2023-07-04 15:53:37 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-07-04 15:53:37 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.ja-en.ja\n",
      "2023-07-04 15:53:38 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-bin/valid.ja-en.en\n",
      "2023-07-04 15:53:38 | INFO | fairseq.tasks.translation | data-bin valid ja-en 1166 examples\n",
      "2023-07-04 15:53:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-07-04 15:53:43 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.651 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-04 15:53:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-07-04 15:53:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-07-04 15:53:43 | INFO | fairseq_cli.train | max tokens per device = 4000 and max sentences per device = None\n",
      "2023-07-04 15:53:43 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/checkpoint_last.pt\n",
      "2023-07-04 15:53:43 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/checkpoint_last.pt\n",
      "2023-07-04 15:53:43 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-07-04 15:53:43 | INFO | fairseq.data.data_utils | loaded 329,882 examples from: data-bin/train.ja-en.ja\n",
      "2023-07-04 15:53:43 | INFO | fairseq.data.data_utils | loaded 329,882 examples from: data-bin/train.ja-en.en\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.translation | data-bin train ja-en 329882 examples\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-04 15:53:43 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-04 15:53:43 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-04 15:53:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 001:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 15:53:44 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-07-04 15:53:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2023-07-04 15:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   3%|▉                             | 60/1841 [00:11<04:57,  6.00it/s]2023-07-04 15:53:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   9%| | 174/1841 [00:29<04:22,  6.35it/s, loss=15.034, nll_loss=14.692023-07-04 15:54:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  27%|▎| 502/1841 [01:22<03:23,  6.59it/s, loss=10.088, nll_loss=8.9852023-07-04 15:55:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 15:55:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.37it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 15:55:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.028 | nll_loss 8.897 | ppl 476.66 | wps 47279.3 | wpb 1908.5 | bsz 89.2 | num_updates 500\n",
      "2023-07-04 15:55:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 500 updates\n",
      "2023-07-04 15:55:07 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_1_500.pt\n",
      "2023-07-04 15:55:17 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_1_500.pt\n",
      "2023-07-04 15:55:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_1_500.pt (epoch 1 @ 500 updates, score 10.028) (writing took 32.379466273938306 seconds)\n",
      "epoch 001:  52%|▌| 962/1841 [03:08<02:21,  6.21it/s, loss=8.929, nll_loss=7.683,2023-07-04 15:56:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:  54%|▌| 1003/1841 [03:15<02:14,  6.24it/s, loss=8.929, nll_loss=7.6832023-07-04 15:56:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 15:56:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 18.51it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 22.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 15:56:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.227 | nll_loss 7.959 | ppl 248.79 | wps 46712.7 | wpb 1908.5 | bsz 89.2 | num_updates 1000 | best_loss 9.227\n",
      "2023-07-04 15:56:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates\n",
      "2023-07-04 15:56:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_1_1000.pt\n",
      "2023-07-04 15:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_1_1000.pt\n",
      "2023-07-04 15:57:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 9.227) (writing took 32.02041709201876 seconds)\n",
      "epoch 001:  82%|▊| 1503/1841 [05:07<00:53,  6.33it/s, loss=8.362, nll_loss=7.04,2023-07-04 15:58:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 15:58:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.54it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 15:58:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.856 | nll_loss 7.53 | ppl 184.8 | wps 48228.9 | wpb 1908.5 | bsz 89.2 | num_updates 1500 | best_loss 8.856\n",
      "2023-07-04 15:58:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1500 updates\n",
      "2023-07-04 15:58:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_1_1500.pt\n",
      "2023-07-04 15:59:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_1_1500.pt\n",
      "2023-07-04 15:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_1_1500.pt (epoch 1 @ 1500 updates, score 8.856) (writing took 30.715805045096204 seconds)\n",
      "epoch 001: 100%|▉| 1840/1841 [06:31<00:00,  6.71it/s, loss=8.194, nll_loss=6.8492023-07-04 16:00:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:00:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.24it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.60it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.26it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:00:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.654 | nll_loss 7.298 | ppl 157.39 | wps 48327.5 | wpb 1908.5 | bsz 89.2 | num_updates 1837 | best_loss 8.654\n",
      "2023-07-04 16:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1837 updates\n",
      "2023-07-04 16:00:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_best.pt\n",
      "2023-07-04 16:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_best.pt\n",
      "2023-07-04 16:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 1 @ 1837 updates, score 8.654) (writing took 22.910238821059465 seconds)\n",
      "2023-07-04 16:00:39 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-07-04 16:00:39 | INFO | train | epoch 001 | loss 9.406 | nll_loss 8.232 | ppl 300.72 | wps 15044.2 | ups 4.44 | wpb 3389.3 | bsz 177.5 | num_updates 1837 | lr 0.0009185 | gnorm 1.729 | clip 86.4 | loss_scale 8 | train_wall 290 | gb_free 11.5 | wall 416\n",
      "2023-07-04 16:00:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:00:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 002:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:00:39 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-07-04 16:00:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:   9%| | 162/1841 [00:25<04:23,  6.38it/s, loss=7.957, nll_loss=6.582,2023-07-04 16:01:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:01:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.58it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.68it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.23it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:01:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.684 | nll_loss 7.32 | ppl 159.76 | wps 48527.4 | wpb 1908.5 | bsz 89.2 | num_updates 2000 | best_loss 8.654\n",
      "2023-07-04 16:01:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates\n",
      "2023-07-04 16:01:06 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_2000.pt\n",
      "2023-07-04 16:01:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_2000.pt\n",
      "2023-07-04 16:01:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 8.684) (writing took 22.174390575964935 seconds)\n",
      "epoch 002:  36%|▎| 662/1841 [02:08<03:16,  6.01it/s, loss=7.776, nll_loss=6.38, 2023-07-04 16:02:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:02:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.49it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.71it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.07it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:02:48 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.372 | nll_loss 6.963 | ppl 124.73 | wps 48319.7 | wpb 1908.5 | bsz 89.2 | num_updates 2500 | best_loss 8.372\n",
      "2023-07-04 16:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2500 updates\n",
      "2023-07-04 16:02:48 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_2500.pt\n",
      "2023-07-04 16:02:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_2500.pt\n",
      "2023-07-04 16:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_2_2500.pt (epoch 2 @ 2500 updates, score 8.372) (writing took 31.411820953013375 seconds)\n",
      "epoch 002:  63%|▋| 1162/1841 [03:59<01:41,  6.66it/s, loss=7.668, nll_loss=6.2632023-07-04 16:04:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:04:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.93it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.16it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:04:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.21 | nll_loss 6.828 | ppl 113.6 | wps 47667.5 | wpb 1908.5 | bsz 89.2 | num_updates 3000 | best_loss 8.21\n",
      "2023-07-04 16:04:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3000 updates\n",
      "2023-07-04 16:04:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_3000.pt\n",
      "2023-07-04 16:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_3000.pt\n",
      "2023-07-04 16:05:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_2_3000.pt (epoch 2 @ 3000 updates, score 8.21) (writing took 31.367372442036867 seconds)\n",
      "epoch 002:  90%|▉| 1662/1841 [05:51<00:29,  5.97it/s, loss=7.487, nll_loss=6.0592023-07-04 16:06:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:06:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.13it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:00<00:00, 18.19it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:00<00:00, 22.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:06:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.032 | nll_loss 6.601 | ppl 97.04 | wps 47189.7 | wpb 1908.5 | bsz 89.2 | num_updates 3500 | best_loss 8.032\n",
      "2023-07-04 16:06:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3500 updates\n",
      "2023-07-04 16:06:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_3500.pt\n",
      "2023-07-04 16:06:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_2_3500.pt\n",
      "2023-07-04 16:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_2_3500.pt (epoch 2 @ 3500 updates, score 8.032) (writing took 31.360103651997633 seconds)\n",
      "epoch 002: 100%|▉| 1840/1841 [06:51<00:00,  6.95it/s, loss=7.341, nll_loss=5.8942023-07-04 16:07:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:07:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.49it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.33it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.71it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:07:31 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.051 | nll_loss 6.643 | ppl 99.91 | wps 49212.8 | wpb 1908.5 | bsz 89.2 | num_updates 3678 | best_loss 8.032\n",
      "2023-07-04 16:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3678 updates\n",
      "2023-07-04 16:07:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:07:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 2 @ 3678 updates, score 8.051) (writing took 11.508152605965734 seconds)\n",
      "2023-07-04 16:07:43 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-07-04 16:07:43 | INFO | train | epoch 002 | loss 7.635 | nll_loss 6.223 | ppl 74.67 | wps 14720.7 | ups 4.34 | wpb 3390.2 | bsz 179.2 | num_updates 3678 | lr 0.00073741 | gnorm 1.104 | clip 44.1 | loss_scale 8 | train_wall 289 | gb_free 12.3 | wall 840\n",
      "2023-07-04 16:07:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:07:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 003:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:07:43 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-07-04 16:07:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  17%|▏| 321/1841 [00:51<04:01,  6.30it/s, loss=7.03, nll_loss=5.543, 2023-07-04 16:08:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:08:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 17.21it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.53it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.85it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:08:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.003 | nll_loss 6.564 | ppl 94.63 | wps 48831.9 | wpb 1908.5 | bsz 89.2 | num_updates 4000 | best_loss 8.003\n",
      "2023-07-04 16:08:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates\n",
      "2023-07-04 16:08:35 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_4000.pt\n",
      "2023-07-04 16:08:46 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_4000.pt\n",
      "2023-07-04 16:09:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 8.003) (writing took 31.622196501935832 seconds)\n",
      "epoch 003:  45%|▍| 821/1841 [02:43<02:48,  6.05it/s, loss=7.135, nll_loss=5.664,2023-07-04 16:10:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:10:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.07it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.43it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.80it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:10:28 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.902 | nll_loss 6.442 | ppl 86.92 | wps 46993.9 | wpb 1908.5 | bsz 89.2 | num_updates 4500 | best_loss 7.902\n",
      "2023-07-04 16:10:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4500 updates\n",
      "2023-07-04 16:10:28 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_4500.pt\n",
      "2023-07-04 16:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_4500.pt\n",
      "2023-07-04 16:10:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_3_4500.pt (epoch 3 @ 4500 updates, score 7.902) (writing took 30.457178294076584 seconds)\n",
      "epoch 003:  72%|▋| 1321/1841 [04:35<01:25,  6.11it/s, loss=6.978, nll_loss=5.4882023-07-04 16:12:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:12:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.90it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.23it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.65it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:12:19 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.905 | nll_loss 6.481 | ppl 89.35 | wps 47218.5 | wpb 1908.5 | bsz 89.2 | num_updates 5000 | best_loss 7.902\n",
      "2023-07-04 16:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5000 updates\n",
      "2023-07-04 16:12:19 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_5000.pt\n",
      "2023-07-04 16:12:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_5000.pt\n",
      "2023-07-04 16:12:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_3_5000.pt (epoch 3 @ 5000 updates, score 7.905) (writing took 22.14440630399622 seconds)\n",
      "epoch 003:  99%|▉| 1821/1841 [06:18<00:03,  6.41it/s, loss=6.951, nll_loss=5.4572023-07-04 16:14:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:14:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.42it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:14:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.812 | nll_loss 6.351 | ppl 81.65 | wps 47232.3 | wpb 1908.5 | bsz 89.2 | num_updates 5500 | best_loss 7.812\n",
      "2023-07-04 16:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5500 updates\n",
      "2023-07-04 16:14:02 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_5500.pt\n",
      "2023-07-04 16:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_3_5500.pt\n",
      "2023-07-04 16:14:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_3_5500.pt (epoch 3 @ 5500 updates, score 7.812) (writing took 31.462992217042483 seconds)\n",
      "epoch 003: 100%|▉| 1840/1841 [06:53<00:00,  5.76it/s, loss=6.966, nll_loss=5.4752023-07-04 16:14:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:14:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.30it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 18.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.62it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:14:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.826 | nll_loss 6.397 | ppl 84.29 | wps 48322.9 | wpb 1908.5 | bsz 89.2 | num_updates 5519 | best_loss 7.812\n",
      "2023-07-04 16:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5519 updates\n",
      "2023-07-04 16:14:37 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:14:48 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:14:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 3 @ 5519 updates, score 7.826) (writing took 10.874940106994472 seconds)\n",
      "2023-07-04 16:14:48 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-07-04 16:14:48 | INFO | train | epoch 003 | loss 7.015 | nll_loss 5.528 | ppl 46.15 | wps 14684.5 | ups 4.33 | wpb 3390.2 | bsz 179.2 | num_updates 5519 | lr 0.000601984 | gnorm 1.03 | clip 33.9 | loss_scale 8 | train_wall 291 | gb_free 13.8 | wall 1265\n",
      "2023-07-04 16:14:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:14:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 004:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:14:48 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-07-04 16:14:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  26%|▎| 480/1841 [01:16<03:32,  6.40it/s, loss=6.702, nll_loss=5.175,2023-07-04 16:16:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:16:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 17.08it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.13it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.58it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:16:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.822 | nll_loss 6.391 | ppl 83.92 | wps 48608.2 | wpb 1908.5 | bsz 89.2 | num_updates 6000 | best_loss 7.812\n",
      "2023-07-04 16:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6000 updates\n",
      "2023-07-04 16:16:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_4_6000.pt\n",
      "2023-07-04 16:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_4_6000.pt\n",
      "2023-07-04 16:16:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_4_6000.pt (epoch 4 @ 6000 updates, score 7.822) (writing took 22.64139232400339 seconds)\n",
      "epoch 004:  53%|▌| 980/1841 [02:59<02:14,  6.38it/s, loss=6.731, nll_loss=5.21, 2023-07-04 16:17:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:17:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.08it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.52it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.79it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:17:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.874 | nll_loss 6.406 | ppl 84.83 | wps 47167 | wpb 1908.5 | bsz 89.2 | num_updates 6500 | best_loss 7.812\n",
      "2023-07-04 16:17:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6500 updates\n",
      "2023-07-04 16:17:48 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_4_6500.pt\n",
      "2023-07-04 16:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_4_6500.pt\n",
      "2023-07-04 16:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_4_6500.pt (epoch 4 @ 6500 updates, score 7.874) (writing took 23.969474466051906 seconds)\n",
      "epoch 004:  80%|▊| 1480/1841 [04:44<00:56,  6.38it/s, loss=6.728, nll_loss=5.2072023-07-04 16:19:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:19:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.62it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.40it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:19:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.79 | nll_loss 6.336 | ppl 80.77 | wps 47657.8 | wpb 1908.5 | bsz 89.2 | num_updates 7000 | best_loss 7.79\n",
      "2023-07-04 16:19:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7000 updates\n",
      "2023-07-04 16:19:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_4_7000.pt\n",
      "2023-07-04 16:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_4_7000.pt\n",
      "2023-07-04 16:20:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_4_7000.pt (epoch 4 @ 7000 updates, score 7.79) (writing took 30.599266269942746 seconds)\n",
      "epoch 004: 100%|▉| 1840/1841 [06:13<00:00,  6.41it/s, loss=6.591, nll_loss=5.0542023-07-04 16:21:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:21:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.56it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.64it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:21:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.713 | nll_loss 6.263 | ppl 76.8 | wps 47599.9 | wpb 1908.5 | bsz 89.2 | num_updates 7360 | best_loss 7.713\n",
      "2023-07-04 16:21:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7360 updates\n",
      "2023-07-04 16:21:02 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_best.pt\n",
      "2023-07-04 16:21:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_best.pt\n",
      "2023-07-04 16:21:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 4 @ 7360 updates, score 7.713) (writing took 23.303547011106275 seconds)\n",
      "2023-07-04 16:21:25 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-07-04 16:21:25 | INFO | train | epoch 004 | loss 6.67 | nll_loss 5.141 | ppl 35.28 | wps 15711.5 | ups 4.63 | wpb 3390.2 | bsz 179.2 | num_updates 7360 | lr 0.000521286 | gnorm 1.053 | clip 39.2 | loss_scale 8 | train_wall 290 | gb_free 11.5 | wall 1663\n",
      "2023-07-04 16:21:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:21:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 005:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:21:25 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-07-04 16:21:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:   8%| | 139/1841 [00:22<04:35,  6.19it/s, loss=6.512, nll_loss=4.964,2023-07-04 16:21:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:21:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.12it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 18.95it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.48it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:21:48 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.727 | nll_loss 6.272 | ppl 77.26 | wps 46785.8 | wpb 1908.5 | bsz 89.2 | num_updates 7500 | best_loss 7.713\n",
      "2023-07-04 16:21:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7500 updates\n",
      "2023-07-04 16:21:48 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_7500.pt\n",
      "2023-07-04 16:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_7500.pt\n",
      "2023-07-04 16:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_5_7500.pt (epoch 5 @ 7500 updates, score 7.727) (writing took 21.901393620995805 seconds)\n",
      "epoch 005:  35%|▎| 639/1841 [02:04<03:16,  6.13it/s, loss=6.422, nll_loss=4.861,2023-07-04 16:23:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:23:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.60it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.21it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.39it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:23:31 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.713 | nll_loss 6.242 | ppl 75.7 | wps 48363.8 | wpb 1908.5 | bsz 89.2 | num_updates 8000 | best_loss 7.713\n",
      "2023-07-04 16:23:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8000 updates\n",
      "2023-07-04 16:23:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_8000.pt\n",
      "2023-07-04 16:23:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_8000.pt\n",
      "2023-07-04 16:24:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_5_8000.pt (epoch 5 @ 8000 updates, score 7.713) (writing took 31.43812811502721 seconds)\n",
      "epoch 005:  62%|▌| 1139/1841 [03:56<01:50,  6.35it/s, loss=6.425, nll_loss=4.8652023-07-04 16:25:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:25:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.99it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.25it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.66it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:25:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.681 | nll_loss 6.211 | ppl 74.08 | wps 46662.9 | wpb 1908.5 | bsz 89.2 | num_updates 8500 | best_loss 7.681\n",
      "2023-07-04 16:25:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8500 updates\n",
      "2023-07-04 16:25:23 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_8500.pt\n",
      "2023-07-04 16:25:33 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_8500.pt\n",
      "2023-07-04 16:25:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_5_8500.pt (epoch 5 @ 8500 updates, score 7.681) (writing took 31.381102000013925 seconds)\n",
      "epoch 005:  89%|▉| 1639/1841 [05:48<00:30,  6.54it/s, loss=6.461, nll_loss=4.9062023-07-04 16:27:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:27:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.51it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 18.76it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.39it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:27:15 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.709 | nll_loss 6.291 | ppl 78.32 | wps 46686.6 | wpb 1908.5 | bsz 89.2 | num_updates 9000 | best_loss 7.681\n",
      "2023-07-04 16:27:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9000 updates\n",
      "2023-07-04 16:27:15 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_9000.pt\n",
      "2023-07-04 16:27:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_5_9000.pt\n",
      "2023-07-04 16:27:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_5_9000.pt (epoch 5 @ 9000 updates, score 7.709) (writing took 22.648552644997835 seconds)\n",
      "epoch 005: 100%|▉| 1840/1841 [06:44<00:00,  6.40it/s, loss=6.426, nll_loss=4.8672023-07-04 16:28:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:28:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.22it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.35it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.89it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:28:10 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.708 | nll_loss 6.265 | ppl 76.89 | wps 48222.5 | wpb 1908.5 | bsz 89.2 | num_updates 9201 | best_loss 7.681\n",
      "2023-07-04 16:28:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9201 updates\n",
      "2023-07-04 16:28:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:28:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 5 @ 9201 updates, score 7.708) (writing took 11.136553256073967 seconds)\n",
      "2023-07-04 16:28:21 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-07-04 16:28:21 | INFO | train | epoch 005 | loss 6.424 | nll_loss 4.865 | ppl 29.14 | wps 14998.4 | ups 4.42 | wpb 3390.2 | bsz 179.2 | num_updates 9201 | lr 0.000466227 | gnorm 1.092 | clip 44.9 | loss_scale 8 | train_wall 290 | gb_free 12 | wall 2079\n",
      "2023-07-04 16:28:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:28:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 006:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:28:21 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-07-04 16:28:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  16%|▏| 298/1841 [00:47<04:06,  6.27it/s, loss=6.19, nll_loss=4.6, pp2023-07-04 16:29:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:29:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.19it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.93it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.44it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:29:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.736 | nll_loss 6.273 | ppl 77.32 | wps 49208.7 | wpb 1908.5 | bsz 89.2 | num_updates 9500 | best_loss 7.681\n",
      "2023-07-04 16:29:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9500 updates\n",
      "2023-07-04 16:29:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_9500.pt\n",
      "2023-07-04 16:29:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_9500.pt\n",
      "2023-07-04 16:29:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_6_9500.pt (epoch 6 @ 9500 updates, score 7.736) (writing took 22.778256783960387 seconds)\n",
      "epoch 006:  43%|▍| 798/1841 [02:31<02:41,  6.45it/s, loss=6.141, nll_loss=4.544,2023-07-04 16:30:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:30:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.23it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.39it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.77it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:30:53 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.749 | nll_loss 6.312 | ppl 79.48 | wps 46906 | wpb 1908.5 | bsz 89.2 | num_updates 10000 | best_loss 7.681\n",
      "2023-07-04 16:30:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10000 updates\n",
      "2023-07-04 16:30:53 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_10000.pt\n",
      "2023-07-04 16:31:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_10000.pt\n",
      "2023-07-04 16:31:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_6_10000.pt (epoch 6 @ 10000 updates, score 7.749) (writing took 26.581181540037505 seconds)\n",
      "epoch 006:  71%|▋| 1298/1841 [04:18<01:25,  6.38it/s, loss=6.242, nll_loss=4.6582023-07-04 16:32:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:32:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.52it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.16it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.46it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:32:40 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.659 | nll_loss 6.198 | ppl 73.43 | wps 48397.1 | wpb 1908.5 | bsz 89.2 | num_updates 10500 | best_loss 7.659\n",
      "2023-07-04 16:32:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10500 updates\n",
      "2023-07-04 16:32:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_10500.pt\n",
      "2023-07-04 16:32:50 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_10500.pt\n",
      "2023-07-04 16:33:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_6_10500.pt (epoch 6 @ 10500 updates, score 7.659) (writing took 29.944689312949777 seconds)\n",
      "epoch 006:  98%|▉| 1798/1841 [06:09<00:06,  6.32it/s, loss=6.224, nll_loss=4.64,2023-07-04 16:34:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:34:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.78it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.22it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.60it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:34:31 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.728 | nll_loss 6.264 | ppl 76.86 | wps 47989.9 | wpb 1908.5 | bsz 89.2 | num_updates 11000 | best_loss 7.659\n",
      "2023-07-04 16:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 11000 updates\n",
      "2023-07-04 16:34:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_11000.pt\n",
      "2023-07-04 16:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_6_11000.pt\n",
      "2023-07-04 16:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_6_11000.pt (epoch 6 @ 11000 updates, score 7.728) (writing took 22.36953553603962 seconds)\n",
      "epoch 006: 100%|▉| 1840/1841 [06:38<00:00,  6.33it/s, loss=6.235, nll_loss=4.6532023-07-04 16:35:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:35:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.00it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.33it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.94it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:35:01 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.636 | nll_loss 6.199 | ppl 73.44 | wps 48311.4 | wpb 1908.5 | bsz 89.2 | num_updates 11042 | best_loss 7.636\n",
      "2023-07-04 16:35:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 11042 updates\n",
      "2023-07-04 16:35:01 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_best.pt\n",
      "2023-07-04 16:35:12 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_best.pt\n",
      "2023-07-04 16:35:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_best.pt (epoch 6 @ 11042 updates, score 7.636) (writing took 24.394773831008933 seconds)\n",
      "2023-07-04 16:35:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-07-04 16:35:25 | INFO | train | epoch 006 | loss 6.233 | nll_loss 4.649 | ppl 25.1 | wps 14717 | ups 4.34 | wpb 3390.2 | bsz 179.2 | num_updates 11042 | lr 0.00042559 | gnorm 1.125 | clip 53.4 | loss_scale 8 | train_wall 290 | gb_free 11.9 | wall 2503\n",
      "2023-07-04 16:35:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:35:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 007:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:35:25 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-07-04 16:35:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  25%|▏| 457/1841 [01:13<03:41,  6.26it/s, loss=6, nll_loss=4.386, ppl2023-07-04 16:36:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:36:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.50it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.63it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.25it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:36:39 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.795 | nll_loss 6.326 | ppl 80.21 | wps 49391.4 | wpb 1908.5 | bsz 89.2 | num_updates 11500 | best_loss 7.636\n",
      "2023-07-04 16:36:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11500 updates\n",
      "2023-07-04 16:36:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_7_11500.pt\n",
      "2023-07-04 16:37:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_7_11500.pt\n",
      "2023-07-04 16:37:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_7_11500.pt (epoch 7 @ 11500 updates, score 7.795) (writing took 58.02342468907591 seconds)\n",
      "epoch 007:  52%|▌| 957/1841 [03:31<02:23,  6.14it/s, loss=6.127, nll_loss=4.53, 2023-07-04 16:38:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:38:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.97it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.44it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.84it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:38:58 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.708 | nll_loss 6.26 | ppl 76.63 | wps 47732 | wpb 1908.5 | bsz 89.2 | num_updates 12000 | best_loss 7.636\n",
      "2023-07-04 16:39:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12000 updates\n",
      "2023-07-04 16:39:07 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_7_12000.pt\n",
      "2023-07-04 16:39:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_7_12000.pt\n",
      "2023-07-04 16:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_7_12000.pt (epoch 7 @ 12000 updates, score 7.708) (writing took 65.6483470130479 seconds)\n",
      "epoch 007:  65%|▋| 1202/1841 [05:26<01:39,  6.42it/s, loss=6.083, nll_loss=4.4832023-07-04 16:40:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 007:  79%|▊| 1458/1841 [06:06<01:03,  6.08it/s, loss=6.144, nll_loss=4.5512023-07-04 16:41:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:41:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.15it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:00<00:00, 17.31it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:00<00:00, 20.52it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:00<00:00, 22.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:41:33 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.721 | nll_loss 6.274 | ppl 77.39 | wps 45194.5 | wpb 1908.5 | bsz 89.2 | num_updates 12500 | best_loss 7.636\n",
      "2023-07-04 16:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12500 updates\n",
      "2023-07-04 16:41:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_7_12500.pt\n",
      "2023-07-04 16:42:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_7_12500.pt\n",
      "2023-07-04 16:42:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_7_12500.pt (epoch 7 @ 12500 updates, score 7.721) (writing took 81.0124456640333 seconds)\n",
      "epoch 007: 100%|▉| 1840/1841 [08:29<00:00,  6.24it/s, loss=6.161, nll_loss=4.5722023-07-04 16:43:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:43:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.78it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.02it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.55it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:43:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.69 | nll_loss 6.255 | ppl 76.38 | wps 47141.6 | wpb 1908.5 | bsz 89.2 | num_updates 12882 | best_loss 7.636\n",
      "2023-07-04 16:43:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12882 updates\n",
      "2023-07-04 16:43:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:44:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 7 @ 12882 updates, score 7.69) (writing took 41.18991215992719 seconds)\n",
      "2023-07-04 16:44:37 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-07-04 16:44:37 | INFO | train | epoch 007 | loss 6.073 | nll_loss 4.47 | ppl 22.16 | wps 11309.7 | ups 3.34 | wpb 3389.8 | bsz 178.2 | num_updates 12882 | lr 0.000394025 | gnorm 1.147 | clip 62.9 | loss_scale 4 | train_wall 289 | gb_free 11.2 | wall 3054\n",
      "2023-07-04 16:44:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:44:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 008:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:44:37 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-07-04 16:44:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:   6%| | 117/1841 [00:18<04:46,  6.02it/s, loss=6.051, nll_loss=4.445,2023-07-04 16:44:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:44:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.21it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.40it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.76it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:44:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.778 | nll_loss 6.352 | ppl 81.67 | wps 47142.2 | wpb 1908.5 | bsz 89.2 | num_updates 13000 | best_loss 7.636\n",
      "2023-07-04 16:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13000 updates\n",
      "2023-07-04 16:44:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_13000.pt\n",
      "2023-07-04 16:45:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_13000.pt\n",
      "2023-07-04 16:45:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_8_13000.pt (epoch 8 @ 13000 updates, score 7.778) (writing took 60.73377744492609 seconds)\n",
      "epoch 008:  34%|▎| 617/1841 [02:39<03:18,  6.16it/s, loss=5.989, nll_loss=4.375,2023-07-04 16:47:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:47:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.99it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.92it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.33it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.02it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:47:17 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.745 | nll_loss 6.297 | ppl 78.65 | wps 48258.3 | wpb 1908.5 | bsz 89.2 | num_updates 13500 | best_loss 7.636\n",
      "2023-07-04 16:47:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13500 updates\n",
      "2023-07-04 16:47:23 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_13500.pt\n",
      "2023-07-04 16:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_13500.pt\n",
      "2023-07-04 16:48:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_8_13500.pt (epoch 8 @ 13500 updates, score 7.745) (writing took 86.66467279905919 seconds)\n",
      "epoch 008:  61%|▌| 1117/1841 [05:32<01:57,  6.16it/s, loss=5.959, nll_loss=4.3412023-07-04 16:50:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:50:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.74it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.88it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.39it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:50:10 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.781 | nll_loss 6.36 | ppl 82.13 | wps 48676.8 | wpb 1908.5 | bsz 89.2 | num_updates 14000 | best_loss 7.636\n",
      "2023-07-04 16:50:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14000 updates\n",
      "2023-07-04 16:50:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_14000.pt\n",
      "2023-07-04 16:50:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_14000.pt\n",
      "2023-07-04 16:50:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_8_14000.pt (epoch 8 @ 14000 updates, score 7.781) (writing took 39.03054594004061 seconds)\n",
      "epoch 008:  88%|▉| 1617/1841 [07:32<00:35,  6.39it/s, loss=5.926, nll_loss=4.3052023-07-04 16:52:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:52:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.87it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.40it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.14it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:52:10 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.707 | nll_loss 6.258 | ppl 76.55 | wps 47725.2 | wpb 1908.5 | bsz 89.2 | num_updates 14500 | best_loss 7.636\n",
      "2023-07-04 16:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14500 updates\n",
      "2023-07-04 16:52:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_14500.pt\n",
      "2023-07-04 16:52:56 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_8_14500.pt\n",
      "2023-07-04 16:53:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_8_14500.pt (epoch 8 @ 14500 updates, score 7.707) (writing took 66.73444952000864 seconds)\n",
      "epoch 008: 100%|▉| 1840/1841 [09:15<00:00,  6.51it/s, loss=6.023, nll_loss=4.4152023-07-04 16:53:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:53:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.31it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:00<00:00, 18.09it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:00<00:00, 21.24it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:00<00:00, 23.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:53:53 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.717 | nll_loss 6.292 | ppl 78.37 | wps 47285.2 | wpb 1908.5 | bsz 89.2 | num_updates 14723 | best_loss 7.636\n",
      "2023-07-04 16:53:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14723 updates\n",
      "2023-07-04 16:53:53 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:54:35 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 16:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 8 @ 14723 updates, score 7.717) (writing took 42.77147487306502 seconds)\n",
      "2023-07-04 16:54:36 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-07-04 16:54:36 | INFO | train | epoch 008 | loss 5.937 | nll_loss 4.317 | ppl 19.93 | wps 10424.7 | ups 3.07 | wpb 3390.2 | bsz 179.2 | num_updates 14723 | lr 0.000368567 | gnorm 1.184 | clip 72.5 | loss_scale 4 | train_wall 290 | gb_free 14 | wall 3653\n",
      "2023-07-04 16:54:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 16:54:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 009:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 16:54:36 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-07-04 16:54:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:  15%|▏| 276/1841 [00:44<04:09,  6.28it/s, loss=5.671, nll_loss=4.018,2023-07-04 16:55:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 14.29it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 18.97it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.37it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 22.98it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:55:21 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.872 | nll_loss 6.406 | ppl 84.77 | wps 46395.8 | wpb 1908.5 | bsz 89.2 | num_updates 15000 | best_loss 7.636\n",
      "2023-07-04 16:55:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15000 updates\n",
      "2023-07-04 16:55:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_15000.pt\n",
      "2023-07-04 16:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_15000.pt\n",
      "2023-07-04 16:56:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_9_15000.pt (epoch 9 @ 15000 updates, score 7.872) (writing took 61.912616522051394 seconds)\n",
      "epoch 009:  42%|▍| 776/1841 [03:06<02:43,  6.52it/s, loss=5.873, nll_loss=4.244,2023-07-04 16:57:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 16:57:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:00<00:00, 18.16it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:00<00:00, 21.13it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:00<00:00, 22.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 16:57:43 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.815 | nll_loss 6.382 | ppl 83.38 | wps 46603 | wpb 1908.5 | bsz 89.2 | num_updates 15500 | best_loss 7.636\n",
      "2023-07-04 16:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15500 updates\n",
      "2023-07-04 16:57:43 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_15500.pt\n",
      "2023-07-04 16:58:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_15500.pt\n",
      "2023-07-04 16:58:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_9_15500.pt (epoch 9 @ 15500 updates, score 7.815) (writing took 70.1729083340615 seconds)\n",
      "epoch 009:  69%|▋| 1276/1841 [05:36<01:33,  6.02it/s, loss=5.781, nll_loss=4.1412023-07-04 17:00:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:00:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.25it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.01it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.46it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:00:13 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.79 | nll_loss 6.355 | ppl 81.84 | wps 49222.3 | wpb 1908.5 | bsz 89.2 | num_updates 16000 | best_loss 7.636\n",
      "2023-07-04 17:00:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16000 updates\n",
      "2023-07-04 17:00:13 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_16000.pt\n",
      "2023-07-04 17:00:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_16000.pt\n",
      "2023-07-04 17:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_9_16000.pt (epoch 9 @ 16000 updates, score 7.79) (writing took 60.841325960936956 seconds)\n",
      "epoch 009:  96%|▉| 1776/1841 [07:58<00:10,  6.44it/s, loss=5.901, nll_loss=4.2772023-07-04 17:02:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:02:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.15it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.10it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.51it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:02:35 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.772 | nll_loss 6.332 | ppl 80.56 | wps 48561.7 | wpb 1908.5 | bsz 89.2 | num_updates 16500 | best_loss 7.636\n",
      "2023-07-04 17:02:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16500 updates\n",
      "2023-07-04 17:02:35 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_16500.pt\n",
      "2023-07-04 17:03:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_9_16500.pt\n",
      "2023-07-04 17:03:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_9_16500.pt (epoch 9 @ 16500 updates, score 7.772) (writing took 68.71873566601425 seconds)\n",
      "epoch 009: 100%|▉| 1840/1841 [09:17<00:00,  6.86it/s, loss=5.882, nll_loss=4.2572023-07-04 17:03:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:03:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.58it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.70it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 21.97it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:03:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.85 | nll_loss 6.452 | ppl 87.52 | wps 46892.3 | wpb 1908.5 | bsz 89.2 | num_updates 16564 | best_loss 7.636\n",
      "2023-07-04 17:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16564 updates\n",
      "2023-07-04 17:03:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 17:04:30 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 17:04:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 9 @ 16564 updates, score 7.85) (writing took 35.31418800004758 seconds)\n",
      "2023-07-04 17:04:30 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-07-04 17:04:30 | INFO | train | epoch 009 | loss 5.816 | nll_loss 4.181 | ppl 18.14 | wps 10505.1 | ups 3.1 | wpb 3390.2 | bsz 179.2 | num_updates 16564 | lr 0.000347482 | gnorm 1.214 | clip 80.2 | loss_scale 4 | train_wall 290 | gb_free 12.4 | wall 4247\n",
      "2023-07-04 17:04:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 17:04:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1841\n",
      "epoch 010:   0%|                                       | 0/1841 [00:00<?, ?it/s]2023-07-04 17:04:30 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-07-04 17:04:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:  24%|▏| 435/1841 [01:08<03:35,  6.51it/s, loss=5.618, nll_loss=3.958,2023-07-04 17:05:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:05:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 16.79it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  31%|██▏    | 4/13 [00:00<00:00, 17.99it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  54%|███▊   | 7/13 [00:00<00:00, 21.10it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  77%|████▌ | 10/13 [00:00<00:00, 22.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:05:39 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.849 | nll_loss 6.424 | ppl 85.89 | wps 46449.7 | wpb 1908.5 | bsz 89.2 | num_updates 17000 | best_loss 7.636\n",
      "2023-07-04 17:05:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17000 updates\n",
      "2023-07-04 17:05:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_10_17000.pt\n",
      "2023-07-04 17:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_10_17000.pt\n",
      "2023-07-04 17:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_10_17000.pt (epoch 10 @ 17000 updates, score 7.849) (writing took 61.22526276099961 seconds)\n",
      "epoch 010:  51%|▌| 935/1841 [03:29<02:21,  6.41it/s, loss=5.753, nll_loss=4.108,2023-07-04 17:08:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:08:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.19it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.12it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.53it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:08:00 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.822 | nll_loss 6.408 | ppl 84.93 | wps 49237.2 | wpb 1908.5 | bsz 89.2 | num_updates 17500 | best_loss 7.636\n",
      "2023-07-04 17:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17500 updates\n",
      "2023-07-04 17:08:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_10_17500.pt\n",
      "2023-07-04 17:08:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_10_17500.pt\n",
      "2023-07-04 17:08:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_10_17500.pt (epoch 10 @ 17500 updates, score 7.822) (writing took 56.22090306901373 seconds)\n",
      "epoch 010:  78%|▊| 1435/1841 [05:45<01:02,  6.55it/s, loss=5.745, nll_loss=4.1012023-07-04 17:10:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:10:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.68it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 20.33it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.66it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 24.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:10:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.78 | nll_loss 6.333 | ppl 80.62 | wps 48915.5 | wpb 1908.5 | bsz 89.2 | num_updates 18000 | best_loss 7.636\n",
      "2023-07-04 17:10:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18000 updates\n",
      "2023-07-04 17:10:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_10_18000.pt\n",
      "2023-07-04 17:10:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_10_18000.pt\n",
      "2023-07-04 17:10:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_10_18000.pt (epoch 10 @ 18000 updates, score 7.78) (writing took 42.20113865402527 seconds)\n",
      "epoch 010: 100%|▉| 1840/1841 [07:32<00:00,  6.43it/s, loss=5.831, nll_loss=4.1982023-07-04 17:12:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-04 17:12:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|               | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  15%|█      | 2/13 [00:00<00:00, 15.83it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|██▋    | 5/13 [00:00<00:00, 19.64it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  62%|████▎  | 8/13 [00:00<00:00, 22.23it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  85%|█████ | 11/13 [00:00<00:00, 23.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-04 17:12:03 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.794 | nll_loss 6.354 | ppl 81.79 | wps 48529.8 | wpb 1908.5 | bsz 89.2 | num_updates 18405 | best_loss 7.636\n",
      "2023-07-04 17:12:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18405 updates\n",
      "2023-07-04 17:12:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 17:12:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints/checkpoint_last.pt\n",
      "2023-07-04 17:12:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/checkpoint_last.pt (epoch 10 @ 18405 updates, score 7.794) (writing took 32.773270438076 seconds)\n",
      "2023-07-04 17:12:36 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-07-04 17:12:36 | INFO | train | epoch 010 | loss 5.71 | nll_loss 4.061 | ppl 16.69 | wps 12839.6 | ups 3.79 | wpb 3390.2 | bsz 179.2 | num_updates 18405 | lr 0.000329645 | gnorm 1.243 | clip 85.6 | loss_scale 4 | train_wall 287 | gb_free 13.2 | wall 4733\n",
      "2023-07-04 17:12:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 17:12:36 | INFO | fairseq_cli.train | done training in 4732.4 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin \\\n",
    "    --arch transformer \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.2 --weight-decay 1e-4 --clip-norm 1.0 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4000 --update-freq 1 \\\n",
    "    --save-interval-updates 500 --validate-interval-updates 500 \\\n",
    "    --keep-interval-updates 500 --no-epoch-checkpoints \\\n",
    "    --fp16 \\\n",
    "    --max-epoch 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92.機械翻訳モデルの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:13:12 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-04 17:13:14 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-04 17:13:15 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-04 17:13:15 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-04 17:13:15 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/checkpoint_best.pt\n",
      "2023-07-04 17:13:20 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.ja\n",
      "2023-07-04 17:13:26 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.en\n",
      "2023-07-04 17:13:26 | INFO | fairseq.tasks.translation | data-bin test ja-en 1160 examples\n",
      "2023-07-04 17:13:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-04 17:13:28 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-04 17:13:28 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-04 17:13:28 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-04 17:13:45 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-04 17:13:45 | INFO | fairseq_cli.generate | Translated 1,160 sentences (23,460 tokens) in 12.2s (95.17 sentences/s, 1924.70 tokens/s)\n",
      "S-144\t1979 年 車 籍 抹消 。\n",
      "T-144\tRegistration deleted in 1979 .\n",
      "H-144\t-1.6123895645141602\tIn 1979 , it was classified as a tumulus .\n",
      "D-144\t-1.6123895645141602\tIn 1979 , it was classified as a tumulus .\n",
      "P-144\t-1.5295 -0.2674 -0.6858 -3.2622 -0.2522 -4.5911 -0.6931 -1.3257 -4.2752 -0.7614 -0.0928\n",
      "S-509\t類纂 形態 を と る 。\n",
      "T-509\tIt takes Ruisan form .\n",
      "H-509\t-1.3371968269348145\tIt is a type of classification .\n",
      "D-509\t-1.3371968269348145\tIt is a type of classification .\n",
      "P-509\t-1.1194 -0.8645 -0.9328 -2.2762 -0.2787 -3.6722 -1.4749 -0.0788\n",
      "S-373\t地上 アナログ テレビジョン 放送 送信 設備\n",
      "T-373\t<<unk>> <<unk>> Television Broadcasting Facilities\n",
      "H-373\t-2.099926471710205\tNHK TV Drama Prize\n",
      "D-373\t-2.099926471710205\tNHK TV Drama Prize\n",
      "P-373\t-1.7636 -1.8456 -2.6822 -2.6415 -1.5667\n",
      "S-371\t地上 デジタル テレビジョン 放送 送信 設備\n",
      "T-371\t<<unk>> Digital Television Broadcasting Facilities\n",
      "H-371\t-2.1664342880249023\tNHK TV broadcasting company\n",
      "D-371\t-2.1664342880249023\tNHK TV broadcasting company\n",
      "P-371\t-1.4427 -1.8075 -3.0297 -2.5687 -1.9836\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-generate data-bin --path checkpoints/checkpoint_best.pt --batch-size 128 --beam 5 > result.txt\n",
    "!head -20 result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrance hall\n",
      "Dogen ( dates of birth and death unknown ) was a Zen monk in the early Kamakura period .\n",
      "He was the founder of the Soto sect .\n",
      "His first name was also written in the same way .\n",
      "It is said that he was a devout believer .\n",
      "His posthumous Buddhist name was Chisho Daishi .\n",
      "It is commonly known as the Zen sect .\n",
      "In Japan , it is said that he or she was a young man of refined taste .\n",
      "It is also said that he made a replica of bamboo leaves in his hometown .\n",
      "There is a view that he was a rakuin of the Minister of the Left , MINAMOTO no Yoshimitsu , and others , but the truth is unknown .\n"
     ]
    }
   ],
   "source": [
    "!grep \"^H-\" result.txt | sort -V | cut -f3 > result.en.txt\n",
    "!head result.en.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93.BLEUスコアの計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-04 17:13:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "Namespace(sys='result.en.txt', ref='kftt-data-1.0/data/tok/kyoto-test.en', order=4, ignore_case=False, sacrebleu=False, sentence_bleu=False)\n",
      "BLEU4 = 6.01, 35.6/9.9/4.1/2.0 (BP=0.820, ratio=0.834, syslen=22300, reflen=26734)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys result.en.txt --ref kftt-data-1.0/data/tok/kyoto-test.en"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94.ビーム探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:32:33 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:32:35 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-05 13:32:36 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-05 13:32:36 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-05 13:32:36 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/checkpoint_best.pt\n",
      "2023-07-05 13:32:40 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.ja\n",
      "2023-07-05 13:32:40 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.en\n",
      "2023-07-05 13:32:40 | INFO | fairseq.tasks.translation | data-bin test ja-en 1160 examples\n",
      "2023-07-05 13:32:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-05 13:32:41 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-05 13:32:41 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-05 13:32:41 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-05 13:32:51 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-05 13:32:51 | INFO | fairseq_cli.generate | Translated 1,160 sentences (23,944 tokens) in 5.2s (223.89 sentences/s, 4621.45 tokens/s)\n",
      "2023-07-05 13:33:00 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:33:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:33:06 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 2, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-05 13:33:06 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-05 13:33:06 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-05 13:33:06 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/checkpoint_best.pt\n",
      "2023-07-05 13:33:11 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.ja\n",
      "2023-07-05 13:33:11 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.en\n",
      "2023-07-05 13:33:11 | INFO | fairseq.tasks.translation | data-bin test ja-en 1160 examples\n",
      "2023-07-05 13:33:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-05 13:33:12 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-05 13:33:12 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-05 13:33:12 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-05 13:33:25 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-05 13:33:25 | INFO | fairseq_cli.generate | Translated 1,160 sentences (24,075 tokens) in 8.8s (131.32 sentences/s, 2725.42 tokens/s)\n",
      "2023-07-05 13:33:35 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:33:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:33:41 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 3, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-05 13:33:42 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-05 13:33:42 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-05 13:33:42 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/checkpoint_best.pt\n",
      "2023-07-05 13:33:46 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.ja\n",
      "2023-07-05 13:33:46 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.en\n",
      "2023-07-05 13:33:46 | INFO | fairseq.tasks.translation | data-bin test ja-en 1160 examples\n",
      "2023-07-05 13:33:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-05 13:33:47 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-05 13:33:47 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-05 13:33:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-05 13:34:01 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-05 13:34:01 | INFO | fairseq_cli.generate | Translated 1,160 sentences (23,745 tokens) in 9.1s (128.09 sentences/s, 2621.99 tokens/s)\n",
      "2023-07-05 13:34:09 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:34:13 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:34:15 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-05 13:34:16 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-05 13:34:16 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-05 13:34:16 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/checkpoint_best.pt\n",
      "2023-07-05 13:34:20 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.ja\n",
      "2023-07-05 13:34:20 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.en\n",
      "2023-07-05 13:34:20 | INFO | fairseq.tasks.translation | data-bin test ja-en 1160 examples\n",
      "2023-07-05 13:34:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-05 13:34:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-05 13:34:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-05 13:34:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-05 13:34:35 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-05 13:34:35 | INFO | fairseq_cli.generate | Translated 1,160 sentences (23,463 tokens) in 8.8s (131.18 sentences/s, 2653.36 tokens/s)\n",
      "2023-07-05 13:34:44 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:34:48 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-05 13:34:50 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-05 13:34:51 | INFO | fairseq.tasks.translation | [ja] dictionary: 114288 types\n",
      "2023-07-05 13:34:51 | INFO | fairseq.tasks.translation | [en] dictionary: 161664 types\n",
      "2023-07-05 13:34:51 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/checkpoint_best.pt\n",
      "2023-07-05 13:34:55 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.ja\n",
      "2023-07-05 13:34:55 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-bin/test.ja-en.en\n",
      "2023-07-05 13:34:55 | INFO | fairseq.tasks.translation | data-bin test ja-en 1160 examples\n",
      "2023-07-05 13:34:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-05 13:34:56 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-05 13:34:56 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-05 13:34:56 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-05 13:35:13 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-05 13:35:13 | INFO | fairseq_cli.generate | Translated 1,160 sentences (23,460 tokens) in 11.6s (100.13 sentences/s, 2025.01 tokens/s)\n",
      "2023-07-05 13:35:21 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in 1 2 3 4 5\n",
    "do\n",
    "CUDA_VISIBLE_DEVICES=0 fairseq-generate data-bin --path checkpoints/checkpoint_best.pt --batch-size 128 --beam $i > result.$i.txt\n",
    "grep \"^H-\" result.$i.txt | sort -V | cut -f3 > result.$i.en.txt\n",
    "fairseq-score --sys result.$i.en.txt --ref kftt-data-1.0/data/tok/kyoto-test.en >> BLEU.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.39, 5.65, 5.83, 5.93, 6.01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "fname = 'BLEU.txt'\n",
    "def read_score(fname):\n",
    "    score = []\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('BLEU4'):\n",
    "                match = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', line)\n",
    "                score.append(float(match.group(0)))\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = read_score(fname)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"beam\")\n",
    "plt.ylabel(\"BLEU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95.サブワード化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=./spm-model/spm.ja --vocab_size=32000 --character_coverage=0.9995 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "  input_format: \n",
      "  model_prefix: ./spm-model/spm.ja\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ./kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 440288 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=17202261\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=3975\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 440288 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=5834117\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 1003975 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 440288\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 427559\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 427559 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=576458 obj=151.975 num_tokens=6372434 num_tokens/piece=11.0545\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=494930 obj=137.483 num_tokens=6422465 num_tokens/piece=12.9765\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=370304 obj=137.706 num_tokens=6571764 num_tokens/piece=17.7469\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=368500 obj=137.171 num_tokens=6597890 num_tokens/piece=17.9047\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=276234 obj=138.61 num_tokens=6782170 num_tokens/piece=24.5523\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=276076 obj=138.033 num_tokens=6785109 num_tokens/piece=24.577\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=207043 obj=140.182 num_tokens=7016751 num_tokens/piece=33.8903\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=207025 obj=139.489 num_tokens=7020474 num_tokens/piece=33.9112\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=155267 obj=142.201 num_tokens=7275281 num_tokens/piece=46.8566\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=155267 obj=141.466 num_tokens=7278832 num_tokens/piece=46.8795\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=116450 obj=144.659 num_tokens=7545243 num_tokens/piece=64.7938\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=116449 obj=143.924 num_tokens=7546202 num_tokens/piece=64.8026\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=87335 obj=147.445 num_tokens=7823574 num_tokens/piece=89.5812\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=87335 obj=146.732 num_tokens=7825780 num_tokens/piece=89.6065\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=65501 obj=150.583 num_tokens=8119702 num_tokens/piece=123.963\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=65501 obj=149.852 num_tokens=8119828 num_tokens/piece=123.965\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49125 obj=153.942 num_tokens=8434196 num_tokens/piece=171.688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49125 obj=153.177 num_tokens=8435614 num_tokens/piece=171.717\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=36843 obj=157.539 num_tokens=8771656 num_tokens/piece=238.082\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=36843 obj=156.709 num_tokens=8772150 num_tokens/piece=238.095\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=35200 obj=157.384 num_tokens=8828311 num_tokens/piece=250.804\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=35200 obj=157.251 num_tokens=8828692 num_tokens/piece=250.815\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ./spm-model/spm.ja.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ./spm-model/spm.ja.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=./kftt-data-1.0/data/orig/kyoto-train.en --model_prefix=./spm-model/spm.en --vocab_size=32000 --character_coverage=1.0 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./kftt-data-1.0/data/orig/kyoto-train.en\n",
      "  input_format: \n",
      "  model_prefix: ./spm-model/spm.en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ./kftt-data-1.0/data/orig/kyoto-train.en\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 440286 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=59809000\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 100% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=3609\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 440286 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=30957521\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 414564 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 440286\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 458305\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 458305 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=177351 obj=11.6505 num_tokens=1094685 num_tokens/piece=6.17242\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=153060 obj=9.15223 num_tokens=1102520 num_tokens/piece=7.20319\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=114764 obj=9.12363 num_tokens=1141800 num_tokens/piece=9.94911\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=114652 obj=9.113 num_tokens=1147665 num_tokens/piece=10.01\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=85987 obj=9.16451 num_tokens=1206005 num_tokens/piece=14.0254\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=85984 obj=9.15341 num_tokens=1206645 num_tokens/piece=14.0334\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=64487 obj=9.22894 num_tokens=1273380 num_tokens/piece=19.7463\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=64487 obj=9.21453 num_tokens=1273546 num_tokens/piece=19.7489\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=48365 obj=9.31789 num_tokens=1345929 num_tokens/piece=27.8286\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=48365 obj=9.2989 num_tokens=1345914 num_tokens/piece=27.8283\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=36273 obj=9.43691 num_tokens=1421791 num_tokens/piece=39.197\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=36273 obj=9.41153 num_tokens=1421804 num_tokens/piece=39.1973\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=35200 obj=9.42603 num_tokens=1429694 num_tokens/piece=40.6163\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=35200 obj=9.42346 num_tokens=1429701 num_tokens/piece=40.6165\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ./spm-model/spm.en.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ./spm-model/spm.en.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=./kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=./spm-model/spm.ja --vocab_size=32000 --character_coverage=0.9995 --model_type=unigram'\n",
    ")\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=./kftt-data-1.0/data/orig/kyoto-train.en --model_prefix=./spm-model/spm.en --vocab_size=32000 --character_coverage=1.0 --model_type=unigram'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp_ja = spm.SentencePieceProcessor(model_file='./spm-model/spm.ja.model')\n",
    "sp_en = spm.SentencePieceProcessor(model_file='./spm-model/spm.en.model')\n",
    "\n",
    "def make_corpus(fname_unigram, fname_raw, model):\n",
    "    with open(fname_unigram, 'w') as f_uni, open(fname_raw, 'r') as f_raw:\n",
    "        for line in f_raw:\n",
    "            f_uni.write(' '.join(model.encode(line, out_type=str)))\n",
    "            f_uni.write('\\n')\n",
    "        \n",
    "make_corpus('./unigram-corpus/train.ja', './kftt-data-1.0/data/orig/kyoto-train.ja', sp_ja)\n",
    "make_corpus('./unigram-corpus/dev.ja', './kftt-data-1.0/data/orig/kyoto-dev.ja', sp_ja)\n",
    "make_corpus('./unigram-corpus/test.ja', './kftt-data-1.0/data/orig/kyoto-test.ja', sp_ja)\n",
    "make_corpus('./unigram-corpus/train.en', './kftt-data-1.0/data/orig/kyoto-train.en', sp_en)\n",
    "make_corpus('./unigram-corpus/dev.en', './kftt-data-1.0/data/orig/kyoto-dev.en', sp_en)\n",
    "make_corpus('./unigram-corpus/test.en', './kftt-data-1.0/data/orig/kyoto-test.en', sp_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-10 16:32:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-07-10 16:32:05 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='ja', target_lang='en', trainpref='./unigram-corpus/train', validpref='./unigram-corpus/dev', testpref='./unigram-corpus/test', align_suffix=None, destdir='data-spm-bin', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sugihara/anaconda3/envs/NLP100_2/bin/fairseq-preprocess\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/net/nas5/data/home/sugihara/workspace/NLP100_90~99/utils/fairseq/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
      "    main(args)\n",
      "  File \"/net/nas5/data/home/sugihara/workspace/NLP100_90~99/utils/fairseq/fairseq_cli/preprocess.py\", line 299, in main\n",
      "    raise FileExistsError(_dict_path(args.source_lang, args.destdir))\n",
      "FileExistsError: data-spm-bin/dict.ja.txt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-preprocess \\\n",
    "    --source-lang ja \\\n",
    "    --target-lang en \\\n",
    "    --trainpref ./unigram-corpus/train \\\n",
    "    --validpref ./unigram-corpus/dev \\\n",
    "    --testpref ./unigram-corpus/test \\\n",
    "    --destdir data-spm-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-10 17:33:31 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18116\n",
      "2023-07-10 17:33:31 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18116\n",
      "2023-07-10 17:33:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-07-10 17:33:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "2023-07-10 17:33:31 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "2023-07-10 17:33:31 | INFO | fairseq.distributed.utils | initialized host opus3 as rank 1\n",
      "2023-07-10 17:33:31 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "2023-07-10 17:33:31 | INFO | fairseq.distributed.utils | initialized host opus3 as rank 0\n",
      "2023-07-10 17:33:33 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18116', 'distributed_port': 18116, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-spm', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=100, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=2, distributed_num_procs=2, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-spm', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=100, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-spm-bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2000, warmup_init_lr=-1, pad=1, eos=2, unk=3, dropout=0.2, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 17:33:33 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 17:33:33 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 17:33:34 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(34208, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(32008, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=32008, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-07-10 17:33:34 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2023-07-10 17:33:34 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2023-07-10 17:33:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-07-10 17:33:34 | INFO | fairseq_cli.train | num. shared model params: 94,429,184 (num. trained: 94,429,184)\n",
      "2023-07-10 17:33:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-07-10 17:33:34 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-spm-bin/valid.ja-en.ja\n",
      "2023-07-10 17:33:34 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-spm-bin/valid.ja-en.en\n",
      "2023-07-10 17:33:34 | INFO | fairseq.tasks.translation | data-spm-bin valid ja-en 1166 examples\n",
      "2023-07-10 17:33:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-07-10 17:33:35 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "2023-07-10 17:33:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\n",
      "2023-07-10 17:33:35 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.651 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-10 17:33:35 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-10 17:33:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\n",
      "2023-07-10 17:33:35 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)\n",
      "2023-07-10 17:33:35 | INFO | fairseq_cli.train | max tokens per device = 4000 and max sentences per device = None\n",
      "2023-07-10 17:33:35 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:33:35 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:33:35 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-07-10 17:33:35 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-spm-bin/train.ja-en.ja\n",
      "2023-07-10 17:33:35 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-spm-bin/train.ja-en.en\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.translation | data-spm-bin train ja-en 440288 examples\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 17:33:35 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 17:33:35 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 17:33:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 001:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 17:33:40 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-07-10 17:33:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2023-07-10 17:33:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                             | 1/1832 [00:04<2:15:58,  4.46s/it]2023-07-10 17:33:44 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.\n",
      "epoch 001:   0%|                               | 2/1832 [00:04<58:16,  1.91s/it]/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "epoch 001:   0%|                               | 4/1832 [00:04<23:35,  1.29it/s]2023-07-10 17:33:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   5%|█▌                            | 95/1832 [00:14<03:05,  9.39it/s]2023-07-10 17:33:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   6%|█▌                           | 102/1832 [00:14<02:59,  9.62it/s]2023-07-10 17:33:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:33:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.54it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:33:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.788 | nll_loss 11.335 | ppl 2582.46 | wps 161873 | wpb 3509.2 | bsz 145 | num_updates 100\n",
      "2023-07-10 17:33:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 100 updates\n",
      "2023-07-10 17:33:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_100.pt\n",
      "2023-07-10 17:33:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_100.pt\n",
      "2023-07-10 17:34:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_100.pt (epoch 1 @ 100 updates, score 11.788) (writing took 10.525301506975666 seconds)\n",
      "epoch 001:  11%| | 202/1832 [00:36<02:46,  9.79it/s, loss=13.313, nll_loss=13.042023-07-10 17:34:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:34:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:34:16 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.647 | nll_loss 9.944 | ppl 984.77 | wps 159133 | wpb 3509.2 | bsz 145 | num_updates 200 | best_loss 10.647\n",
      "2023-07-10 17:34:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 200 updates\n",
      "2023-07-10 17:34:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_200.pt\n",
      "2023-07-10 17:34:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_200.pt\n",
      "2023-07-10 17:34:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 10.647) (writing took 11.028425568947569 seconds)\n",
      "epoch 001:  16%|▏| 302/1832 [00:57<02:40,  9.56it/s, loss=11.048, nll_loss=10.462023-07-10 17:34:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:34:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.87it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:34:38 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.215 | nll_loss 9.427 | ppl 688.52 | wps 158970 | wpb 3509.2 | bsz 145 | num_updates 300 | best_loss 10.215\n",
      "2023-07-10 17:34:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 300 updates\n",
      "2023-07-10 17:34:38 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_300.pt\n",
      "2023-07-10 17:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_300.pt\n",
      "2023-07-10 17:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_300.pt (epoch 1 @ 300 updates, score 10.215) (writing took 10.90985831595026 seconds)\n",
      "epoch 001:  22%|▏| 402/1832 [01:19<02:28,  9.63it/s, loss=10.196, nll_loss=9.4482023-07-10 17:34:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:34:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.09it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:34:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.776 | nll_loss 8.927 | ppl 486.67 | wps 156601 | wpb 3509.2 | bsz 145 | num_updates 400 | best_loss 9.776\n",
      "2023-07-10 17:34:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 400 updates\n",
      "2023-07-10 17:34:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_400.pt\n",
      "2023-07-10 17:35:03 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_400.pt\n",
      "2023-07-10 17:35:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_400.pt (epoch 1 @ 400 updates, score 9.776) (writing took 10.763876637909561 seconds)\n",
      "epoch 001:  27%|▎| 502/1832 [01:40<02:16,  9.77it/s, loss=9.775, nll_loss=8.96, 2023-07-10 17:35:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:35:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:35:20 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.479 | nll_loss 8.594 | ppl 386.31 | wps 159953 | wpb 3509.2 | bsz 145 | num_updates 500 | best_loss 9.479\n",
      "2023-07-10 17:35:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 500 updates\n",
      "2023-07-10 17:35:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_500.pt\n",
      "2023-07-10 17:35:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_500.pt\n",
      "2023-07-10 17:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_500.pt (epoch 1 @ 500 updates, score 9.479) (writing took 10.667383976979181 seconds)\n",
      "epoch 001:  33%|▎| 602/1832 [02:01<02:05,  9.80it/s, loss=9.398, nll_loss=8.527,2023-07-10 17:35:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:35:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.14it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:35:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.254 | nll_loss 8.315 | ppl 318.49 | wps 157645 | wpb 3509.2 | bsz 145 | num_updates 600 | best_loss 9.254\n",
      "2023-07-10 17:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 600 updates\n",
      "2023-07-10 17:35:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_600.pt\n",
      "2023-07-10 17:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_600.pt\n",
      "2023-07-10 17:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_600.pt (epoch 1 @ 600 updates, score 9.254) (writing took 10.720475716982037 seconds)\n",
      "epoch 001:  38%|▍| 702/1832 [02:23<01:57,  9.61it/s, loss=9.134, nll_loss=8.221,2023-07-10 17:36:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:36:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.43it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:36:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.036 | nll_loss 8.091 | ppl 272.73 | wps 160192 | wpb 3509.2 | bsz 145 | num_updates 700 | best_loss 9.036\n",
      "2023-07-10 17:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 700 updates\n",
      "2023-07-10 17:36:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_700.pt\n",
      "2023-07-10 17:36:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_700.pt\n",
      "2023-07-10 17:36:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_700.pt (epoch 1 @ 700 updates, score 9.036) (writing took 11.209353444864973 seconds)\n",
      "epoch 001:  44%|▍| 802/1832 [02:44<01:48,  9.46it/s, loss=8.883, nll_loss=7.929,2023-07-10 17:36:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:36:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.30it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:36:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.838 | nll_loss 7.831 | ppl 227.65 | wps 150187 | wpb 3509.2 | bsz 145 | num_updates 800 | best_loss 8.838\n",
      "2023-07-10 17:36:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 800 updates\n",
      "2023-07-10 17:36:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_800.pt\n",
      "2023-07-10 17:36:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_800.pt\n",
      "2023-07-10 17:36:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_800.pt (epoch 1 @ 800 updates, score 8.838) (writing took 11.287682255962864 seconds)\n",
      "epoch 001:  49%|▍| 902/1832 [03:06<01:36,  9.59it/s, loss=8.634, nll_loss=7.64, 2023-07-10 17:36:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:36:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:36:47 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.667 | nll_loss 7.628 | ppl 197.87 | wps 158022 | wpb 3509.2 | bsz 145 | num_updates 900 | best_loss 8.667\n",
      "2023-07-10 17:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 900 updates\n",
      "2023-07-10 17:36:47 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_900.pt\n",
      "2023-07-10 17:36:50 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_900.pt\n",
      "2023-07-10 17:36:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_900.pt (epoch 1 @ 900 updates, score 8.667) (writing took 10.663767733843997 seconds)\n",
      "epoch 001:  55%|▌| 1002/1832 [03:28<01:25,  9.68it/s, loss=8.503, nll_loss=7.4862023-07-10 17:37:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:37:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:37:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.575 | nll_loss 7.532 | ppl 185.04 | wps 159661 | wpb 3509.2 | bsz 145 | num_updates 1000 | best_loss 8.575\n",
      "2023-07-10 17:37:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates\n",
      "2023-07-10 17:37:08 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1000.pt\n",
      "2023-07-10 17:37:11 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1000.pt\n",
      "2023-07-10 17:37:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 8.575) (writing took 10.65742418100126 seconds)\n",
      "epoch 001:  60%|▌| 1102/1832 [03:49<01:18,  9.30it/s, loss=8.303, nll_loss=7.2532023-07-10 17:37:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:37:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.67it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:37:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.363 | nll_loss 7.273 | ppl 154.72 | wps 153428 | wpb 3509.2 | bsz 145 | num_updates 1100 | best_loss 8.363\n",
      "2023-07-10 17:37:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1100 updates\n",
      "2023-07-10 17:37:29 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1100.pt\n",
      "2023-07-10 17:37:33 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1100.pt\n",
      "2023-07-10 17:37:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1100.pt (epoch 1 @ 1100 updates, score 8.363) (writing took 10.902055635815486 seconds)\n",
      "epoch 001:  66%|▋| 1201/1832 [04:10<01:05,  9.70it/s, loss=8.234, nll_loss=7.1712023-07-10 17:37:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:37:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.41it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:37:51 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.299 | nll_loss 7.179 | ppl 144.94 | wps 154228 | wpb 3509.2 | bsz 145 | num_updates 1200 | best_loss 8.299\n",
      "2023-07-10 17:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1200 updates\n",
      "2023-07-10 17:37:51 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1200.pt\n",
      "2023-07-10 17:37:55 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1200.pt\n",
      "2023-07-10 17:38:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1200.pt (epoch 1 @ 1200 updates, score 8.299) (writing took 10.918814546894282 seconds)\n",
      "epoch 001:  71%|▋| 1302/1832 [04:32<00:54,  9.81it/s, loss=8.116, nll_loss=7.0332023-07-10 17:38:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:38:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.69it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:38:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.162 | nll_loss 7.046 | ppl 132.1 | wps 154123 | wpb 3509.2 | bsz 145 | num_updates 1300 | best_loss 8.162\n",
      "2023-07-10 17:38:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1300 updates\n",
      "2023-07-10 17:38:12 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1300.pt\n",
      "2023-07-10 17:38:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1300.pt\n",
      "2023-07-10 17:38:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1300.pt (epoch 1 @ 1300 updates, score 8.162) (writing took 10.80559570598416 seconds)\n",
      "epoch 001:  77%|▊| 1402/1832 [04:53<00:44,  9.73it/s, loss=8.022, nll_loss=6.9232023-07-10 17:38:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:38:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.41it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:38:34 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.057 | nll_loss 6.909 | ppl 120.18 | wps 158984 | wpb 3509.2 | bsz 145 | num_updates 1400 | best_loss 8.057\n",
      "2023-07-10 17:38:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1400 updates\n",
      "2023-07-10 17:38:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1400.pt\n",
      "2023-07-10 17:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1400.pt\n",
      "2023-07-10 17:38:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1400.pt (epoch 1 @ 1400 updates, score 8.057) (writing took 10.888670686865225 seconds)\n",
      "epoch 001:  82%|▊| 1502/1832 [05:15<00:34,  9.60it/s, loss=7.914, nll_loss=6.7982023-07-10 17:38:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:38:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:38:56 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.996 | nll_loss 6.83 | ppl 113.78 | wps 156531 | wpb 3509.2 | bsz 145 | num_updates 1500 | best_loss 7.996\n",
      "2023-07-10 17:38:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1500 updates\n",
      "2023-07-10 17:38:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1500.pt\n",
      "2023-07-10 17:38:59 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1500.pt\n",
      "2023-07-10 17:39:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1500.pt (epoch 1 @ 1500 updates, score 7.996) (writing took 10.954618689138442 seconds)\n",
      "epoch 001:  87%|▊| 1602/1832 [05:37<00:23,  9.80it/s, loss=7.831, nll_loss=6.7022023-07-10 17:39:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:39:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.07it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:39:18 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.921 | nll_loss 6.74 | ppl 106.91 | wps 161372 | wpb 3509.2 | bsz 145 | num_updates 1600 | best_loss 7.921\n",
      "2023-07-10 17:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1600 updates\n",
      "2023-07-10 17:39:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1600.pt\n",
      "2023-07-10 17:39:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1600.pt\n",
      "2023-07-10 17:39:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1600.pt (epoch 1 @ 1600 updates, score 7.921) (writing took 10.971646653022617 seconds)\n",
      "epoch 001:  93%|▉| 1701/1832 [05:59<00:13,  9.72it/s, loss=7.727, nll_loss=6.5812023-07-10 17:39:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:39:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:39:40 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.889 | nll_loss 6.698 | ppl 103.86 | wps 159443 | wpb 3509.2 | bsz 145 | num_updates 1700 | best_loss 7.889\n",
      "2023-07-10 17:39:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1700 updates\n",
      "2023-07-10 17:39:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1700.pt\n",
      "2023-07-10 17:39:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1700.pt\n",
      "2023-07-10 17:39:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1700.pt (epoch 1 @ 1700 updates, score 7.889) (writing took 12.032850180985406 seconds)\n",
      "epoch 001:  98%|▉| 1802/1832 [06:22<00:03,  9.67it/s, loss=7.667, nll_loss=6.5112023-07-10 17:40:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:40:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|████    | 4/8 [00:00<00:00, 32.78it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:40:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.818 | nll_loss 6.639 | ppl 99.64 | wps 158179 | wpb 3509.2 | bsz 145 | num_updates 1800 | best_loss 7.818\n",
      "2023-07-10 17:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1800 updates\n",
      "2023-07-10 17:40:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1800.pt\n",
      "2023-07-10 17:40:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_1_1800.pt\n",
      "2023-07-10 17:40:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_1_1800.pt (epoch 1 @ 1800 updates, score 7.818) (writing took 11.37677392992191 seconds)\n",
      "epoch 001: 100%|▉| 1831/1832 [06:37<00:00,  9.80it/s, loss=7.687, nll_loss=6.5342023-07-10 17:40:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:40:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:40:18 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.877 | nll_loss 6.681 | ppl 102.61 | wps 158888 | wpb 3509.2 | bsz 145 | num_updates 1829 | best_loss 7.818\n",
      "2023-07-10 17:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1829 updates\n",
      "2023-07-10 17:40:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:40:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:40:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_last.pt (epoch 1 @ 1829 updates, score 7.877) (writing took 3.8870802170131356 seconds)\n",
      "2023-07-10 17:40:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-07-10 17:40:22 | INFO | train | epoch 001 | loss 8.888 | nll_loss 7.932 | ppl 244.13 | wps 33156.4 | ups 4.6 | wpb 7207.9 | bsz 239.2 | num_updates 1829 | lr 0.0009145 | gnorm 1.274 | clip 59.7 | loss_scale 16 | train_wall 185 | gb_free 20 | wall 407\n",
      "2023-07-10 17:40:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 17:40:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 002:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 17:40:22 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-07-10 17:40:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:   4%|█▏                            | 70/1832 [00:07<03:03,  9.58it/s]2023-07-10 17:40:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:40:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.52it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:40:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.743 | nll_loss 6.55 | ppl 93.67 | wps 159674 | wpb 3509.2 | bsz 145 | num_updates 1900 | best_loss 7.743\n",
      "2023-07-10 17:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1900 updates\n",
      "2023-07-10 17:40:29 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_1900.pt\n",
      "2023-07-10 17:40:34 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_1900.pt\n",
      "2023-07-10 17:40:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_1900.pt (epoch 2 @ 1900 updates, score 7.743) (writing took 12.095136379124597 seconds)\n",
      "epoch 002:   9%| | 170/1832 [00:30<02:52,  9.64it/s, loss=7.617, nll_loss=6.455,2023-07-10 17:40:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:40:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.41it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:40:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.726 | nll_loss 6.508 | ppl 91 | wps 156796 | wpb 3509.2 | bsz 145 | num_updates 2000 | best_loss 7.726\n",
      "2023-07-10 17:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates\n",
      "2023-07-10 17:40:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2000.pt\n",
      "2023-07-10 17:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2000.pt\n",
      "2023-07-10 17:41:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 7.726) (writing took 11.052865260047838 seconds)\n",
      "epoch 002:  15%|▏| 270/1832 [00:51<02:40,  9.75it/s, loss=7.454, nll_loss=6.268,2023-07-10 17:41:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:41:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.83it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:41:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.651 | nll_loss 6.443 | ppl 87.02 | wps 160945 | wpb 3509.2 | bsz 145 | num_updates 2100 | best_loss 7.651\n",
      "2023-07-10 17:41:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2100 updates\n",
      "2023-07-10 17:41:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2100.pt\n",
      "2023-07-10 17:41:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2100.pt\n",
      "2023-07-10 17:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2100.pt (epoch 2 @ 2100 updates, score 7.651) (writing took 11.034102469915524 seconds)\n",
      "epoch 002:  18%|▏| 338/1832 [01:10<02:32,  9.80it/s, loss=7.448, nll_loss=6.26, 2023-07-10 17:41:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 002:  20%|▏| 371/1832 [01:14<02:31,  9.64it/s, loss=7.448, nll_loss=6.26, 2023-07-10 17:41:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:41:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.45it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:41:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.594 | nll_loss 6.365 | ppl 82.44 | wps 159894 | wpb 3509.2 | bsz 145 | num_updates 2200 | best_loss 7.594\n",
      "2023-07-10 17:41:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2200 updates\n",
      "2023-07-10 17:41:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2200.pt\n",
      "2023-07-10 17:41:40 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2200.pt\n",
      "2023-07-10 17:41:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2200.pt (epoch 2 @ 2200 updates, score 7.594) (writing took 10.911572242155671 seconds)\n",
      "epoch 002:  26%|▎| 471/1832 [01:35<02:19,  9.74it/s, loss=7.412, nll_loss=6.22, 2023-07-10 17:41:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:41:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.70it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:41:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.523 | nll_loss 6.283 | ppl 77.87 | wps 161643 | wpb 3509.2 | bsz 145 | num_updates 2300 | best_loss 7.523\n",
      "2023-07-10 17:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2300 updates\n",
      "2023-07-10 17:41:58 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2300.pt\n",
      "2023-07-10 17:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2300.pt\n",
      "2023-07-10 17:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2300.pt (epoch 2 @ 2300 updates, score 7.523) (writing took 10.683033134089783 seconds)\n",
      "epoch 002:  31%|▎| 570/1832 [01:57<02:16,  9.24it/s, loss=7.298, nll_loss=6.091,2023-07-10 17:42:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:42:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.52it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:42:19 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.471 | nll_loss 6.235 | ppl 75.34 | wps 161703 | wpb 3509.2 | bsz 145 | num_updates 2400 | best_loss 7.471\n",
      "2023-07-10 17:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2400 updates\n",
      "2023-07-10 17:42:19 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2400.pt\n",
      "2023-07-10 17:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2400.pt\n",
      "2023-07-10 17:42:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2400.pt (epoch 2 @ 2400 updates, score 7.471) (writing took 10.781844178913161 seconds)\n",
      "epoch 002:  37%|▎| 671/1832 [02:19<01:58,  9.83it/s, loss=7.266, nll_loss=6.054,2023-07-10 17:42:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:42:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.32it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:42:41 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.453 | nll_loss 6.207 | ppl 73.87 | wps 161276 | wpb 3509.2 | bsz 145 | num_updates 2500 | best_loss 7.453\n",
      "2023-07-10 17:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2500 updates\n",
      "2023-07-10 17:42:41 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2500.pt\n",
      "2023-07-10 17:42:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2500.pt\n",
      "2023-07-10 17:42:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2500.pt (epoch 2 @ 2500 updates, score 7.453) (writing took 11.295465249102563 seconds)\n",
      "epoch 002:  42%|▍| 771/1832 [02:40<01:49,  9.70it/s, loss=7.199, nll_loss=5.978,2023-07-10 17:43:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:43:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.04it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:43:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.432 | nll_loss 6.175 | ppl 72.23 | wps 156856 | wpb 3509.2 | bsz 145 | num_updates 2600 | best_loss 7.432\n",
      "2023-07-10 17:43:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2600 updates\n",
      "2023-07-10 17:43:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2600.pt\n",
      "2023-07-10 17:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2600.pt\n",
      "2023-07-10 17:43:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2600.pt (epoch 2 @ 2600 updates, score 7.432) (writing took 11.608178884955123 seconds)\n",
      "epoch 002:  48%|▍| 871/1832 [03:03<01:37,  9.90it/s, loss=7.144, nll_loss=5.916,2023-07-10 17:43:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:43:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.74it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:43:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.357 | nll_loss 6.105 | ppl 68.83 | wps 158753 | wpb 3509.2 | bsz 145 | num_updates 2700 | best_loss 7.357\n",
      "2023-07-10 17:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2700 updates\n",
      "2023-07-10 17:43:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2700.pt\n",
      "2023-07-10 17:43:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2700.pt\n",
      "2023-07-10 17:43:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2700.pt (epoch 2 @ 2700 updates, score 7.357) (writing took 11.016382127068937 seconds)\n",
      "epoch 002:  53%|▌| 971/1832 [03:24<01:30,  9.48it/s, loss=7.149, nll_loss=5.922,2023-07-10 17:43:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:43:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.60it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.52it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:43:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.364 | nll_loss 6.102 | ppl 68.67 | wps 148086 | wpb 3509.2 | bsz 145 | num_updates 2800 | best_loss 7.357\n",
      "2023-07-10 17:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2800 updates\n",
      "2023-07-10 17:43:47 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2800.pt\n",
      "2023-07-10 17:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2800.pt\n",
      "2023-07-10 17:43:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2800.pt (epoch 2 @ 2800 updates, score 7.364) (writing took 8.458157679066062 seconds)\n",
      "epoch 002:  58%|▌| 1070/1832 [03:44<01:17,  9.79it/s, loss=7.066, nll_loss=5.8292023-07-10 17:44:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:44:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  50%|████    | 4/8 [00:00<00:00, 33.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:44:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.281 | nll_loss 6.002 | ppl 64.1 | wps 158138 | wpb 3509.2 | bsz 145 | num_updates 2900 | best_loss 7.281\n",
      "2023-07-10 17:44:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2900 updates\n",
      "2023-07-10 17:44:06 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2900.pt\n",
      "2023-07-10 17:44:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_2900.pt\n",
      "2023-07-10 17:44:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_2900.pt (epoch 2 @ 2900 updates, score 7.281) (writing took 12.721153421094641 seconds)\n",
      "epoch 002:  64%|▋| 1171/1832 [04:07<01:09,  9.56it/s, loss=7.048, nll_loss=5.8082023-07-10 17:44:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:44:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.04it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:44:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.251 | nll_loss 5.974 | ppl 62.85 | wps 154660 | wpb 3509.2 | bsz 145 | num_updates 3000 | best_loss 7.251\n",
      "2023-07-10 17:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3000 updates\n",
      "2023-07-10 17:44:30 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3000.pt\n",
      "2023-07-10 17:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3000.pt\n",
      "2023-07-10 17:44:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3000.pt (epoch 2 @ 3000 updates, score 7.251) (writing took 12.715409680036828 seconds)\n",
      "epoch 002:  69%|▋| 1271/1832 [04:31<00:56,  9.85it/s, loss=7.025, nll_loss=5.7832023-07-10 17:44:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:44:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.23it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:44:53 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.225 | nll_loss 5.95 | ppl 61.8 | wps 155158 | wpb 3509.2 | bsz 145 | num_updates 3100 | best_loss 7.225\n",
      "2023-07-10 17:44:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3100 updates\n",
      "2023-07-10 17:44:53 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3100.pt\n",
      "2023-07-10 17:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3100.pt\n",
      "2023-07-10 17:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3100.pt (epoch 2 @ 3100 updates, score 7.225) (writing took 10.763275394914672 seconds)\n",
      "epoch 002:  75%|▋| 1370/1832 [04:52<00:46, 10.03it/s, loss=6.987, nll_loss=5.7392023-07-10 17:45:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:45:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.52it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:45:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.226 | nll_loss 5.935 | ppl 61.18 | wps 158782 | wpb 3509.2 | bsz 145 | num_updates 3200 | best_loss 7.225\n",
      "2023-07-10 17:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3200 updates\n",
      "2023-07-10 17:45:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3200.pt\n",
      "2023-07-10 17:45:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3200.pt\n",
      "2023-07-10 17:45:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3200.pt (epoch 2 @ 3200 updates, score 7.226) (writing took 8.168071595020592 seconds)\n",
      "epoch 002:  80%|▊| 1470/1832 [05:11<00:37,  9.74it/s, loss=6.885, nll_loss=5.6232023-07-10 17:45:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:45:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.43it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:45:34 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.196 | nll_loss 5.916 | ppl 60.38 | wps 158271 | wpb 3509.2 | bsz 145 | num_updates 3300 | best_loss 7.196\n",
      "2023-07-10 17:45:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3300 updates\n",
      "2023-07-10 17:45:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3300.pt\n",
      "2023-07-10 17:45:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3300.pt\n",
      "2023-07-10 17:45:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3300.pt (epoch 2 @ 3300 updates, score 7.196) (writing took 11.153797795064747 seconds)\n",
      "epoch 002:  86%|▊| 1571/1832 [05:33<00:26,  9.79it/s, loss=6.901, nll_loss=5.6422023-07-10 17:45:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:45:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 24.42it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.36it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:45:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.172 | nll_loss 5.886 | ppl 59.15 | wps 148121 | wpb 3509.2 | bsz 145 | num_updates 3400 | best_loss 7.172\n",
      "2023-07-10 17:45:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3400 updates\n",
      "2023-07-10 17:45:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3400.pt\n",
      "2023-07-10 17:45:59 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3400.pt\n",
      "2023-07-10 17:46:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3400.pt (epoch 2 @ 3400 updates, score 7.172) (writing took 10.522773936856538 seconds)\n",
      "epoch 002:  91%|▉| 1671/1832 [05:55<00:16,  9.62it/s, loss=6.882, nll_loss=5.6222023-07-10 17:46:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:46:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.33it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:46:17 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.104 | nll_loss 5.817 | ppl 56.38 | wps 156384 | wpb 3509.2 | bsz 145 | num_updates 3500 | best_loss 7.104\n",
      "2023-07-10 17:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3500 updates\n",
      "2023-07-10 17:46:17 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3500.pt\n",
      "2023-07-10 17:46:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3500.pt\n",
      "2023-07-10 17:46:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3500.pt (epoch 2 @ 3500 updates, score 7.104) (writing took 10.863972947001457 seconds)\n",
      "epoch 002:  97%|▉| 1771/1832 [06:17<00:06,  9.72it/s, loss=6.775, nll_loss=5.4982023-07-10 17:46:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:46:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:46:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.151 | nll_loss 5.871 | ppl 58.52 | wps 160172 | wpb 3509.2 | bsz 145 | num_updates 3600 | best_loss 7.104\n",
      "2023-07-10 17:46:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3600 updates\n",
      "2023-07-10 17:46:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3600.pt\n",
      "2023-07-10 17:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_2_3600.pt\n",
      "2023-07-10 17:46:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_2_3600.pt (epoch 2 @ 3600 updates, score 7.151) (writing took 7.608711099019274 seconds)\n",
      "epoch 002: 100%|▉| 1830/1832 [06:31<00:00,  9.87it/s, loss=6.848, nll_loss=5.5832023-07-10 17:46:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:46:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.82it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:46:54 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.08 | nll_loss 5.779 | ppl 54.91 | wps 161367 | wpb 3509.2 | bsz 145 | num_updates 3660 | best_loss 7.08\n",
      "2023-07-10 17:46:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3660 updates\n",
      "2023-07-10 17:46:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 17:46:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 17:47:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_best.pt (epoch 2 @ 3660 updates, score 7.08) (writing took 8.238057975890115 seconds)\n",
      "2023-07-10 17:47:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-07-10 17:47:02 | INFO | train | epoch 002 | loss 7.116 | nll_loss 5.885 | ppl 59.09 | wps 32957.9 | ups 4.57 | wpb 7206.9 | bsz 239.3 | num_updates 3660 | lr 0.000739221 | gnorm 0.72 | clip 6.1 | loss_scale 8 | train_wall 184 | gb_free 19.8 | wall 807\n",
      "2023-07-10 17:47:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 17:47:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 003:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 17:47:02 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-07-10 17:47:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:   2%|▋                             | 39/1832 [00:04<03:06,  9.61it/s]2023-07-10 17:47:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:47:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.17it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:47:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.084 | nll_loss 5.784 | ppl 55.1 | wps 158358 | wpb 3509.2 | bsz 145 | num_updates 3700 | best_loss 7.08\n",
      "2023-07-10 17:47:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3700 updates\n",
      "2023-07-10 17:47:06 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_3700.pt\n",
      "2023-07-10 17:47:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_3700.pt\n",
      "2023-07-10 17:47:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_3700.pt (epoch 3 @ 3700 updates, score 7.084) (writing took 7.8994388149585575 seconds)\n",
      "epoch 003:   8%| | 139/1832 [00:22<02:52,  9.81it/s, loss=6.756, nll_loss=5.478,2023-07-10 17:47:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:47:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.01it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:47:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.07 | nll_loss 5.762 | ppl 54.27 | wps 155297 | wpb 3509.2 | bsz 145 | num_updates 3800 | best_loss 7.07\n",
      "2023-07-10 17:47:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3800 updates\n",
      "2023-07-10 17:47:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_3800.pt\n",
      "2023-07-10 17:47:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_3800.pt\n",
      "2023-07-10 17:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_3800.pt (epoch 3 @ 3800 updates, score 7.07) (writing took 10.56421243515797 seconds)\n",
      "epoch 003:  13%|▏| 239/1832 [00:43<02:45,  9.63it/s, loss=6.612, nll_loss=5.312,2023-07-10 17:47:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:47:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.21it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:47:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.083 | nll_loss 5.773 | ppl 54.67 | wps 150167 | wpb 3509.2 | bsz 145 | num_updates 3900 | best_loss 7.07\n",
      "2023-07-10 17:47:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3900 updates\n",
      "2023-07-10 17:47:46 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_3900.pt\n",
      "2023-07-10 17:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_3900.pt\n",
      "2023-07-10 17:47:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_3900.pt (epoch 3 @ 3900 updates, score 7.083) (writing took 8.127587888855487 seconds)\n",
      "epoch 003:  19%|▏| 339/1832 [01:02<02:36,  9.57it/s, loss=6.601, nll_loss=5.298,2023-07-10 17:48:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:48:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:48:05 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.039 | nll_loss 5.728 | ppl 53.01 | wps 158305 | wpb 3509.2 | bsz 145 | num_updates 4000 | best_loss 7.039\n",
      "2023-07-10 17:48:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates\n",
      "2023-07-10 17:48:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4000.pt\n",
      "2023-07-10 17:48:09 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4000.pt\n",
      "2023-07-10 17:48:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 7.039) (writing took 11.171845020027831 seconds)\n",
      "epoch 003:  24%|▏| 439/1832 [01:24<02:25,  9.57it/s, loss=6.613, nll_loss=5.311,2023-07-10 17:48:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:48:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:48:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.07 | nll_loss 5.751 | ppl 53.84 | wps 158748 | wpb 3509.2 | bsz 145 | num_updates 4100 | best_loss 7.039\n",
      "2023-07-10 17:48:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4100 updates\n",
      "2023-07-10 17:48:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4100.pt\n",
      "2023-07-10 17:48:30 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4100.pt\n",
      "2023-07-10 17:48:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4100.pt (epoch 3 @ 4100 updates, score 7.07) (writing took 7.798644926864654 seconds)\n",
      "epoch 003:  29%|▎| 539/1832 [01:43<02:14,  9.60it/s, loss=6.612, nll_loss=5.31, 2023-07-10 17:48:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:48:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.31it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.68it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:48:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.009 | nll_loss 5.685 | ppl 51.46 | wps 156786 | wpb 3509.2 | bsz 145 | num_updates 4200 | best_loss 7.009\n",
      "2023-07-10 17:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4200 updates\n",
      "2023-07-10 17:48:46 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4200.pt\n",
      "2023-07-10 17:48:49 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4200.pt\n",
      "2023-07-10 17:48:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4200.pt (epoch 3 @ 4200 updates, score 7.009) (writing took 10.885934501886368 seconds)\n",
      "epoch 003:  35%|▎| 639/1832 [02:04<02:02,  9.72it/s, loss=6.49, nll_loss=5.172, 2023-07-10 17:49:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:49:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.32it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:49:07 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.008 | nll_loss 5.701 | ppl 52.01 | wps 155091 | wpb 3509.2 | bsz 145 | num_updates 4300 | best_loss 7.008\n",
      "2023-07-10 17:49:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4300 updates\n",
      "2023-07-10 17:49:07 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4300.pt\n",
      "2023-07-10 17:49:11 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4300.pt\n",
      "2023-07-10 17:49:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4300.pt (epoch 3 @ 4300 updates, score 7.008) (writing took 10.735867704963312 seconds)\n",
      "epoch 003:  40%|▍| 739/1832 [02:26<01:54,  9.58it/s, loss=6.614, nll_loss=5.313,2023-07-10 17:49:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:49:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  25%|██      | 2/8 [00:00<00:00, 19.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 35.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:49:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.985 | nll_loss 5.669 | ppl 50.88 | wps 156851 | wpb 3509.2 | bsz 145 | num_updates 4400 | best_loss 6.985\n",
      "2023-07-10 17:49:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4400 updates\n",
      "2023-07-10 17:49:29 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4400.pt\n",
      "2023-07-10 17:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4400.pt\n",
      "2023-07-10 17:49:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4400.pt (epoch 3 @ 4400 updates, score 6.985) (writing took 14.4123018309474 seconds)\n",
      "epoch 003:  46%|▍| 839/1832 [02:51<01:42,  9.71it/s, loss=6.53, nll_loss=5.216, 2023-07-10 17:49:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:49:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  25%|██      | 2/8 [00:00<00:00, 19.19it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 34.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:49:54 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.965 | nll_loss 5.646 | ppl 50.06 | wps 152621 | wpb 3509.2 | bsz 145 | num_updates 4500 | best_loss 6.965\n",
      "2023-07-10 17:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4500 updates\n",
      "2023-07-10 17:49:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4500.pt\n",
      "2023-07-10 17:49:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4500.pt\n",
      "2023-07-10 17:50:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4500.pt (epoch 3 @ 4500 updates, score 6.965) (writing took 11.028155675856397 seconds)\n",
      "epoch 003:  51%|▌| 938/1832 [03:13<01:31,  9.78it/s, loss=6.602, nll_loss=5.299,2023-07-10 17:50:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:50:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  25%|██      | 2/8 [00:00<00:00, 16.93it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 33.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:50:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.039 | nll_loss 5.719 | ppl 52.68 | wps 162478 | wpb 3509.2 | bsz 145 | num_updates 4600 | best_loss 6.965\n",
      "2023-07-10 17:50:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4600 updates\n",
      "2023-07-10 17:50:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4600.pt\n",
      "2023-07-10 17:50:19 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4600.pt\n",
      "2023-07-10 17:50:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4600.pt (epoch 3 @ 4600 updates, score 7.039) (writing took 7.853011030005291 seconds)\n",
      "epoch 003:  57%|▌| 1038/1832 [03:32<01:22,  9.60it/s, loss=6.626, nll_loss=5.3262023-07-10 17:50:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:50:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.46it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:50:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.924 | nll_loss 5.589 | ppl 48.14 | wps 157959 | wpb 3509.2 | bsz 145 | num_updates 4700 | best_loss 6.924\n",
      "2023-07-10 17:50:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4700 updates\n",
      "2023-07-10 17:50:35 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4700.pt\n",
      "2023-07-10 17:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4700.pt\n",
      "2023-07-10 17:50:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4700.pt (epoch 3 @ 4700 updates, score 6.924) (writing took 10.91114342212677 seconds)\n",
      "epoch 003:  62%|▌| 1139/1832 [03:54<01:10,  9.86it/s, loss=6.496, nll_loss=5.1782023-07-10 17:50:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:50:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.40it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:50:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.948 | nll_loss 5.619 | ppl 49.13 | wps 155020 | wpb 3509.2 | bsz 145 | num_updates 4800 | best_loss 6.924\n",
      "2023-07-10 17:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4800 updates\n",
      "2023-07-10 17:50:57 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4800.pt\n",
      "2023-07-10 17:51:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4800.pt\n",
      "2023-07-10 17:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4800.pt (epoch 3 @ 4800 updates, score 6.948) (writing took 7.853786513907835 seconds)\n",
      "epoch 003:  68%|▋| 1239/1832 [04:12<01:01,  9.58it/s, loss=6.506, nll_loss=5.19,2023-07-10 17:51:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:51:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.24it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:51:15 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.9 | nll_loss 5.575 | ppl 47.68 | wps 152605 | wpb 3509.2 | bsz 145 | num_updates 4900 | best_loss 6.9\n",
      "2023-07-10 17:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4900 updates\n",
      "2023-07-10 17:51:15 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4900.pt\n",
      "2023-07-10 17:51:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_4900.pt\n",
      "2023-07-10 17:51:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_4900.pt (epoch 3 @ 4900 updates, score 6.9) (writing took 10.536177690140903 seconds)\n",
      "epoch 003:  73%|▋| 1339/1832 [04:33<00:50,  9.80it/s, loss=6.475, nll_loss=5.1542023-07-10 17:51:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:51:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.52it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.78it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:51:36 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.896 | nll_loss 5.57 | ppl 47.49 | wps 151159 | wpb 3509.2 | bsz 145 | num_updates 5000 | best_loss 6.896\n",
      "2023-07-10 17:51:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5000 updates\n",
      "2023-07-10 17:51:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5000.pt\n",
      "2023-07-10 17:51:40 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5000.pt\n",
      "2023-07-10 17:51:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_5000.pt (epoch 3 @ 5000 updates, score 6.896) (writing took 10.813602163922042 seconds)\n",
      "epoch 003:  79%|▊| 1439/1832 [04:55<00:40,  9.60it/s, loss=6.471, nll_loss=5.15,2023-07-10 17:51:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:51:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.87it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:51:58 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.884 | nll_loss 5.557 | ppl 47.08 | wps 155046 | wpb 3509.2 | bsz 145 | num_updates 5100 | best_loss 6.884\n",
      "2023-07-10 17:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5100 updates\n",
      "2023-07-10 17:51:58 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5100.pt\n",
      "2023-07-10 17:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5100.pt\n",
      "2023-07-10 17:52:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_5100.pt (epoch 3 @ 5100 updates, score 6.884) (writing took 10.704077888047323 seconds)\n",
      "epoch 003:  84%|▊| 1539/1832 [05:17<00:30,  9.57it/s, loss=6.471, nll_loss=5.1512023-07-10 17:52:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:52:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.82it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:52:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.859 | nll_loss 5.513 | ppl 45.67 | wps 154223 | wpb 3509.2 | bsz 145 | num_updates 5200 | best_loss 6.859\n",
      "2023-07-10 17:52:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5200 updates\n",
      "2023-07-10 17:52:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5200.pt\n",
      "2023-07-10 17:52:23 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5200.pt\n",
      "2023-07-10 17:52:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_5200.pt (epoch 3 @ 5200 updates, score 6.859) (writing took 10.758386295987293 seconds)\n",
      "epoch 003:  89%|▉| 1639/1832 [05:38<00:19,  9.67it/s, loss=6.392, nll_loss=5.0612023-07-10 17:52:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:52:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.71it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:52:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.807 | nll_loss 5.478 | ppl 44.56 | wps 158788 | wpb 3509.2 | bsz 145 | num_updates 5300 | best_loss 6.807\n",
      "2023-07-10 17:52:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5300 updates\n",
      "2023-07-10 17:52:41 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5300.pt\n",
      "2023-07-10 17:52:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5300.pt\n",
      "2023-07-10 17:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_5300.pt (epoch 3 @ 5300 updates, score 6.807) (writing took 11.519881488056853 seconds)\n",
      "epoch 003:  95%|▉| 1739/1832 [06:00<00:09,  9.73it/s, loss=6.427, nll_loss=5.1, 2023-07-10 17:53:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:53:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.55it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:53:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.837 | nll_loss 5.499 | ppl 45.21 | wps 153385 | wpb 3509.2 | bsz 145 | num_updates 5400 | best_loss 6.807\n",
      "2023-07-10 17:53:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5400 updates\n",
      "2023-07-10 17:53:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5400.pt\n",
      "2023-07-10 17:53:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_3_5400.pt\n",
      "2023-07-10 17:53:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_3_5400.pt (epoch 3 @ 5400 updates, score 6.837) (writing took 7.707605717005208 seconds)\n",
      "epoch 003: 100%|▉| 1830/1832 [06:18<00:00,  9.92it/s, loss=6.488, nll_loss=5.17,2023-07-10 17:53:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:53:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.84it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:53:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.812 | nll_loss 5.469 | ppl 44.3 | wps 153094 | wpb 3509.2 | bsz 145 | num_updates 5492 | best_loss 6.807\n",
      "2023-07-10 17:53:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5492 updates\n",
      "2023-07-10 17:53:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:53:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_last.pt (epoch 3 @ 5492 updates, score 6.812) (writing took 4.124969286844134 seconds)\n",
      "2023-07-10 17:53:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-07-10 17:53:25 | INFO | train | epoch 003 | loss 6.526 | nll_loss 5.213 | ppl 37.09 | wps 34497 | ups 4.79 | wpb 7207.3 | bsz 240.3 | num_updates 5492 | lr 0.000603462 | gnorm 0.639 | clip 2.1 | loss_scale 8 | train_wall 185 | gb_free 19.9 | wall 1190\n",
      "2023-07-10 17:53:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 17:53:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 004:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 17:53:25 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-07-10 17:53:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:   0%|                               | 6/1832 [00:00<03:10,  9.59it/s]2023-07-10 17:53:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:53:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:53:26 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.845 | nll_loss 5.49 | ppl 44.94 | wps 159366 | wpb 3509.2 | bsz 145 | num_updates 5500 | best_loss 6.807\n",
      "2023-07-10 17:53:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5500 updates\n",
      "2023-07-10 17:53:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5500.pt\n",
      "2023-07-10 17:53:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5500.pt\n",
      "2023-07-10 17:53:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_5500.pt (epoch 4 @ 5500 updates, score 6.845) (writing took 7.724978565936908 seconds)\n",
      "epoch 004:   6%| | 107/1832 [00:19<03:01,  9.52it/s, loss=6.396, nll_loss=5.066,2023-07-10 17:53:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:53:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.47it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:53:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.812 | nll_loss 5.462 | ppl 44.07 | wps 160139 | wpb 3509.2 | bsz 145 | num_updates 5600 | best_loss 6.807\n",
      "2023-07-10 17:53:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5600 updates\n",
      "2023-07-10 17:53:44 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5600.pt\n",
      "2023-07-10 17:53:48 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5600.pt\n",
      "2023-07-10 17:53:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_5600.pt (epoch 4 @ 5600 updates, score 6.812) (writing took 8.050712199881673 seconds)\n",
      "epoch 004:  11%| | 207/1832 [00:37<02:47,  9.69it/s, loss=6.158, nll_loss=4.794,2023-07-10 17:54:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:54:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.19it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:54:03 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.862 | nll_loss 5.524 | ppl 46.02 | wps 158601 | wpb 3509.2 | bsz 145 | num_updates 5700 | best_loss 6.807\n",
      "2023-07-10 17:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5700 updates\n",
      "2023-07-10 17:54:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5700.pt\n",
      "2023-07-10 17:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5700.pt\n",
      "2023-07-10 17:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_5700.pt (epoch 4 @ 5700 updates, score 6.862) (writing took 7.943528745090589 seconds)\n",
      "epoch 004:  17%|▏| 307/1832 [00:56<02:35,  9.78it/s, loss=6.245, nll_loss=4.891,2023-07-10 17:54:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:54:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:54:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.839 | nll_loss 5.49 | ppl 44.94 | wps 159679 | wpb 3509.2 | bsz 145 | num_updates 5800 | best_loss 6.807\n",
      "2023-07-10 17:54:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5800 updates\n",
      "2023-07-10 17:54:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5800.pt\n",
      "2023-07-10 17:54:26 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5800.pt\n",
      "2023-07-10 17:54:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_5800.pt (epoch 4 @ 5800 updates, score 6.839) (writing took 7.63464331603609 seconds)\n",
      "epoch 004:  22%|▏| 407/1832 [01:15<02:26,  9.71it/s, loss=6.197, nll_loss=4.835,2023-07-10 17:54:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:54:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:54:40 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.822 | nll_loss 5.479 | ppl 44.59 | wps 157941 | wpb 3509.2 | bsz 145 | num_updates 5900 | best_loss 6.807\n",
      "2023-07-10 17:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5900 updates\n",
      "2023-07-10 17:54:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5900.pt\n",
      "2023-07-10 17:54:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_5900.pt\n",
      "2023-07-10 17:54:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_5900.pt (epoch 4 @ 5900 updates, score 6.822) (writing took 7.754951837006956 seconds)\n",
      "epoch 004:  28%|▎| 507/1832 [01:33<02:16,  9.70it/s, loss=6.299, nll_loss=4.951,2023-07-10 17:54:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:54:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.03it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:54:59 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.844 | nll_loss 5.49 | ppl 44.94 | wps 159214 | wpb 3509.2 | bsz 145 | num_updates 6000 | best_loss 6.807\n",
      "2023-07-10 17:54:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6000 updates\n",
      "2023-07-10 17:54:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6000.pt\n",
      "2023-07-10 17:55:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6000.pt\n",
      "2023-07-10 17:55:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6000.pt (epoch 4 @ 6000 updates, score 6.844) (writing took 8.095065526897088 seconds)\n",
      "epoch 004:  33%|▎| 607/1832 [01:52<02:05,  9.76it/s, loss=6.235, nll_loss=4.879,2023-07-10 17:55:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:55:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.78it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:55:18 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.808 | nll_loss 5.454 | ppl 43.83 | wps 155022 | wpb 3509.2 | bsz 145 | num_updates 6100 | best_loss 6.807\n",
      "2023-07-10 17:55:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6100 updates\n",
      "2023-07-10 17:55:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6100.pt\n",
      "2023-07-10 17:55:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6100.pt\n",
      "2023-07-10 17:55:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6100.pt (epoch 4 @ 6100 updates, score 6.808) (writing took 7.904023956041783 seconds)\n",
      "epoch 004:  39%|▍| 707/1832 [02:11<01:57,  9.56it/s, loss=6.216, nll_loss=4.856,2023-07-10 17:55:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:55:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.61it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:55:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.86 | nll_loss 5.527 | ppl 46.1 | wps 151908 | wpb 3509.2 | bsz 145 | num_updates 6200 | best_loss 6.807\n",
      "2023-07-10 17:55:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6200 updates\n",
      "2023-07-10 17:55:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6200.pt\n",
      "2023-07-10 17:55:40 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6200.pt\n",
      "2023-07-10 17:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6200.pt (epoch 4 @ 6200 updates, score 6.86) (writing took 7.668359369970858 seconds)\n",
      "epoch 004:  44%|▍| 807/1832 [02:29<01:49,  9.36it/s, loss=6.194, nll_loss=4.831,2023-07-10 17:55:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:55:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.75it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:55:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.768 | nll_loss 5.416 | ppl 42.71 | wps 157348 | wpb 3509.2 | bsz 145 | num_updates 6300 | best_loss 6.768\n",
      "2023-07-10 17:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6300 updates\n",
      "2023-07-10 17:55:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6300.pt\n",
      "2023-07-10 17:55:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6300.pt\n",
      "2023-07-10 17:56:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6300.pt (epoch 4 @ 6300 updates, score 6.768) (writing took 10.672593024093658 seconds)\n",
      "epoch 004:  50%|▍| 907/1832 [02:50<01:34,  9.78it/s, loss=6.224, nll_loss=4.865,2023-07-10 17:56:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:56:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.05it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:56:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.791 | nll_loss 5.434 | ppl 43.23 | wps 159345 | wpb 3509.2 | bsz 145 | num_updates 6400 | best_loss 6.768\n",
      "2023-07-10 17:56:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6400 updates\n",
      "2023-07-10 17:56:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6400.pt\n",
      "2023-07-10 17:56:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6400.pt\n",
      "2023-07-10 17:56:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6400.pt (epoch 4 @ 6400 updates, score 6.791) (writing took 11.009266966953874 seconds)\n",
      "epoch 004:  55%|▌| 1007/1832 [03:13<01:28,  9.28it/s, loss=6.231, nll_loss=4.8742023-07-10 17:56:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:56:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.72it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.92it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:56:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.767 | nll_loss 5.415 | ppl 42.68 | wps 159978 | wpb 3509.2 | bsz 145 | num_updates 6500 | best_loss 6.767\n",
      "2023-07-10 17:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6500 updates\n",
      "2023-07-10 17:56:38 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6500.pt\n",
      "2023-07-10 17:56:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6500.pt\n",
      "2023-07-10 17:56:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6500.pt (epoch 4 @ 6500 updates, score 6.767) (writing took 11.21642087888904 seconds)\n",
      "epoch 004:  60%|▌| 1107/1832 [03:35<01:13,  9.89it/s, loss=6.218, nll_loss=4.8582023-07-10 17:57:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:57:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.84it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:57:00 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.726 | nll_loss 5.367 | ppl 41.28 | wps 154306 | wpb 3509.2 | bsz 145 | num_updates 6600 | best_loss 6.726\n",
      "2023-07-10 17:57:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6600 updates\n",
      "2023-07-10 17:57:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6600.pt\n",
      "2023-07-10 17:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6600.pt\n",
      "2023-07-10 17:57:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6600.pt (epoch 4 @ 6600 updates, score 6.726) (writing took 10.991507002152503 seconds)\n",
      "epoch 004:  66%|▋| 1207/1832 [03:57<01:06,  9.46it/s, loss=6.215, nll_loss=4.8552023-07-10 17:57:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:57:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.41it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:57:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.711 | nll_loss 5.363 | ppl 41.15 | wps 150568 | wpb 3509.2 | bsz 145 | num_updates 6700 | best_loss 6.711\n",
      "2023-07-10 17:57:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6700 updates\n",
      "2023-07-10 17:57:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6700.pt\n",
      "2023-07-10 17:57:26 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6700.pt\n",
      "2023-07-10 17:57:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6700.pt (epoch 4 @ 6700 updates, score 6.711) (writing took 10.857567192986608 seconds)\n",
      "epoch 004:  71%|▋| 1306/1832 [04:18<00:54,  9.61it/s, loss=6.147, nll_loss=4.7782023-07-10 17:57:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:57:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.42it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:57:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.711 | nll_loss 5.358 | ppl 41.02 | wps 155308 | wpb 3509.2 | bsz 145 | num_updates 6800 | best_loss 6.711\n",
      "2023-07-10 17:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6800 updates\n",
      "2023-07-10 17:57:44 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6800.pt\n",
      "2023-07-10 17:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6800.pt\n",
      "2023-07-10 17:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6800.pt (epoch 4 @ 6800 updates, score 6.711) (writing took 10.571827275911346 seconds)\n",
      "epoch 004:  77%|▊| 1406/1832 [04:39<00:42,  9.95it/s, loss=6.202, nll_loss=4.84,2023-07-10 17:58:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:58:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.71it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:58:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.701 | nll_loss 5.334 | ppl 40.33 | wps 157696 | wpb 3509.2 | bsz 145 | num_updates 6900 | best_loss 6.701\n",
      "2023-07-10 17:58:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6900 updates\n",
      "2023-07-10 17:58:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6900.pt\n",
      "2023-07-10 17:58:09 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_6900.pt\n",
      "2023-07-10 17:58:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_6900.pt (epoch 4 @ 6900 updates, score 6.701) (writing took 10.82783400407061 seconds)\n",
      "epoch 004:  82%|▊| 1507/1832 [05:01<00:33,  9.76it/s, loss=6.226, nll_loss=4.8682023-07-10 17:58:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:58:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.95it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:58:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.714 | nll_loss 5.34 | ppl 40.5 | wps 157963 | wpb 3509.2 | bsz 145 | num_updates 7000 | best_loss 6.701\n",
      "2023-07-10 17:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7000 updates\n",
      "2023-07-10 17:58:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7000.pt\n",
      "2023-07-10 17:58:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7000.pt\n",
      "2023-07-10 17:58:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_7000.pt (epoch 4 @ 7000 updates, score 6.714) (writing took 8.097157117212191 seconds)\n",
      "epoch 004:  88%|▉| 1607/1832 [05:20<00:23,  9.65it/s, loss=6.183, nll_loss=4.82,2023-07-10 17:58:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:58:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.10it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:58:45 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.698 | nll_loss 5.335 | ppl 40.37 | wps 156189 | wpb 3509.2 | bsz 145 | num_updates 7100 | best_loss 6.698\n",
      "2023-07-10 17:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7100 updates\n",
      "2023-07-10 17:58:45 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7100.pt\n",
      "2023-07-10 17:58:49 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7100.pt\n",
      "2023-07-10 17:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_7100.pt (epoch 4 @ 7100 updates, score 6.698) (writing took 10.631557144923136 seconds)\n",
      "epoch 004:  93%|▉| 1707/1832 [05:41<00:13,  9.56it/s, loss=6.235, nll_loss=4.8792023-07-10 17:59:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:59:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.87it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.36it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:59:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.744 | nll_loss 5.371 | ppl 41.39 | wps 155348 | wpb 3509.2 | bsz 145 | num_updates 7200 | best_loss 6.698\n",
      "2023-07-10 17:59:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7200 updates\n",
      "2023-07-10 17:59:07 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7200.pt\n",
      "2023-07-10 17:59:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7200.pt\n",
      "2023-07-10 17:59:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_7200.pt (epoch 4 @ 7200 updates, score 6.744) (writing took 8.109829544089735 seconds)\n",
      "epoch 004:  99%|▉| 1807/1832 [06:00<00:02,  9.44it/s, loss=6.162, nll_loss=4.7952023-07-10 17:59:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:59:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.88it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:59:26 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.677 | nll_loss 5.313 | ppl 39.75 | wps 160357 | wpb 3509.2 | bsz 145 | num_updates 7300 | best_loss 6.677\n",
      "2023-07-10 17:59:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7300 updates\n",
      "2023-07-10 17:59:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7300.pt\n",
      "2023-07-10 17:59:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_4_7300.pt\n",
      "2023-07-10 17:59:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_4_7300.pt (epoch 4 @ 7300 updates, score 6.677) (writing took 10.518007086822763 seconds)\n",
      "epoch 004: 100%|▉| 1830/1832 [06:13<00:00,  9.67it/s, loss=6.074, nll_loss=4.6962023-07-10 17:59:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:59:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.82it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:59:39 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.682 | nll_loss 5.318 | ppl 39.9 | wps 148869 | wpb 3509.2 | bsz 145 | num_updates 7324 | best_loss 6.677\n",
      "2023-07-10 17:59:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7324 updates\n",
      "2023-07-10 17:59:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:59:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 17:59:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_last.pt (epoch 4 @ 7324 updates, score 6.682) (writing took 3.782087483908981 seconds)\n",
      "2023-07-10 17:59:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-07-10 17:59:43 | INFO | train | epoch 004 | loss 6.205 | nll_loss 4.844 | ppl 28.72 | wps 34942.1 | ups 4.85 | wpb 7207.3 | bsz 240.3 | num_updates 7324 | lr 0.000522566 | gnorm 0.655 | clip 2.7 | loss_scale 8 | train_wall 185 | gb_free 19.8 | wall 1568\n",
      "2023-07-10 17:59:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 17:59:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 005:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 17:59:43 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-07-10 17:59:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:   4%|█▏                            | 75/1832 [00:07<03:03,  9.58it/s]2023-07-10 17:59:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 17:59:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|████    | 4/8 [00:00<00:00, 33.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 17:59:51 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.695 | nll_loss 5.313 | ppl 39.76 | wps 158422 | wpb 3509.2 | bsz 145 | num_updates 7400 | best_loss 6.677\n",
      "2023-07-10 17:59:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7400 updates\n",
      "2023-07-10 17:59:51 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7400.pt\n",
      "2023-07-10 17:59:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7400.pt\n",
      "2023-07-10 17:59:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_7400.pt (epoch 5 @ 7400 updates, score 6.695) (writing took 7.873819977045059 seconds)\n",
      "epoch 005:  10%| | 175/1832 [00:26<02:51,  9.69it/s, loss=5.982, nll_loss=4.589,2023-07-10 18:00:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:00:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.15it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.30it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:00:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.683 | nll_loss 5.31 | ppl 39.68 | wps 157756 | wpb 3509.2 | bsz 145 | num_updates 7500 | best_loss 6.677\n",
      "2023-07-10 18:00:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7500 updates\n",
      "2023-07-10 18:00:09 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7500.pt\n",
      "2023-07-10 18:00:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7500.pt\n",
      "2023-07-10 18:00:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_7500.pt (epoch 5 @ 7500 updates, score 6.683) (writing took 7.861566585022956 seconds)\n",
      "epoch 005:  15%|▏| 275/1832 [00:44<02:43,  9.55it/s, loss=5.934, nll_loss=4.533,2023-07-10 18:00:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:00:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:00:28 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.676 | nll_loss 5.306 | ppl 39.57 | wps 151338 | wpb 3509.2 | bsz 145 | num_updates 7600 | best_loss 6.676\n",
      "2023-07-10 18:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7600 updates\n",
      "2023-07-10 18:00:28 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7600.pt\n",
      "2023-07-10 18:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7600.pt\n",
      "2023-07-10 18:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_7600.pt (epoch 5 @ 7600 updates, score 6.676) (writing took 11.25766202295199 seconds)\n",
      "epoch 005:  20%|▏| 375/1832 [01:06<02:33,  9.46it/s, loss=5.999, nll_loss=4.606,2023-07-10 18:00:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:00:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  12%|█       | 1/8 [00:00<00:00,  9.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 6/8 [00:00<00:00, 30.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:00:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.69 | nll_loss 5.308 | ppl 39.62 | wps 157384 | wpb 3509.2 | bsz 145 | num_updates 7700 | best_loss 6.676\n",
      "2023-07-10 18:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7700 updates\n",
      "2023-07-10 18:00:50 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7700.pt\n",
      "2023-07-10 18:00:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7700.pt\n",
      "2023-07-10 18:00:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_7700.pt (epoch 5 @ 7700 updates, score 6.69) (writing took 8.5542753781192 seconds)\n",
      "epoch 005:  26%|▎| 475/1832 [01:26<02:19,  9.70it/s, loss=6.008, nll_loss=4.616,2023-07-10 18:01:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:01:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.55it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.23it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:01:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.664 | nll_loss 5.276 | ppl 38.76 | wps 152612 | wpb 3509.2 | bsz 145 | num_updates 7800 | best_loss 6.664\n",
      "2023-07-10 18:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7800 updates\n",
      "2023-07-10 18:01:09 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7800.pt\n",
      "2023-07-10 18:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7800.pt\n",
      "2023-07-10 18:01:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_7800.pt (epoch 5 @ 7800 updates, score 6.664) (writing took 10.623950456967577 seconds)\n",
      "epoch 005:  31%|▎| 575/1832 [01:47<02:09,  9.72it/s, loss=6.014, nll_loss=4.623,2023-07-10 18:01:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:01:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.96it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.15it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:01:31 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.65 | nll_loss 5.267 | ppl 38.51 | wps 159413 | wpb 3509.2 | bsz 145 | num_updates 7900 | best_loss 6.65\n",
      "2023-07-10 18:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7900 updates\n",
      "2023-07-10 18:01:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7900.pt\n",
      "2023-07-10 18:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_7900.pt\n",
      "2023-07-10 18:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_7900.pt (epoch 5 @ 7900 updates, score 6.65) (writing took 10.814569770125672 seconds)\n",
      "epoch 005:  37%|▎| 675/1832 [02:08<01:57,  9.81it/s, loss=6.024, nll_loss=4.634,2023-07-10 18:01:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:01:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.81it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:01:52 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.658 | nll_loss 5.276 | ppl 38.76 | wps 156405 | wpb 3509.2 | bsz 145 | num_updates 8000 | best_loss 6.65\n",
      "2023-07-10 18:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8000 updates\n",
      "2023-07-10 18:01:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8000.pt\n",
      "2023-07-10 18:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8000.pt\n",
      "2023-07-10 18:02:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8000.pt (epoch 5 @ 8000 updates, score 6.658) (writing took 8.030352493049577 seconds)\n",
      "epoch 005:  42%|▍| 775/1832 [02:27<01:49,  9.68it/s, loss=5.93, nll_loss=4.527, 2023-07-10 18:02:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:02:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.32it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 33.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:02:11 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.64 | nll_loss 5.259 | ppl 38.3 | wps 140451 | wpb 3509.2 | bsz 145 | num_updates 8100 | best_loss 6.64\n",
      "2023-07-10 18:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8100 updates\n",
      "2023-07-10 18:02:11 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8100.pt\n",
      "2023-07-10 18:02:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8100.pt\n",
      "2023-07-10 18:02:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8100.pt (epoch 5 @ 8100 updates, score 6.64) (writing took 12.4846083719749 seconds)\n",
      "epoch 005:  48%|▍| 875/1832 [02:51<01:41,  9.42it/s, loss=6.008, nll_loss=4.616,2023-07-10 18:02:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:02:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.50it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:02:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.714 | nll_loss 5.348 | ppl 40.72 | wps 156110 | wpb 3509.2 | bsz 145 | num_updates 8200 | best_loss 6.64\n",
      "2023-07-10 18:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8200 updates\n",
      "2023-07-10 18:02:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8200.pt\n",
      "2023-07-10 18:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8200.pt\n",
      "2023-07-10 18:02:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8200.pt (epoch 5 @ 8200 updates, score 6.714) (writing took 7.527303555049002 seconds)\n",
      "epoch 005:  53%|▌| 975/1832 [03:09<01:29,  9.54it/s, loss=6.016, nll_loss=4.625,2023-07-10 18:02:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:02:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.28it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:02:52 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.639 | nll_loss 5.261 | ppl 38.34 | wps 150391 | wpb 3509.2 | bsz 145 | num_updates 8300 | best_loss 6.639\n",
      "2023-07-10 18:02:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8300 updates\n",
      "2023-07-10 18:02:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8300.pt\n",
      "2023-07-10 18:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8300.pt\n",
      "2023-07-10 18:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8300.pt (epoch 5 @ 8300 updates, score 6.639) (writing took 10.77850389899686 seconds)\n",
      "epoch 005:  59%|▌| 1075/1832 [03:30<01:19,  9.46it/s, loss=5.953, nll_loss=4.5542023-07-10 18:03:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:03:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:03:14 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.645 | nll_loss 5.268 | ppl 38.54 | wps 158088 | wpb 3509.2 | bsz 145 | num_updates 8400 | best_loss 6.639\n",
      "2023-07-10 18:03:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8400 updates\n",
      "2023-07-10 18:03:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8400.pt\n",
      "2023-07-10 18:03:17 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8400.pt\n",
      "2023-07-10 18:03:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8400.pt (epoch 5 @ 8400 updates, score 6.645) (writing took 8.000977241899818 seconds)\n",
      "epoch 005:  64%|▋| 1175/1832 [03:49<01:08,  9.64it/s, loss=5.94, nll_loss=4.539,2023-07-10 18:03:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:03:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.66it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:03:32 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.625 | nll_loss 5.252 | ppl 38.11 | wps 159837 | wpb 3509.2 | bsz 145 | num_updates 8500 | best_loss 6.625\n",
      "2023-07-10 18:03:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8500 updates\n",
      "2023-07-10 18:03:32 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8500.pt\n",
      "2023-07-10 18:03:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8500.pt\n",
      "2023-07-10 18:03:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8500.pt (epoch 5 @ 8500 updates, score 6.625) (writing took 10.952480281004682 seconds)\n",
      "epoch 005:  70%|▋| 1275/1832 [04:11<00:58,  9.56it/s, loss=6.025, nll_loss=4.6362023-07-10 18:03:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:03:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.83it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.53it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:03:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.624 | nll_loss 5.248 | ppl 37.99 | wps 156924 | wpb 3509.2 | bsz 145 | num_updates 8600 | best_loss 6.624\n",
      "2023-07-10 18:03:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8600 updates\n",
      "2023-07-10 18:03:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8600.pt\n",
      "2023-07-10 18:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8600.pt\n",
      "2023-07-10 18:04:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8600.pt (epoch 5 @ 8600 updates, score 6.624) (writing took 10.928502701222897 seconds)\n",
      "epoch 005:  75%|▊| 1374/1832 [04:32<00:48,  9.39it/s, loss=5.982, nll_loss=4.5872023-07-10 18:04:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:04:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.49it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:04:16 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.632 | nll_loss 5.252 | ppl 38.1 | wps 159835 | wpb 3509.2 | bsz 145 | num_updates 8700 | best_loss 6.624\n",
      "2023-07-10 18:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8700 updates\n",
      "2023-07-10 18:04:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8700.pt\n",
      "2023-07-10 18:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8700.pt\n",
      "2023-07-10 18:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8700.pt (epoch 5 @ 8700 updates, score 6.632) (writing took 7.734830241883174 seconds)\n",
      "epoch 005:  81%|▊| 1475/1832 [04:51<00:36,  9.69it/s, loss=6, nll_loss=4.608, pp2023-07-10 18:04:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:04:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.60it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:04:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.589 | nll_loss 5.21 | ppl 37.01 | wps 156093 | wpb 3509.2 | bsz 145 | num_updates 8800 | best_loss 6.589\n",
      "2023-07-10 18:04:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8800 updates\n",
      "2023-07-10 18:04:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8800.pt\n",
      "2023-07-10 18:04:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8800.pt\n",
      "2023-07-10 18:04:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8800.pt (epoch 5 @ 8800 updates, score 6.589) (writing took 11.200357438065112 seconds)\n",
      "epoch 005:  86%|▊| 1575/1832 [05:13<00:26,  9.70it/s, loss=5.967, nll_loss=4.5692023-07-10 18:04:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:04:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|████    | 4/8 [00:00<00:00, 33.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:04:56 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.575 | nll_loss 5.189 | ppl 36.49 | wps 158786 | wpb 3509.2 | bsz 145 | num_updates 8900 | best_loss 6.575\n",
      "2023-07-10 18:04:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8900 updates\n",
      "2023-07-10 18:04:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8900.pt\n",
      "2023-07-10 18:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_8900.pt\n",
      "2023-07-10 18:05:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_8900.pt (epoch 5 @ 8900 updates, score 6.575) (writing took 10.594735238002613 seconds)\n",
      "epoch 005:  91%|▉| 1675/1832 [05:34<00:16,  9.78it/s, loss=5.977, nll_loss=4.5822023-07-10 18:05:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:05:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.93it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:05:18 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.577 | nll_loss 5.2 | ppl 36.77 | wps 147906 | wpb 3509.2 | bsz 145 | num_updates 9000 | best_loss 6.575\n",
      "2023-07-10 18:05:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9000 updates\n",
      "2023-07-10 18:05:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_9000.pt\n",
      "2023-07-10 18:05:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_9000.pt\n",
      "2023-07-10 18:05:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_9000.pt (epoch 5 @ 9000 updates, score 6.577) (writing took 7.802447715075687 seconds)\n",
      "epoch 005:  97%|▉| 1775/1832 [05:53<00:05,  9.67it/s, loss=6.001, nll_loss=4.6092023-07-10 18:05:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:05:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.14it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:05:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.577 | nll_loss 5.184 | ppl 36.36 | wps 159405 | wpb 3509.2 | bsz 145 | num_updates 9100 | best_loss 6.575\n",
      "2023-07-10 18:05:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9100 updates\n",
      "2023-07-10 18:05:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_9100.pt\n",
      "2023-07-10 18:05:40 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_5_9100.pt\n",
      "2023-07-10 18:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_5_9100.pt (epoch 5 @ 9100 updates, score 6.577) (writing took 7.627814006991684 seconds)\n",
      "epoch 005: 100%|▉| 1831/1832 [06:06<00:00, 10.06it/s, loss=5.963, nll_loss=4.5662023-07-10 18:05:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:05:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.04it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:05:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.548 | nll_loss 5.163 | ppl 35.84 | wps 160966 | wpb 3509.2 | bsz 145 | num_updates 9156 | best_loss 6.548\n",
      "2023-07-10 18:05:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9156 updates\n",
      "2023-07-10 18:05:50 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:05:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_best.pt (epoch 5 @ 9156 updates, score 6.548) (writing took 8.246185780968517 seconds)\n",
      "2023-07-10 18:05:58 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-07-10 18:05:58 | INFO | train | epoch 005 | loss 5.98 | nll_loss 4.585 | ppl 24 | wps 35157.9 | ups 4.88 | wpb 7207.3 | bsz 240.3 | num_updates 9156 | lr 0.000467371 | gnorm 0.668 | clip 2.2 | loss_scale 8 | train_wall 184 | gb_free 20.1 | wall 1943\n",
      "2023-07-10 18:05:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:05:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 006:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 18:05:58 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-07-10 18:05:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:   2%|▋                             | 43/1832 [00:04<03:03,  9.73it/s]2023-07-10 18:06:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:06:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.93it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:06:03 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.553 | nll_loss 5.161 | ppl 35.77 | wps 155692 | wpb 3509.2 | bsz 145 | num_updates 9200 | best_loss 6.548\n",
      "2023-07-10 18:06:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9200 updates\n",
      "2023-07-10 18:06:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9200.pt\n",
      "2023-07-10 18:06:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9200.pt\n",
      "2023-07-10 18:06:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9200.pt (epoch 6 @ 9200 updates, score 6.553) (writing took 7.754871875979006 seconds)\n",
      "epoch 006:   8%| | 142/1832 [00:22<02:56,  9.56it/s, loss=5.899, nll_loss=4.493,2023-07-10 18:06:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:06:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.19it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.23it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:06:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.626 | nll_loss 5.243 | ppl 37.86 | wps 160815 | wpb 3509.2 | bsz 145 | num_updates 9300 | best_loss 6.548\n",
      "2023-07-10 18:06:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9300 updates\n",
      "2023-07-10 18:06:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9300.pt\n",
      "2023-07-10 18:06:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9300.pt\n",
      "2023-07-10 18:06:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9300.pt (epoch 6 @ 9300 updates, score 6.626) (writing took 7.842249940847978 seconds)\n",
      "epoch 006:  13%|▏| 242/1832 [00:41<02:43,  9.71it/s, loss=5.75, nll_loss=4.32, p2023-07-10 18:06:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:06:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.42it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:06:40 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.563 | nll_loss 5.173 | ppl 36.08 | wps 162014 | wpb 3509.2 | bsz 145 | num_updates 9400 | best_loss 6.548\n",
      "2023-07-10 18:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9400 updates\n",
      "2023-07-10 18:06:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9400.pt\n",
      "2023-07-10 18:06:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9400.pt\n",
      "2023-07-10 18:06:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9400.pt (epoch 6 @ 9400 updates, score 6.563) (writing took 7.761193518061191 seconds)\n",
      "epoch 006:  19%|▏| 343/1832 [01:00<02:31,  9.81it/s, loss=5.794, nll_loss=4.369,2023-07-10 18:06:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:06:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.08it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:06:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.543 | nll_loss 5.142 | ppl 35.32 | wps 160699 | wpb 3509.2 | bsz 145 | num_updates 9500 | best_loss 6.543\n",
      "2023-07-10 18:06:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9500 updates\n",
      "2023-07-10 18:06:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9500.pt\n",
      "2023-07-10 18:07:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9500.pt\n",
      "2023-07-10 18:07:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9500.pt (epoch 6 @ 9500 updates, score 6.543) (writing took 10.81318101589568 seconds)\n",
      "epoch 006:  24%|▏| 443/1832 [01:22<02:23,  9.67it/s, loss=5.747, nll_loss=4.315,2023-07-10 18:07:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:07:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.96it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:07:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.5 | nll_loss 5.101 | ppl 34.33 | wps 157881 | wpb 3509.2 | bsz 145 | num_updates 9600 | best_loss 6.5\n",
      "2023-07-10 18:07:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9600 updates\n",
      "2023-07-10 18:07:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9600.pt\n",
      "2023-07-10 18:07:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9600.pt\n",
      "2023-07-10 18:07:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9600.pt (epoch 6 @ 9600 updates, score 6.5) (writing took 10.514258976094425 seconds)\n",
      "epoch 006:  30%|▎| 542/1832 [01:43<02:16,  9.44it/s, loss=5.832, nll_loss=4.412,2023-07-10 18:07:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:07:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:07:42 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.525 | nll_loss 5.119 | ppl 34.76 | wps 161463 | wpb 3509.2 | bsz 145 | num_updates 9700 | best_loss 6.5\n",
      "2023-07-10 18:07:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9700 updates\n",
      "2023-07-10 18:07:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9700.pt\n",
      "2023-07-10 18:07:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9700.pt\n",
      "2023-07-10 18:07:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9700.pt (epoch 6 @ 9700 updates, score 6.525) (writing took 7.535570619860664 seconds)\n",
      "epoch 006:  35%|▎| 643/1832 [02:01<02:02,  9.67it/s, loss=5.737, nll_loss=4.303,2023-07-10 18:08:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:08:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.88it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.61it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:08:00 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.496 | nll_loss 5.096 | ppl 34.19 | wps 159555 | wpb 3509.2 | bsz 145 | num_updates 9800 | best_loss 6.496\n",
      "2023-07-10 18:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9800 updates\n",
      "2023-07-10 18:08:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9800.pt\n",
      "2023-07-10 18:08:03 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9800.pt\n",
      "2023-07-10 18:08:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9800.pt (epoch 6 @ 9800 updates, score 6.496) (writing took 11.076511526945978 seconds)\n",
      "epoch 006:  41%|▍| 743/1832 [02:23<01:53,  9.62it/s, loss=5.865, nll_loss=4.449,2023-07-10 18:08:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:08:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.67it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:08:22 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.441 | nll_loss 5.03 | ppl 32.66 | wps 160096 | wpb 3509.2 | bsz 145 | num_updates 9900 | best_loss 6.441\n",
      "2023-07-10 18:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9900 updates\n",
      "2023-07-10 18:08:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9900.pt\n",
      "2023-07-10 18:08:26 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_9900.pt\n",
      "2023-07-10 18:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_9900.pt (epoch 6 @ 9900 updates, score 6.441) (writing took 10.766423045890406 seconds)\n",
      "epoch 006:  46%|▍| 842/1832 [02:44<01:39,  9.92it/s, loss=5.799, nll_loss=4.375,2023-07-10 18:08:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:08:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.82it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:08:43 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.565 | nll_loss 5.178 | ppl 36.2 | wps 159774 | wpb 3509.2 | bsz 145 | num_updates 10000 | best_loss 6.441\n",
      "2023-07-10 18:08:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10000 updates\n",
      "2023-07-10 18:08:43 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10000.pt\n",
      "2023-07-10 18:08:47 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10000.pt\n",
      "2023-07-10 18:08:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10000.pt (epoch 6 @ 10000 updates, score 6.565) (writing took 8.119067318039015 seconds)\n",
      "epoch 006:  51%|▌| 943/1832 [03:03<01:31,  9.67it/s, loss=5.814, nll_loss=4.392,2023-07-10 18:09:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:09:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.42it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:09:02 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.423 | nll_loss 5.005 | ppl 32.11 | wps 155679 | wpb 3509.2 | bsz 145 | num_updates 10100 | best_loss 6.423\n",
      "2023-07-10 18:09:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10100 updates\n",
      "2023-07-10 18:09:02 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10100.pt\n",
      "2023-07-10 18:09:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10100.pt\n",
      "2023-07-10 18:09:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10100.pt (epoch 6 @ 10100 updates, score 6.423) (writing took 12.21525906585157 seconds)\n",
      "epoch 006:  57%|▌| 1043/1832 [03:26<01:20,  9.76it/s, loss=5.784, nll_loss=4.3572023-07-10 18:09:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:09:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:09:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.494 | nll_loss 5.085 | ppl 33.94 | wps 157985 | wpb 3509.2 | bsz 145 | num_updates 10200 | best_loss 6.423\n",
      "2023-07-10 18:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10200 updates\n",
      "2023-07-10 18:09:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10200.pt\n",
      "2023-07-10 18:09:30 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10200.pt\n",
      "2023-07-10 18:09:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10200.pt (epoch 6 @ 10200 updates, score 6.494) (writing took 12.310672639170662 seconds)\n",
      "epoch 006:  62%|▌| 1143/1832 [03:49<01:11,  9.63it/s, loss=5.794, nll_loss=4.37,2023-07-10 18:09:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:09:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.55it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:09:48 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.444 | nll_loss 5.03 | ppl 32.67 | wps 157288 | wpb 3509.2 | bsz 145 | num_updates 10300 | best_loss 6.423\n",
      "2023-07-10 18:09:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10300 updates\n",
      "2023-07-10 18:09:48 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10300.pt\n",
      "2023-07-10 18:09:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10300.pt\n",
      "2023-07-10 18:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10300.pt (epoch 6 @ 10300 updates, score 6.444) (writing took 7.765059510944411 seconds)\n",
      "epoch 006:  68%|▋| 1243/1832 [04:07<01:00,  9.73it/s, loss=5.76, nll_loss=4.33, 2023-07-10 18:10:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:10:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.44it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:10:06 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.374 | nll_loss 4.953 | ppl 30.98 | wps 159839 | wpb 3509.2 | bsz 145 | num_updates 10400 | best_loss 6.374\n",
      "2023-07-10 18:10:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10400 updates\n",
      "2023-07-10 18:10:06 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10400.pt\n",
      "2023-07-10 18:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10400.pt\n",
      "2023-07-10 18:10:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10400.pt (epoch 6 @ 10400 updates, score 6.374) (writing took 10.668926360085607 seconds)\n",
      "epoch 006:  73%|▋| 1343/1832 [04:29<00:51,  9.55it/s, loss=5.809, nll_loss=4.3872023-07-10 18:10:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:10:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.77it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:10:28 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.357 | nll_loss 4.929 | ppl 30.46 | wps 156188 | wpb 3509.2 | bsz 145 | num_updates 10500 | best_loss 6.357\n",
      "2023-07-10 18:10:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10500 updates\n",
      "2023-07-10 18:10:28 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10500.pt\n",
      "2023-07-10 18:10:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10500.pt\n",
      "2023-07-10 18:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10500.pt (epoch 6 @ 10500 updates, score 6.357) (writing took 10.981588925002143 seconds)\n",
      "epoch 006:  79%|▊| 1442/1832 [04:50<00:40,  9.55it/s, loss=5.828, nll_loss=4.4082023-07-10 18:10:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:10:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.54it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:10:49 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.336 | nll_loss 4.908 | ppl 30.01 | wps 158998 | wpb 3509.2 | bsz 145 | num_updates 10600 | best_loss 6.336\n",
      "2023-07-10 18:10:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10600 updates\n",
      "2023-07-10 18:10:49 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10600.pt\n",
      "2023-07-10 18:10:53 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10600.pt\n",
      "2023-07-10 18:11:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10600.pt (epoch 6 @ 10600 updates, score 6.336) (writing took 10.807408898836002 seconds)\n",
      "epoch 006:  84%|▊| 1543/1832 [05:12<00:29,  9.66it/s, loss=5.776, nll_loss=4.3492023-07-10 18:11:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:11:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.28it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:11:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.325 | nll_loss 4.897 | ppl 29.79 | wps 147339 | wpb 3509.2 | bsz 145 | num_updates 10700 | best_loss 6.325\n",
      "2023-07-10 18:11:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10700 updates\n",
      "2023-07-10 18:11:11 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10700.pt\n",
      "2023-07-10 18:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10700.pt\n",
      "2023-07-10 18:11:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10700.pt (epoch 6 @ 10700 updates, score 6.325) (writing took 10.737003294052556 seconds)\n",
      "epoch 006:  90%|▉| 1643/1832 [05:34<00:20,  9.44it/s, loss=5.825, nll_loss=4.4062023-07-10 18:11:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:11:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.89it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:11:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.327 | nll_loss 4.895 | ppl 29.76 | wps 152145 | wpb 3509.2 | bsz 145 | num_updates 10800 | best_loss 6.325\n",
      "2023-07-10 18:11:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10800 updates\n",
      "2023-07-10 18:11:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10800.pt\n",
      "2023-07-10 18:11:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10800.pt\n",
      "2023-07-10 18:11:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10800.pt (epoch 6 @ 10800 updates, score 6.327) (writing took 7.958568808156997 seconds)\n",
      "epoch 006:  95%|▉| 1743/1832 [05:52<00:09,  9.79it/s, loss=5.789, nll_loss=4.3652023-07-10 18:11:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:11:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:11:51 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.29 | nll_loss 4.86 | ppl 29.04 | wps 148768 | wpb 3509.2 | bsz 145 | num_updates 10900 | best_loss 6.29\n",
      "2023-07-10 18:11:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10900 updates\n",
      "2023-07-10 18:11:51 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10900.pt\n",
      "2023-07-10 18:11:55 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_6_10900.pt\n",
      "2023-07-10 18:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_6_10900.pt (epoch 6 @ 10900 updates, score 6.29) (writing took 10.684723180020228 seconds)\n",
      "epoch 006: 100%|▉| 1831/1832 [06:12<00:00,  9.98it/s, loss=5.815, nll_loss=4.3942023-07-10 18:12:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:12:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.01it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:12:12 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.304 | nll_loss 4.872 | ppl 29.28 | wps 158249 | wpb 3509.2 | bsz 145 | num_updates 10988 | best_loss 6.29\n",
      "2023-07-10 18:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10988 updates\n",
      "2023-07-10 18:12:12 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 18:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 18:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_last.pt (epoch 6 @ 10988 updates, score 6.304) (writing took 4.3244556740392 seconds)\n",
      "2023-07-10 18:12:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-07-10 18:12:16 | INFO | train | epoch 006 | loss 5.792 | nll_loss 4.368 | ppl 20.64 | wps 34952.7 | ups 4.85 | wpb 7207.3 | bsz 240.3 | num_updates 10988 | lr 0.000426634 | gnorm 0.696 | clip 2.5 | loss_scale 16 | train_wall 184 | gb_free 19.8 | wall 2321\n",
      "2023-07-10 18:12:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:12:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 007:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 18:12:16 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-07-10 18:12:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:   1%|▏                             | 11/1832 [00:01<03:05,  9.83it/s]2023-07-10 18:12:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:12:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.31it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:12:18 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.276 | nll_loss 4.836 | ppl 28.56 | wps 156915 | wpb 3509.2 | bsz 145 | num_updates 11000 | best_loss 6.276\n",
      "2023-07-10 18:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11000 updates\n",
      "2023-07-10 18:12:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11000.pt\n",
      "2023-07-10 18:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11000.pt\n",
      "2023-07-10 18:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11000.pt (epoch 7 @ 11000 updates, score 6.276) (writing took 10.511727798031643 seconds)\n",
      "epoch 007:   6%| | 111/1832 [00:22<02:57,  9.70it/s, loss=5.693, nll_loss=4.255,2023-07-10 18:12:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:12:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.88it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:12:39 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.266 | nll_loss 4.824 | ppl 28.32 | wps 159510 | wpb 3509.2 | bsz 145 | num_updates 11100 | best_loss 6.266\n",
      "2023-07-10 18:12:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11100 updates\n",
      "2023-07-10 18:12:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11100.pt\n",
      "2023-07-10 18:12:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11100.pt\n",
      "2023-07-10 18:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11100.pt (epoch 7 @ 11100 updates, score 6.266) (writing took 11.22189826797694 seconds)\n",
      "epoch 007:  12%| | 211/1832 [00:44<02:50,  9.51it/s, loss=5.528, nll_loss=4.065,2023-07-10 18:13:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:13:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.78it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.46it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:13:01 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.27 | nll_loss 4.828 | ppl 28.4 | wps 159041 | wpb 3509.2 | bsz 145 | num_updates 11200 | best_loss 6.266\n",
      "2023-07-10 18:13:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11200 updates\n",
      "2023-07-10 18:13:01 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11200.pt\n",
      "2023-07-10 18:13:04 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11200.pt\n",
      "2023-07-10 18:13:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11200.pt (epoch 7 @ 11200 updates, score 6.27) (writing took 7.771085150074214 seconds)\n",
      "epoch 007:  17%|▏| 311/1832 [01:02<02:38,  9.62it/s, loss=5.582, nll_loss=4.125,2023-07-10 18:13:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:13:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.44it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:13:19 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.283 | nll_loss 4.841 | ppl 28.66 | wps 158925 | wpb 3509.2 | bsz 145 | num_updates 11300 | best_loss 6.266\n",
      "2023-07-10 18:13:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11300 updates\n",
      "2023-07-10 18:13:19 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11300.pt\n",
      "2023-07-10 18:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11300.pt\n",
      "2023-07-10 18:13:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11300.pt (epoch 7 @ 11300 updates, score 6.283) (writing took 7.796484641963616 seconds)\n",
      "epoch 007:  22%|▏| 411/1832 [01:21<02:25,  9.74it/s, loss=5.61, nll_loss=4.157, 2023-07-10 18:13:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:13:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.58it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:13:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.265 | nll_loss 4.816 | ppl 28.17 | wps 144789 | wpb 3509.2 | bsz 145 | num_updates 11400 | best_loss 6.265\n",
      "2023-07-10 18:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11400 updates\n",
      "2023-07-10 18:13:38 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11400.pt\n",
      "2023-07-10 18:13:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11400.pt\n",
      "2023-07-10 18:13:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11400.pt (epoch 7 @ 11400 updates, score 6.265) (writing took 10.622867518104613 seconds)\n",
      "epoch 007:  28%|▎| 511/1832 [01:42<02:15,  9.73it/s, loss=5.564, nll_loss=4.104,2023-07-10 18:13:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:13:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.51it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:13:59 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.219 | nll_loss 4.77 | ppl 27.29 | wps 153979 | wpb 3509.2 | bsz 145 | num_updates 11500 | best_loss 6.219\n",
      "2023-07-10 18:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11500 updates\n",
      "2023-07-10 18:13:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11500.pt\n",
      "2023-07-10 18:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11500.pt\n",
      "2023-07-10 18:14:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11500.pt (epoch 7 @ 11500 updates, score 6.219) (writing took 11.05561386491172 seconds)\n",
      "epoch 007:  33%|▎| 611/1832 [02:04<02:09,  9.43it/s, loss=5.608, nll_loss=4.154,2023-07-10 18:14:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:14:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 24.76it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:14:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.216 | nll_loss 4.764 | ppl 27.17 | wps 154627 | wpb 3509.2 | bsz 145 | num_updates 11600 | best_loss 6.216\n",
      "2023-07-10 18:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11600 updates\n",
      "2023-07-10 18:14:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11600.pt\n",
      "2023-07-10 18:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11600.pt\n",
      "2023-07-10 18:14:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11600.pt (epoch 7 @ 11600 updates, score 6.216) (writing took 10.794662964064628 seconds)\n",
      "epoch 007:  39%|▍| 711/1832 [02:26<01:59,  9.35it/s, loss=5.622, nll_loss=4.17, 2023-07-10 18:14:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:14:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.56it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:14:43 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.17 | nll_loss 4.718 | ppl 26.31 | wps 157331 | wpb 3509.2 | bsz 145 | num_updates 11700 | best_loss 6.17\n",
      "2023-07-10 18:14:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11700 updates\n",
      "2023-07-10 18:14:43 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11700.pt\n",
      "2023-07-10 18:14:46 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11700.pt\n",
      "2023-07-10 18:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11700.pt (epoch 7 @ 11700 updates, score 6.17) (writing took 11.000677719945088 seconds)\n",
      "epoch 007:  44%|▍| 811/1832 [02:47<01:43,  9.85it/s, loss=5.603, nll_loss=4.149,2023-07-10 18:15:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:15:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.93it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:15:04 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.17 | nll_loss 4.709 | ppl 26.15 | wps 159313 | wpb 3509.2 | bsz 145 | num_updates 11800 | best_loss 6.17\n",
      "2023-07-10 18:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11800 updates\n",
      "2023-07-10 18:15:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11800.pt\n",
      "2023-07-10 18:15:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11800.pt\n",
      "2023-07-10 18:15:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11800.pt (epoch 7 @ 11800 updates, score 6.17) (writing took 10.948710076976568 seconds)\n",
      "epoch 007:  50%|▍| 911/1832 [03:09<01:35,  9.66it/s, loss=5.671, nll_loss=4.226,2023-07-10 18:15:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:15:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.95it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:15:26 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.169 | nll_loss 4.71 | ppl 26.17 | wps 159241 | wpb 3509.2 | bsz 145 | num_updates 11900 | best_loss 6.169\n",
      "2023-07-10 18:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 11900 updates\n",
      "2023-07-10 18:15:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11900.pt\n",
      "2023-07-10 18:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_11900.pt\n",
      "2023-07-10 18:15:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_11900.pt (epoch 7 @ 11900 updates, score 6.169) (writing took 10.466920177917928 seconds)\n",
      "epoch 007:  55%|▌| 1011/1832 [03:30<01:25,  9.63it/s, loss=5.612, nll_loss=4.1592023-07-10 18:15:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:15:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.86it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:15:47 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.144 | nll_loss 4.687 | ppl 25.76 | wps 157317 | wpb 3509.2 | bsz 145 | num_updates 12000 | best_loss 6.144\n",
      "2023-07-10 18:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12000 updates\n",
      "2023-07-10 18:15:47 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12000.pt\n",
      "2023-07-10 18:15:51 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12000.pt\n",
      "2023-07-10 18:15:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12000.pt (epoch 7 @ 12000 updates, score 6.144) (writing took 10.64349925913848 seconds)\n",
      "epoch 007:  61%|▌| 1111/1832 [03:52<01:17,  9.25it/s, loss=5.578, nll_loss=4.1212023-07-10 18:16:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:16:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  25%|██      | 2/8 [00:00<00:00, 19.61it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 34.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:16:09 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.143 | nll_loss 4.685 | ppl 25.72 | wps 154286 | wpb 3509.2 | bsz 145 | num_updates 12100 | best_loss 6.143\n",
      "2023-07-10 18:16:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12100 updates\n",
      "2023-07-10 18:16:09 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12100.pt\n",
      "2023-07-10 18:16:12 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12100.pt\n",
      "2023-07-10 18:16:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12100.pt (epoch 7 @ 12100 updates, score 6.143) (writing took 10.771107585867867 seconds)\n",
      "epoch 007:  66%|▋| 1211/1832 [04:13<01:03,  9.82it/s, loss=5.566, nll_loss=4.1062023-07-10 18:16:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:16:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.32it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.90it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:16:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.134 | nll_loss 4.674 | ppl 25.52 | wps 160992 | wpb 3509.2 | bsz 145 | num_updates 12200 | best_loss 6.134\n",
      "2023-07-10 18:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12200 updates\n",
      "2023-07-10 18:16:30 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12200.pt\n",
      "2023-07-10 18:16:34 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12200.pt\n",
      "2023-07-10 18:16:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12200.pt (epoch 7 @ 12200 updates, score 6.134) (writing took 11.170001666992903 seconds)\n",
      "epoch 007:  72%|▋| 1310/1832 [04:35<00:52,  9.91it/s, loss=5.629, nll_loss=4.1792023-07-10 18:16:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:16:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|████    | 4/8 [00:00<00:00, 33.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:16:52 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.137 | nll_loss 4.673 | ppl 25.51 | wps 158991 | wpb 3509.2 | bsz 145 | num_updates 12300 | best_loss 6.134\n",
      "2023-07-10 18:16:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12300 updates\n",
      "2023-07-10 18:16:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12300.pt\n",
      "2023-07-10 18:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12300.pt\n",
      "2023-07-10 18:17:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12300.pt (epoch 7 @ 12300 updates, score 6.137) (writing took 7.7517070539761335 seconds)\n",
      "epoch 007:  77%|▊| 1410/1832 [04:53<00:43,  9.78it/s, loss=5.544, nll_loss=4.0832023-07-10 18:17:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:17:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.70it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:17:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.117 | nll_loss 4.658 | ppl 25.25 | wps 159501 | wpb 3509.2 | bsz 145 | num_updates 12400 | best_loss 6.117\n",
      "2023-07-10 18:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12400 updates\n",
      "2023-07-10 18:17:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12400.pt\n",
      "2023-07-10 18:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12400.pt\n",
      "2023-07-10 18:17:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12400.pt (epoch 7 @ 12400 updates, score 6.117) (writing took 13.018426271155477 seconds)\n",
      "epoch 007:  82%|▊| 1511/1832 [05:17<00:32,  9.77it/s, loss=5.67, nll_loss=4.226,2023-07-10 18:17:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:17:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.74it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.06it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:17:34 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.1 | nll_loss 4.645 | ppl 25.01 | wps 154695 | wpb 3509.2 | bsz 145 | num_updates 12500 | best_loss 6.1\n",
      "2023-07-10 18:17:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12500 updates\n",
      "2023-07-10 18:17:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12500.pt\n",
      "2023-07-10 18:17:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12500.pt\n",
      "2023-07-10 18:17:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12500.pt (epoch 7 @ 12500 updates, score 6.1) (writing took 14.208243728149682 seconds)\n",
      "epoch 007:  88%|▉| 1611/1832 [05:42<00:23,  9.60it/s, loss=5.607, nll_loss=4.1552023-07-10 18:17:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:17:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.97it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:17:59 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.089 | nll_loss 4.622 | ppl 24.62 | wps 158867 | wpb 3509.2 | bsz 145 | num_updates 12600 | best_loss 6.089\n",
      "2023-07-10 18:17:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12600 updates\n",
      "2023-07-10 18:17:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12600.pt\n",
      "2023-07-10 18:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12600.pt\n",
      "2023-07-10 18:18:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12600.pt (epoch 7 @ 12600 updates, score 6.089) (writing took 10.7634826018475 seconds)\n",
      "epoch 007:  93%|▉| 1711/1832 [06:03<00:12,  9.68it/s, loss=5.626, nll_loss=4.1772023-07-10 18:18:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:18:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.69it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:18:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.089 | nll_loss 4.622 | ppl 24.63 | wps 154657 | wpb 3509.2 | bsz 145 | num_updates 12700 | best_loss 6.089\n",
      "2023-07-10 18:18:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12700 updates\n",
      "2023-07-10 18:18:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12700.pt\n",
      "2023-07-10 18:18:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12700.pt\n",
      "2023-07-10 18:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12700.pt (epoch 7 @ 12700 updates, score 6.089) (writing took 10.639479013858363 seconds)\n",
      "epoch 007:  99%|▉| 1811/1832 [06:25<00:02,  9.78it/s, loss=5.614, nll_loss=4.1632023-07-10 18:18:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:18:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.44it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:18:42 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.067 | nll_loss 4.601 | ppl 24.27 | wps 156571 | wpb 3509.2 | bsz 145 | num_updates 12800 | best_loss 6.067\n",
      "2023-07-10 18:18:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12800 updates\n",
      "2023-07-10 18:18:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12800.pt\n",
      "2023-07-10 18:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_7_12800.pt\n",
      "2023-07-10 18:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_7_12800.pt (epoch 7 @ 12800 updates, score 6.067) (writing took 10.898466637125239 seconds)\n",
      "epoch 007: 100%|▉| 1831/1832 [06:39<00:00,  9.01it/s, loss=5.569, nll_loss=4.1112023-07-10 18:18:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:18:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.53it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:18:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.063 | nll_loss 4.591 | ppl 24.1 | wps 157392 | wpb 3509.2 | bsz 145 | num_updates 12820 | best_loss 6.063\n",
      "2023-07-10 18:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12820 updates\n",
      "2023-07-10 18:18:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:19:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:19:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_best.pt (epoch 7 @ 12820 updates, score 6.063) (writing took 8.439097726950422 seconds)\n",
      "2023-07-10 18:19:04 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-07-10 18:19:04 | INFO | train | epoch 007 | loss 5.598 | nll_loss 4.144 | ppl 17.68 | wps 32349.7 | ups 4.49 | wpb 7207.3 | bsz 240.3 | num_updates 12820 | lr 0.000394976 | gnorm 0.724 | clip 2.6 | loss_scale 16 | train_wall 185 | gb_free 19.7 | wall 2729\n",
      "2023-07-10 18:19:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:19:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 008:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 18:19:04 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-07-10 18:19:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:   4%|█▎                            | 79/1832 [00:08<03:02,  9.60it/s]2023-07-10 18:19:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:19:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.13it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.90it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:19:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.086 | nll_loss 4.614 | ppl 24.49 | wps 155876 | wpb 3509.2 | bsz 145 | num_updates 12900 | best_loss 6.063\n",
      "2023-07-10 18:19:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12900 updates\n",
      "2023-07-10 18:19:13 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_12900.pt\n",
      "2023-07-10 18:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_12900.pt\n",
      "2023-07-10 18:19:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_12900.pt (epoch 8 @ 12900 updates, score 6.086) (writing took 8.058307152008638 seconds)\n",
      "epoch 008:  10%| | 179/1832 [00:26<02:48,  9.78it/s, loss=5.365, nll_loss=3.879,2023-07-10 18:19:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:19:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.76it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:19:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.071 | nll_loss 4.594 | ppl 24.15 | wps 159944 | wpb 3509.2 | bsz 145 | num_updates 13000 | best_loss 6.063\n",
      "2023-07-10 18:19:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13000 updates\n",
      "2023-07-10 18:19:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13000.pt\n",
      "2023-07-10 18:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13000.pt\n",
      "2023-07-10 18:19:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13000.pt (epoch 8 @ 13000 updates, score 6.071) (writing took 7.6935550309717655 seconds)\n",
      "epoch 008:  15%|▏| 279/1832 [00:45<02:42,  9.56it/s, loss=5.363, nll_loss=3.873,2023-07-10 18:19:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:19:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.43it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:19:50 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.077 | nll_loss 4.601 | ppl 24.28 | wps 154366 | wpb 3509.2 | bsz 145 | num_updates 13100 | best_loss 6.063\n",
      "2023-07-10 18:19:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13100 updates\n",
      "2023-07-10 18:19:50 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13100.pt\n",
      "2023-07-10 18:19:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13100.pt\n",
      "2023-07-10 18:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13100.pt (epoch 8 @ 13100 updates, score 6.077) (writing took 7.838759823003784 seconds)\n",
      "epoch 008:  21%|▏| 378/1832 [01:03<02:29,  9.72it/s, loss=5.411, nll_loss=3.928,2023-07-10 18:20:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:20:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.35it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:20:08 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.066 | nll_loss 4.591 | ppl 24.11 | wps 154370 | wpb 3509.2 | bsz 145 | num_updates 13200 | best_loss 6.063\n",
      "2023-07-10 18:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13200 updates\n",
      "2023-07-10 18:20:08 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13200.pt\n",
      "2023-07-10 18:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13200.pt\n",
      "2023-07-10 18:20:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13200.pt (epoch 8 @ 13200 updates, score 6.066) (writing took 7.735509634949267 seconds)\n",
      "epoch 008:  26%|▎| 478/1832 [01:22<02:23,  9.43it/s, loss=5.4, nll_loss=3.915, p2023-07-10 18:20:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:20:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.47it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:20:27 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.051 | nll_loss 4.573 | ppl 23.81 | wps 153470 | wpb 3509.2 | bsz 145 | num_updates 13300 | best_loss 6.051\n",
      "2023-07-10 18:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13300 updates\n",
      "2023-07-10 18:20:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13300.pt\n",
      "2023-07-10 18:20:32 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13300.pt\n",
      "2023-07-10 18:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13300.pt (epoch 8 @ 13300 updates, score 6.051) (writing took 12.14392810408026 seconds)\n",
      "epoch 008:  32%|▎| 578/1832 [01:45<02:07,  9.84it/s, loss=5.397, nll_loss=3.912,2023-07-10 18:20:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:20:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.71it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:20:50 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.037 | nll_loss 4.55 | ppl 23.43 | wps 157463 | wpb 3509.2 | bsz 145 | num_updates 13400 | best_loss 6.037\n",
      "2023-07-10 18:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13400 updates\n",
      "2023-07-10 18:20:50 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13400.pt\n",
      "2023-07-10 18:20:53 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13400.pt\n",
      "2023-07-10 18:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13400.pt (epoch 8 @ 13400 updates, score 6.037) (writing took 10.555731942877173 seconds)\n",
      "epoch 008:  37%|▎| 679/1832 [02:06<01:58,  9.69it/s, loss=5.449, nll_loss=3.971,2023-07-10 18:21:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:21:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.81it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:21:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.019 | nll_loss 4.535 | ppl 23.18 | wps 161376 | wpb 3509.2 | bsz 145 | num_updates 13500 | best_loss 6.019\n",
      "2023-07-10 18:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13500 updates\n",
      "2023-07-10 18:21:11 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13500.pt\n",
      "2023-07-10 18:21:14 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13500.pt\n",
      "2023-07-10 18:21:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13500.pt (epoch 8 @ 13500 updates, score 6.019) (writing took 11.174414366018027 seconds)\n",
      "epoch 008:  43%|▍| 779/1832 [02:28<01:49,  9.64it/s, loss=5.404, nll_loss=3.92, 2023-07-10 18:21:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:21:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.65it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:21:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.017 | nll_loss 4.528 | ppl 23.07 | wps 154799 | wpb 3509.2 | bsz 145 | num_updates 13600 | best_loss 6.017\n",
      "2023-07-10 18:21:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13600 updates\n",
      "2023-07-10 18:21:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13600.pt\n",
      "2023-07-10 18:21:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13600.pt\n",
      "2023-07-10 18:21:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13600.pt (epoch 8 @ 13600 updates, score 6.017) (writing took 10.934020995162427 seconds)\n",
      "epoch 008:  48%|▍| 879/1832 [02:50<01:37,  9.74it/s, loss=5.443, nll_loss=3.964,2023-07-10 18:21:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:21:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.08it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:21:55 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.024 | nll_loss 4.547 | ppl 23.38 | wps 159002 | wpb 3509.2 | bsz 145 | num_updates 13700 | best_loss 6.017\n",
      "2023-07-10 18:21:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13700 updates\n",
      "2023-07-10 18:21:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13700.pt\n",
      "2023-07-10 18:21:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13700.pt\n",
      "2023-07-10 18:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13700.pt (epoch 8 @ 13700 updates, score 6.024) (writing took 7.78568946197629 seconds)\n",
      "epoch 008:  53%|▌| 979/1832 [03:08<01:31,  9.30it/s, loss=5.36, nll_loss=3.87, p2023-07-10 18:22:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:22:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 24.61it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:22:14 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.99 | nll_loss 4.51 | ppl 22.79 | wps 157493 | wpb 3509.2 | bsz 145 | num_updates 13800 | best_loss 5.99\n",
      "2023-07-10 18:22:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13800 updates\n",
      "2023-07-10 18:22:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13800.pt\n",
      "2023-07-10 18:22:17 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13800.pt\n",
      "2023-07-10 18:22:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13800.pt (epoch 8 @ 13800 updates, score 5.99) (writing took 12.475455367937684 seconds)\n",
      "epoch 008:  59%|▌| 1079/1832 [03:32<01:19,  9.49it/s, loss=5.417, nll_loss=3.9352023-07-10 18:22:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:22:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.05it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:22:37 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.003 | nll_loss 4.525 | ppl 23.02 | wps 156734 | wpb 3509.2 | bsz 145 | num_updates 13900 | best_loss 5.99\n",
      "2023-07-10 18:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 13900 updates\n",
      "2023-07-10 18:22:37 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13900.pt\n",
      "2023-07-10 18:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_13900.pt\n",
      "2023-07-10 18:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_13900.pt (epoch 8 @ 13900 updates, score 6.003) (writing took 7.792342676082626 seconds)\n",
      "epoch 008:  64%|▋| 1179/1832 [03:51<01:07,  9.62it/s, loss=5.369, nll_loss=3.88,2023-07-10 18:22:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:22:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.00it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:22:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.998 | nll_loss 4.517 | ppl 22.89 | wps 159380 | wpb 3509.2 | bsz 145 | num_updates 14000 | best_loss 5.99\n",
      "2023-07-10 18:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14000 updates\n",
      "2023-07-10 18:22:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14000.pt\n",
      "2023-07-10 18:22:59 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14000.pt\n",
      "2023-07-10 18:23:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14000.pt (epoch 8 @ 14000 updates, score 5.998) (writing took 7.750062894076109 seconds)\n",
      "epoch 008:  70%|▋| 1279/1832 [04:09<00:56,  9.82it/s, loss=5.446, nll_loss=3.9692023-07-10 18:23:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:23:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.25it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:23:14 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.992 | nll_loss 4.507 | ppl 22.73 | wps 155572 | wpb 3509.2 | bsz 145 | num_updates 14100 | best_loss 5.99\n",
      "2023-07-10 18:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14100 updates\n",
      "2023-07-10 18:23:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14100.pt\n",
      "2023-07-10 18:23:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14100.pt\n",
      "2023-07-10 18:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14100.pt (epoch 8 @ 14100 updates, score 5.992) (writing took 7.884261201834306 seconds)\n",
      "epoch 008:  75%|▊| 1379/1832 [04:28<00:46,  9.76it/s, loss=5.479, nll_loss=4.0062023-07-10 18:23:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:23:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.14it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:23:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.962 | nll_loss 4.478 | ppl 22.28 | wps 155298 | wpb 3509.2 | bsz 145 | num_updates 14200 | best_loss 5.962\n",
      "2023-07-10 18:23:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14200 updates\n",
      "2023-07-10 18:23:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14200.pt\n",
      "2023-07-10 18:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14200.pt\n",
      "2023-07-10 18:23:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14200.pt (epoch 8 @ 14200 updates, score 5.962) (writing took 10.61994941602461 seconds)\n",
      "epoch 008:  81%|▊| 1479/1832 [04:49<00:36,  9.55it/s, loss=5.468, nll_loss=3.9952023-07-10 18:23:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:23:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.91it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:23:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.981 | nll_loss 4.491 | ppl 22.49 | wps 154586 | wpb 3509.2 | bsz 145 | num_updates 14300 | best_loss 5.962\n",
      "2023-07-10 18:23:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14300 updates\n",
      "2023-07-10 18:23:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14300.pt\n",
      "2023-07-10 18:23:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14300.pt\n",
      "2023-07-10 18:24:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14300.pt (epoch 8 @ 14300 updates, score 5.981) (writing took 7.943839956074953 seconds)\n",
      "epoch 008:  86%|▊| 1579/1832 [05:08<00:26,  9.59it/s, loss=5.401, nll_loss=3.9172023-07-10 18:24:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:24:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.09it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:24:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.981 | nll_loss 4.493 | ppl 22.52 | wps 156258 | wpb 3509.2 | bsz 145 | num_updates 14400 | best_loss 5.962\n",
      "2023-07-10 18:24:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14400 updates\n",
      "2023-07-10 18:24:13 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14400.pt\n",
      "2023-07-10 18:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14400.pt\n",
      "2023-07-10 18:24:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14400.pt (epoch 8 @ 14400 updates, score 5.981) (writing took 7.849468040978536 seconds)\n",
      "epoch 008:  92%|▉| 1679/1832 [05:26<00:15,  9.84it/s, loss=5.366, nll_loss=3.8782023-07-10 18:24:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:24:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.74it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:24:31 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.945 | nll_loss 4.457 | ppl 21.96 | wps 158665 | wpb 3509.2 | bsz 145 | num_updates 14500 | best_loss 5.945\n",
      "2023-07-10 18:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14500 updates\n",
      "2023-07-10 18:24:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14500.pt\n",
      "2023-07-10 18:24:35 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14500.pt\n",
      "2023-07-10 18:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14500.pt (epoch 8 @ 14500 updates, score 5.945) (writing took 11.184439572039992 seconds)\n",
      "epoch 008:  96%|▉| 1751/1832 [05:46<00:08,  9.65it/s, loss=5.449, nll_loss=3.9742023-07-10 18:24:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 008:  97%|▉| 1780/1832 [05:49<00:05,  9.79it/s, loss=5.449, nll_loss=3.9742023-07-10 18:24:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:24:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.58it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:24:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.936 | nll_loss 4.444 | ppl 21.76 | wps 153076 | wpb 3509.2 | bsz 145 | num_updates 14600 | best_loss 5.936\n",
      "2023-07-10 18:24:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14600 updates\n",
      "2023-07-10 18:24:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14600.pt\n",
      "2023-07-10 18:24:57 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_8_14600.pt\n",
      "2023-07-10 18:25:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_8_14600.pt (epoch 8 @ 14600 updates, score 5.936) (writing took 10.647845680825412 seconds)\n",
      "epoch 008: 100%|▉| 1831/1832 [06:05<00:00,  9.75it/s, loss=5.453, nll_loss=3.9772023-07-10 18:25:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:25:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.05it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.98it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:25:10 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.923 | nll_loss 4.428 | ppl 21.52 | wps 158526 | wpb 3509.2 | bsz 145 | num_updates 14651 | best_loss 5.923\n",
      "2023-07-10 18:25:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14651 updates\n",
      "2023-07-10 18:25:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:25:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_best.pt (epoch 8 @ 14651 updates, score 5.923) (writing took 7.873053251998499 seconds)\n",
      "2023-07-10 18:25:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-07-10 18:25:18 | INFO | train | epoch 008 | loss 5.414 | nll_loss 3.932 | ppl 15.27 | wps 35321 | ups 4.9 | wpb 7206.9 | bsz 239.3 | num_updates 14651 | lr 0.000369472 | gnorm 0.749 | clip 3.1 | loss_scale 8 | train_wall 185 | gb_free 19.9 | wall 3103\n",
      "2023-07-10 18:25:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:25:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 009:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 18:25:18 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-07-10 18:25:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:   3%|▊                             | 48/1832 [00:05<03:05,  9.60it/s]2023-07-10 18:25:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:25:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.05it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:25:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.94 | nll_loss 4.439 | ppl 21.69 | wps 155858 | wpb 3509.2 | bsz 145 | num_updates 14700 | best_loss 5.923\n",
      "2023-07-10 18:25:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14700 updates\n",
      "2023-07-10 18:25:23 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_14700.pt\n",
      "2023-07-10 18:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_14700.pt\n",
      "2023-07-10 18:25:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_14700.pt (epoch 9 @ 14700 updates, score 5.94) (writing took 8.03538864897564 seconds)\n",
      "epoch 009:   8%| | 148/1832 [00:23<02:52,  9.74it/s, loss=5.39, nll_loss=3.905, 2023-07-10 18:25:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:25:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.90it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:25:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.944 | nll_loss 4.447 | ppl 21.81 | wps 151854 | wpb 3509.2 | bsz 145 | num_updates 14800 | best_loss 5.923\n",
      "2023-07-10 18:25:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14800 updates\n",
      "2023-07-10 18:25:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_14800.pt\n",
      "2023-07-10 18:25:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_14800.pt\n",
      "2023-07-10 18:25:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_14800.pt (epoch 9 @ 14800 updates, score 5.944) (writing took 7.406810838961974 seconds)\n",
      "epoch 009:  14%|▏| 248/1832 [00:42<02:53,  9.13it/s, loss=5.173, nll_loss=3.654,2023-07-10 18:26:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:26:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.06it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:26:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.96 | nll_loss 4.458 | ppl 21.97 | wps 158771 | wpb 3509.2 | bsz 145 | num_updates 14900 | best_loss 5.923\n",
      "2023-07-10 18:26:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14900 updates\n",
      "2023-07-10 18:26:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_14900.pt\n",
      "2023-07-10 18:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_14900.pt\n",
      "2023-07-10 18:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_14900.pt (epoch 9 @ 14900 updates, score 5.96) (writing took 10.92992961104028 seconds)\n",
      "epoch 009:  19%|▏| 348/1832 [01:03<02:33,  9.68it/s, loss=5.197, nll_loss=3.683,2023-07-10 18:26:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:26:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.99it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:26:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.941 | nll_loss 4.436 | ppl 21.64 | wps 159296 | wpb 3509.2 | bsz 145 | num_updates 15000 | best_loss 5.923\n",
      "2023-07-10 18:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15000 updates\n",
      "2023-07-10 18:26:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15000.pt\n",
      "2023-07-10 18:26:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15000.pt\n",
      "2023-07-10 18:26:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15000.pt (epoch 9 @ 15000 updates, score 5.941) (writing took 7.596719524124637 seconds)\n",
      "epoch 009:  24%|▏| 448/1832 [01:22<02:25,  9.51it/s, loss=5.209, nll_loss=3.695,2023-07-10 18:26:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:26:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.48it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.75it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:26:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.924 | nll_loss 4.43 | ppl 21.56 | wps 158773 | wpb 3509.2 | bsz 145 | num_updates 15100 | best_loss 5.923\n",
      "2023-07-10 18:26:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15100 updates\n",
      "2023-07-10 18:26:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15100.pt\n",
      "2023-07-10 18:26:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15100.pt\n",
      "2023-07-10 18:26:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15100.pt (epoch 9 @ 15100 updates, score 5.924) (writing took 7.75597731419839 seconds)\n",
      "epoch 009:  30%|▎| 548/1832 [01:40<02:18,  9.24it/s, loss=5.265, nll_loss=3.76, 2023-07-10 18:26:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:26:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.26it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:26:59 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.912 | nll_loss 4.412 | ppl 21.29 | wps 158370 | wpb 3509.2 | bsz 145 | num_updates 15200 | best_loss 5.912\n",
      "2023-07-10 18:26:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15200 updates\n",
      "2023-07-10 18:26:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15200.pt\n",
      "2023-07-10 18:27:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15200.pt\n",
      "2023-07-10 18:27:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15200.pt (epoch 9 @ 15200 updates, score 5.912) (writing took 10.80900109000504 seconds)\n",
      "epoch 009:  35%|▎| 648/1832 [02:02<02:02,  9.68it/s, loss=5.25, nll_loss=3.743, 2023-07-10 18:27:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:27:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  25%|██      | 2/8 [00:00<00:00, 17.79it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 34.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:27:20 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.884 | nll_loss 4.377 | ppl 20.78 | wps 160341 | wpb 3509.2 | bsz 145 | num_updates 15300 | best_loss 5.884\n",
      "2023-07-10 18:27:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15300 updates\n",
      "2023-07-10 18:27:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15300.pt\n",
      "2023-07-10 18:27:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15300.pt\n",
      "2023-07-10 18:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15300.pt (epoch 9 @ 15300 updates, score 5.884) (writing took 10.553173137130216 seconds)\n",
      "epoch 009:  41%|▍| 747/1832 [02:23<01:51,  9.70it/s, loss=5.251, nll_loss=3.743,2023-07-10 18:27:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:27:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.45it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:27:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.903 | nll_loss 4.401 | ppl 21.13 | wps 155995 | wpb 3509.2 | bsz 145 | num_updates 15400 | best_loss 5.884\n",
      "2023-07-10 18:27:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15400 updates\n",
      "2023-07-10 18:27:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15400.pt\n",
      "2023-07-10 18:27:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15400.pt\n",
      "2023-07-10 18:27:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15400.pt (epoch 9 @ 15400 updates, score 5.903) (writing took 7.830990260001272 seconds)\n",
      "epoch 009:  46%|▍| 848/1832 [02:42<01:42,  9.62it/s, loss=5.273, nll_loss=3.768,2023-07-10 18:28:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:28:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.01it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:28:01 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.9 | nll_loss 4.384 | ppl 20.87 | wps 159073 | wpb 3509.2 | bsz 145 | num_updates 15500 | best_loss 5.884\n",
      "2023-07-10 18:28:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15500 updates\n",
      "2023-07-10 18:28:01 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15500.pt\n",
      "2023-07-10 18:28:04 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15500.pt\n",
      "2023-07-10 18:28:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15500.pt (epoch 9 @ 15500 updates, score 5.9) (writing took 7.768916186876595 seconds)\n",
      "epoch 009:  52%|▌| 948/1832 [03:00<01:29,  9.88it/s, loss=5.234, nll_loss=3.724,2023-07-10 18:28:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:28:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.76it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:28:19 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.865 | nll_loss 4.358 | ppl 20.51 | wps 157241 | wpb 3509.2 | bsz 145 | num_updates 15600 | best_loss 5.865\n",
      "2023-07-10 18:28:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15600 updates\n",
      "2023-07-10 18:28:19 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15600.pt\n",
      "2023-07-10 18:28:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15600.pt\n",
      "2023-07-10 18:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15600.pt (epoch 9 @ 15600 updates, score 5.865) (writing took 13.078883372945711 seconds)\n",
      "epoch 009:  57%|▌| 1048/1832 [03:25<01:21,  9.56it/s, loss=5.27, nll_loss=3.766,2023-07-10 18:28:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:28:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.37it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:28:43 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.855 | nll_loss 4.358 | ppl 20.51 | wps 160142 | wpb 3509.2 | bsz 145 | num_updates 15700 | best_loss 5.855\n",
      "2023-07-10 18:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15700 updates\n",
      "2023-07-10 18:28:43 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15700.pt\n",
      "2023-07-10 18:28:47 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15700.pt\n",
      "2023-07-10 18:28:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15700.pt (epoch 9 @ 15700 updates, score 5.855) (writing took 10.687574530020356 seconds)\n",
      "epoch 009:  63%|▋| 1148/1832 [03:46<01:12,  9.43it/s, loss=5.28, nll_loss=3.778,2023-07-10 18:29:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:29:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.98it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:29:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.859 | nll_loss 4.354 | ppl 20.45 | wps 161383 | wpb 3509.2 | bsz 145 | num_updates 15800 | best_loss 5.855\n",
      "2023-07-10 18:29:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15800 updates\n",
      "2023-07-10 18:29:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15800.pt\n",
      "2023-07-10 18:29:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15800.pt\n",
      "2023-07-10 18:29:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15800.pt (epoch 9 @ 15800 updates, score 5.859) (writing took 7.445056007942185 seconds)\n",
      "epoch 009:  68%|▋| 1248/1832 [04:05<01:01,  9.52it/s, loss=5.316, nll_loss=3.8192023-07-10 18:29:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:29:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.30it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:29:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.86 | nll_loss 4.354 | ppl 20.45 | wps 157758 | wpb 3509.2 | bsz 145 | num_updates 15900 | best_loss 5.855\n",
      "2023-07-10 18:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 15900 updates\n",
      "2023-07-10 18:29:23 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15900.pt\n",
      "2023-07-10 18:29:27 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_15900.pt\n",
      "2023-07-10 18:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_15900.pt (epoch 9 @ 15900 updates, score 5.86) (writing took 7.652419104939327 seconds)\n",
      "epoch 009:  74%|▋| 1348/1832 [04:23<00:48, 10.01it/s, loss=5.285, nll_loss=3.7832023-07-10 18:29:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:29:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:29:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.845 | nll_loss 4.338 | ppl 20.23 | wps 151591 | wpb 3509.2 | bsz 145 | num_updates 16000 | best_loss 5.845\n",
      "2023-07-10 18:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16000 updates\n",
      "2023-07-10 18:29:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16000.pt\n",
      "2023-07-10 18:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16000.pt\n",
      "2023-07-10 18:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_16000.pt (epoch 9 @ 16000 updates, score 5.845) (writing took 10.783448450965807 seconds)\n",
      "epoch 009:  79%|▊| 1448/1832 [04:44<00:40,  9.40it/s, loss=5.308, nll_loss=3.81,2023-07-10 18:30:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:30:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 24.82it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.64it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:30:03 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.832 | nll_loss 4.325 | ppl 20.04 | wps 146584 | wpb 3509.2 | bsz 145 | num_updates 16100 | best_loss 5.832\n",
      "2023-07-10 18:30:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16100 updates\n",
      "2023-07-10 18:30:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16100.pt\n",
      "2023-07-10 18:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16100.pt\n",
      "2023-07-10 18:30:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_16100.pt (epoch 9 @ 16100 updates, score 5.832) (writing took 10.937010881025344 seconds)\n",
      "epoch 009:  84%|▊| 1548/1832 [05:06<00:29,  9.70it/s, loss=5.298, nll_loss=3.7992023-07-10 18:30:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:30:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.71it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.39it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:30:25 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.833 | nll_loss 4.326 | ppl 20.05 | wps 161237 | wpb 3509.2 | bsz 145 | num_updates 16200 | best_loss 5.832\n",
      "2023-07-10 18:30:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16200 updates\n",
      "2023-07-10 18:30:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16200.pt\n",
      "2023-07-10 18:30:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16200.pt\n",
      "2023-07-10 18:30:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_16200.pt (epoch 9 @ 16200 updates, score 5.833) (writing took 7.727567543974146 seconds)\n",
      "epoch 009:  90%|▉| 1648/1832 [05:25<00:19,  9.68it/s, loss=5.287, nll_loss=3.7862023-07-10 18:30:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:30:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.11it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:30:44 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.813 | nll_loss 4.305 | ppl 19.77 | wps 159498 | wpb 3509.2 | bsz 145 | num_updates 16300 | best_loss 5.813\n",
      "2023-07-10 18:30:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16300 updates\n",
      "2023-07-10 18:30:44 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16300.pt\n",
      "2023-07-10 18:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16300.pt\n",
      "2023-07-10 18:30:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_16300.pt (epoch 9 @ 16300 updates, score 5.813) (writing took 11.023819701047614 seconds)\n",
      "epoch 009:  95%|▉| 1748/1832 [05:47<00:08,  9.58it/s, loss=5.223, nll_loss=3.7132023-07-10 18:31:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:31:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.00it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:31:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.808 | nll_loss 4.298 | ppl 19.67 | wps 158520 | wpb 3509.2 | bsz 145 | num_updates 16400 | best_loss 5.808\n",
      "2023-07-10 18:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16400 updates\n",
      "2023-07-10 18:31:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16400.pt\n",
      "2023-07-10 18:31:09 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_9_16400.pt\n",
      "2023-07-10 18:31:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_9_16400.pt (epoch 9 @ 16400 updates, score 5.808) (writing took 11.430614094017074 seconds)\n",
      "epoch 009: 100%|▉| 1830/1832 [06:07<00:00,  9.91it/s, loss=5.219, nll_loss=3.71,2023-07-10 18:31:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:31:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.75it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:31:26 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.803 | nll_loss 4.287 | ppl 19.52 | wps 159400 | wpb 3509.2 | bsz 145 | num_updates 16483 | best_loss 5.803\n",
      "2023-07-10 18:31:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16483 updates\n",
      "2023-07-10 18:31:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:31:30 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_best.pt (epoch 9 @ 16483 updates, score 5.803) (writing took 8.226219339063391 seconds)\n",
      "2023-07-10 18:31:34 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-07-10 18:31:34 | INFO | train | epoch 009 | loss 5.257 | nll_loss 3.751 | ppl 13.47 | wps 35075.3 | ups 4.87 | wpb 7207.3 | bsz 240.3 | num_updates 16483 | lr 0.000348335 | gnorm 0.779 | clip 4.5 | loss_scale 8 | train_wall 185 | gb_free 19.9 | wall 3479\n",
      "2023-07-10 18:31:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:31:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1832\n",
      "epoch 010:   0%|                                       | 0/1832 [00:00<?, ?it/s]2023-07-10 18:31:34 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-07-10 18:31:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:   1%|▎                             | 16/1832 [00:01<03:07,  9.68it/s]2023-07-10 18:31:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:31:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  50%|████    | 4/8 [00:00<00:00, 33.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:31:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.822 | nll_loss 4.302 | ppl 19.73 | wps 160190 | wpb 3509.2 | bsz 145 | num_updates 16500 | best_loss 5.803\n",
      "2023-07-10 18:31:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 16500 updates\n",
      "2023-07-10 18:31:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16500.pt\n",
      "2023-07-10 18:31:40 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16500.pt\n",
      "2023-07-10 18:31:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_16500.pt (epoch 10 @ 16500 updates, score 5.822) (writing took 7.816463294904679 seconds)\n",
      "epoch 010:   6%| | 116/1832 [00:20<02:57,  9.69it/s, loss=5.243, nll_loss=3.737,2023-07-10 18:31:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:31:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.61it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.41it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:31:55 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.818 | nll_loss 4.304 | ppl 19.76 | wps 153308 | wpb 3509.2 | bsz 145 | num_updates 16600 | best_loss 5.803\n",
      "2023-07-10 18:31:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 16600 updates\n",
      "2023-07-10 18:31:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16600.pt\n",
      "2023-07-10 18:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16600.pt\n",
      "2023-07-10 18:32:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_16600.pt (epoch 10 @ 16600 updates, score 5.818) (writing took 8.270781354047358 seconds)\n",
      "epoch 010:  12%| | 216/1832 [00:39<02:46,  9.73it/s, loss=5.085, nll_loss=3.554,2023-07-10 18:32:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:32:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.21it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.78it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:32:14 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.82 | nll_loss 4.299 | ppl 19.68 | wps 158135 | wpb 3509.2 | bsz 145 | num_updates 16700 | best_loss 5.803\n",
      "2023-07-10 18:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 16700 updates\n",
      "2023-07-10 18:32:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16700.pt\n",
      "2023-07-10 18:32:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16700.pt\n",
      "2023-07-10 18:32:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_16700.pt (epoch 10 @ 16700 updates, score 5.82) (writing took 7.980064116185531 seconds)\n",
      "epoch 010:  17%|▏| 316/1832 [00:58<02:35,  9.78it/s, loss=5.072, nll_loss=3.539,2023-07-10 18:32:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:32:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.11it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:32:33 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.814 | nll_loss 4.29 | ppl 19.56 | wps 160241 | wpb 3509.2 | bsz 145 | num_updates 16800 | best_loss 5.803\n",
      "2023-07-10 18:32:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 16800 updates\n",
      "2023-07-10 18:32:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16800.pt\n",
      "2023-07-10 18:32:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16800.pt\n",
      "2023-07-10 18:32:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_16800.pt (epoch 10 @ 16800 updates, score 5.814) (writing took 7.717154362006113 seconds)\n",
      "epoch 010:  23%|▏| 416/1832 [01:16<02:25,  9.76it/s, loss=5.092, nll_loss=3.561,2023-07-10 18:32:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:32:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.92it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:32:51 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.797 | nll_loss 4.278 | ppl 19.4 | wps 159022 | wpb 3509.2 | bsz 145 | num_updates 16900 | best_loss 5.797\n",
      "2023-07-10 18:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 16900 updates\n",
      "2023-07-10 18:32:51 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16900.pt\n",
      "2023-07-10 18:32:55 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_16900.pt\n",
      "2023-07-10 18:33:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_16900.pt (epoch 10 @ 16900 updates, score 5.797) (writing took 11.068484819028527 seconds)\n",
      "epoch 010:  28%|▎| 516/1832 [01:38<02:15,  9.68it/s, loss=5.099, nll_loss=3.569,2023-07-10 18:33:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:33:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.21it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:33:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.797 | nll_loss 4.273 | ppl 19.34 | wps 156655 | wpb 3509.2 | bsz 145 | num_updates 17000 | best_loss 5.797\n",
      "2023-07-10 18:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17000 updates\n",
      "2023-07-10 18:33:13 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17000.pt\n",
      "2023-07-10 18:33:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17000.pt\n",
      "2023-07-10 18:33:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17000.pt (epoch 10 @ 17000 updates, score 5.797) (writing took 10.927983436034992 seconds)\n",
      "epoch 010:  34%|▎| 616/1832 [02:00<02:05,  9.71it/s, loss=5.108, nll_loss=3.58, 2023-07-10 18:33:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:33:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.99it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:33:35 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.797 | nll_loss 4.274 | ppl 19.34 | wps 151803 | wpb 3509.2 | bsz 145 | num_updates 17100 | best_loss 5.797\n",
      "2023-07-10 18:33:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17100 updates\n",
      "2023-07-10 18:33:35 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17100.pt\n",
      "2023-07-10 18:33:39 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17100.pt\n",
      "2023-07-10 18:33:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17100.pt (epoch 10 @ 17100 updates, score 5.797) (writing took 11.107854414032772 seconds)\n",
      "epoch 010:  39%|▍| 716/1832 [02:22<01:54,  9.74it/s, loss=5.095, nll_loss=3.565,2023-07-10 18:33:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:33:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.96it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:33:57 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.781 | nll_loss 4.258 | ppl 19.13 | wps 157956 | wpb 3509.2 | bsz 145 | num_updates 17200 | best_loss 5.781\n",
      "2023-07-10 18:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17200 updates\n",
      "2023-07-10 18:33:57 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17200.pt\n",
      "2023-07-10 18:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17200.pt\n",
      "2023-07-10 18:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17200.pt (epoch 10 @ 17200 updates, score 5.781) (writing took 12.175628508906811 seconds)\n",
      "epoch 010:  45%|▍| 816/1832 [02:45<01:45,  9.62it/s, loss=5.132, nll_loss=3.607,2023-07-10 18:34:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:34:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.20it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.38it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:34:20 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.771 | nll_loss 4.246 | ppl 18.97 | wps 146246 | wpb 3509.2 | bsz 145 | num_updates 17300 | best_loss 5.771\n",
      "2023-07-10 18:34:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17300 updates\n",
      "2023-07-10 18:34:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17300.pt\n",
      "2023-07-10 18:34:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17300.pt\n",
      "2023-07-10 18:34:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17300.pt (epoch 10 @ 17300 updates, score 5.771) (writing took 14.67614600295201 seconds)\n",
      "epoch 010:  50%|▌| 916/1832 [03:10<01:35,  9.59it/s, loss=5.152, nll_loss=3.63, 2023-07-10 18:34:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:34:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.50it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.32it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:34:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.77 | nll_loss 4.244 | ppl 18.95 | wps 159601 | wpb 3509.2 | bsz 145 | num_updates 17400 | best_loss 5.77\n",
      "2023-07-10 18:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17400 updates\n",
      "2023-07-10 18:34:46 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17400.pt\n",
      "2023-07-10 18:34:49 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17400.pt\n",
      "2023-07-10 18:34:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17400.pt (epoch 10 @ 17400 updates, score 5.77) (writing took 10.78028575098142 seconds)\n",
      "epoch 010:  55%|▌| 1016/1832 [03:32<01:24,  9.65it/s, loss=5.051, nll_loss=3.5152023-07-10 18:35:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:35:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.38it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:35:07 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.767 | nll_loss 4.25 | ppl 19.03 | wps 159928 | wpb 3509.2 | bsz 145 | num_updates 17500 | best_loss 5.767\n",
      "2023-07-10 18:35:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17500 updates\n",
      "2023-07-10 18:35:07 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17500.pt\n",
      "2023-07-10 18:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17500.pt\n",
      "2023-07-10 18:35:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17500.pt (epoch 10 @ 17500 updates, score 5.767) (writing took 13.161552492994815 seconds)\n",
      "epoch 010:  61%|▌| 1116/1832 [03:56<01:16,  9.35it/s, loss=5.174, nll_loss=3.6562023-07-10 18:35:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:35:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 25.41it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 36.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:35:31 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.759 | nll_loss 4.234 | ppl 18.82 | wps 153918 | wpb 3509.2 | bsz 145 | num_updates 17600 | best_loss 5.759\n",
      "2023-07-10 18:35:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17600 updates\n",
      "2023-07-10 18:35:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17600.pt\n",
      "2023-07-10 18:35:35 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17600.pt\n",
      "2023-07-10 18:35:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17600.pt (epoch 10 @ 17600 updates, score 5.759) (writing took 11.20305062807165 seconds)\n",
      "epoch 010:  66%|▋| 1216/1832 [04:18<01:02,  9.86it/s, loss=5.122, nll_loss=3.5962023-07-10 18:35:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:35:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.32it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:35:53 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.749 | nll_loss 4.229 | ppl 18.76 | wps 158401 | wpb 3509.2 | bsz 145 | num_updates 17700 | best_loss 5.749\n",
      "2023-07-10 18:35:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17700 updates\n",
      "2023-07-10 18:35:53 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17700.pt\n",
      "2023-07-10 18:35:56 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17700.pt\n",
      "2023-07-10 18:36:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17700.pt (epoch 10 @ 17700 updates, score 5.749) (writing took 11.207577804801986 seconds)\n",
      "epoch 010:  72%|▋| 1316/1832 [04:40<00:54,  9.45it/s, loss=5.083, nll_loss=3.5522023-07-10 18:36:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:36:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.69it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:36:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.727 | nll_loss 4.192 | ppl 18.28 | wps 157844 | wpb 3509.2 | bsz 145 | num_updates 17800 | best_loss 5.727\n",
      "2023-07-10 18:36:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17800 updates\n",
      "2023-07-10 18:36:15 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17800.pt\n",
      "2023-07-10 18:36:19 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17800.pt\n",
      "2023-07-10 18:36:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17800.pt (epoch 10 @ 17800 updates, score 5.727) (writing took 11.238177655031905 seconds)\n",
      "epoch 010:  77%|▊| 1416/1832 [05:02<00:42,  9.84it/s, loss=5.187, nll_loss=3.67,2023-07-10 18:36:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:36:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.80it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:36:37 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.735 | nll_loss 4.201 | ppl 18.4 | wps 153813 | wpb 3509.2 | bsz 145 | num_updates 17900 | best_loss 5.727\n",
      "2023-07-10 18:36:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 17900 updates\n",
      "2023-07-10 18:36:37 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17900.pt\n",
      "2023-07-10 18:36:40 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_17900.pt\n",
      "2023-07-10 18:36:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_17900.pt (epoch 10 @ 17900 updates, score 5.735) (writing took 10.357034821994603 seconds)\n",
      "epoch 010:  83%|▊| 1516/1832 [05:23<00:33,  9.47it/s, loss=5.137, nll_loss=3.6142023-07-10 18:36:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:36:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 29.11it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 39.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:36:58 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.708 | nll_loss 4.185 | ppl 18.19 | wps 156414 | wpb 3509.2 | bsz 145 | num_updates 18000 | best_loss 5.708\n",
      "2023-07-10 18:36:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18000 updates\n",
      "2023-07-10 18:36:58 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18000.pt\n",
      "2023-07-10 18:37:01 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18000.pt\n",
      "2023-07-10 18:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_18000.pt (epoch 10 @ 18000 updates, score 5.708) (writing took 10.656076496001333 seconds)\n",
      "epoch 010:  88%|▉| 1616/1832 [05:44<00:22,  9.68it/s, loss=5.158, nll_loss=3.6382023-07-10 18:37:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:37:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 27.09it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:37:19 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.7 | nll_loss 4.17 | ppl 18 | wps 152024 | wpb 3509.2 | bsz 145 | num_updates 18100 | best_loss 5.7\n",
      "2023-07-10 18:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18100 updates\n",
      "2023-07-10 18:37:19 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18100.pt\n",
      "2023-07-10 18:37:23 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18100.pt\n",
      "2023-07-10 18:37:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_18100.pt (epoch 10 @ 18100 updates, score 5.7) (writing took 10.721561626065522 seconds)\n",
      "epoch 010:  94%|▉| 1716/1832 [06:06<00:11,  9.73it/s, loss=5.13, nll_loss=3.606,2023-07-10 18:37:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:37:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 28.64it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 38.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:37:41 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.702 | nll_loss 4.164 | ppl 17.92 | wps 154468 | wpb 3509.2 | bsz 145 | num_updates 18200 | best_loss 5.7\n",
      "2023-07-10 18:37:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18200 updates\n",
      "2023-07-10 18:37:41 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18200.pt\n",
      "2023-07-10 18:37:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18200.pt\n",
      "2023-07-10 18:37:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_18200.pt (epoch 10 @ 18200 updates, score 5.702) (writing took 8.171949253883213 seconds)\n",
      "epoch 010:  99%|▉| 1816/1832 [06:25<00:01,  9.27it/s, loss=5.156, nll_loss=3.6382023-07-10 18:38:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:38:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  38%|███     | 3/8 [00:00<00:00, 26.81it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset: 100%|████████| 8/8 [00:00<00:00, 37.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:38:00 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.68 | nll_loss 4.144 | ppl 17.68 | wps 153609 | wpb 3509.2 | bsz 145 | num_updates 18300 | best_loss 5.68\n",
      "2023-07-10 18:38:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18300 updates\n",
      "2023-07-10 18:38:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18300.pt\n",
      "2023-07-10 18:38:04 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_10_18300.pt\n",
      "2023-07-10 18:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_10_18300.pt (epoch 10 @ 18300 updates, score 5.68) (writing took 11.06918367696926 seconds)\n",
      "epoch 010: 100%|▉| 1831/1832 [06:38<00:00,  5.63it/s, loss=5.135, nll_loss=3.6142023-07-10 18:38:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:38:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  25%|██      | 2/8 [00:00<00:00, 19.65it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  88%|███████ | 7/8 [00:00<00:00, 34.77it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:38:13 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.683 | nll_loss 4.15 | ppl 17.75 | wps 156747 | wpb 3509.2 | bsz 145 | num_updates 18315 | best_loss 5.68\n",
      "2023-07-10 18:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18315 updates\n",
      "2023-07-10 18:38:13 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 18:38:17 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-spm/checkpoint_last.pt\n",
      "2023-07-10 18:38:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-spm/checkpoint_last.pt (epoch 10 @ 18315 updates, score 5.683) (writing took 4.166622051037848 seconds)\n",
      "2023-07-10 18:38:17 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-07-10 18:38:17 | INFO | train | epoch 010 | loss 5.119 | nll_loss 3.593 | ppl 12.07 | wps 32759.9 | ups 4.55 | wpb 7207.3 | bsz 240.3 | num_updates 18315 | lr 0.000330454 | gnorm 0.799 | clip 5.5 | loss_scale 8 | train_wall 185 | gb_free 19.8 | wall 3882\n",
      "2023-07-10 18:38:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:38:17 | INFO | fairseq_cli.train | done training in 3877.7 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 fairseq-train data-spm-bin \\\n",
    "    --arch transformer \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.2 --weight-decay 1e-4 --clip-norm 1.0 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 4000 --update-freq 1 \\\n",
    "    --save-dir checkpoints-spm \\\n",
    "    --save-interval-updates 100 --validate-interval-updates 100 \\\n",
    "    --keep-interval-updates 10 --no-epoch-checkpoints \\\n",
    "    --fp16 \\\n",
    "    --tensorboard-logdir tensorboard \\\n",
    "    --max-epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-10 18:43:35 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-spm/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:43:35 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:43:35 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:43:35 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:43:39 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 18:43:39 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 18:43:39 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 18:43:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:43:40 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:43:40 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:43:40 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:43:55 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 18:43:55 | INFO | fairseq_cli.generate | Translated 1,160 sentences (33,029 tokens) in 10.5s (110.58 sentences/s, 3148.63 tokens/s)\n",
      "S-347\t大比叡と四明岳\n",
      "T-347\tDaihiei and Shimeigatake\n",
      "H-347\t-1.397982120513916\tMt. Dai-san and Mt. Ryoun\n",
      "D-347\t-1.397982120513916\tMt. Dai-san and Mt. Ryoun\n",
      "P-347\t-3.1881 -0.1011 -1.7249 -1.5021 -1.5005 -0.3655 -0.3752 -0.1057 -3.8817 -2.4515 -0.1814\n",
      "S-211\tそのため、伝授は困難を極めた。\n",
      "T-211\tTherefore, its initiation was extremely difficult.\n",
      "H-211\t-1.0322842597961426\tTherefore, it is difficult to teach the art of divination.\n",
      "D-211\t-1.0322842597961426\tTherefore, it is difficult to teach the art of divination.\n",
      "P-211\t-1.0137 -0.4351 -1.7098 -1.3601 -0.5645 -0.2619 -2.3737 -1.7242 -1.4837 -0.3285 -1.6295 -0.4236 -0.1113\n",
      "S-109\t1987年静態保存。\n",
      "T-109\tPut into static preservation in 1987.\n",
      "H-109\t-0.6646220684051514\tIn 1987, it was preserved.\n",
      "D-109\t-0.6646220684051514\tIn 1987, it was preserved.\n",
      "P-109\t-1.2507 -0.0932 -0.3265 -1.1956 -0.8781 -0.6697 -0.7917 -0.1115\n",
      "S-1157\t親藩、譜代大名、外様大名\n",
      "T-1157\tShinpan, Fudai Daimyo, Tozama Daimyo\n",
      "H-1157\t-0.3245590627193451\tFudai daimyo, fudai daimyo (a daimyo in hereditary vassal to the Tokugawa family), fudai daimyo (a daimyo in hereditary vassal to the Tokugawa family), tozama daimyo (nonhereditary feudal lord)\n",
      "D-1157\t-0.3245590627193451\tFudai daimyo, fudai daimyo (a daimyo in hereditary vassal to the Tokugawa family), fudai daimyo (a daimyo in hereditary vassal to the Tokugawa family), tozama daimyo (nonhereditary feudal lord)\n",
      "P-1157\t-2.3322 -0.2499 -0.9863 -0.3632 -0.0868 -1.3993 -0.2489 -0.1536 -0.1952 -0.0515 -0.0654 -0.0956 -0.1343 -0.0321 -0.1387 -0.2349 -0.6334 -0.0323 -0.2260 -0.1528 -0.1076 -0.1777 -0.0576 -0.0620 -0.1078 -0.1168 -0.0430 -0.1019 -0.3227 -1.4385 -0.0424 -0.1920 -0.1408 -0.2349 -0.1143 -0.0660 -0.9546 -0.2401\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-generate data-spm-bin --path checkpoints-spm/checkpoint_best.pt --batch-size 128 --beam 5 > ./spm-results/result.txt --remove-bpe=sentencepiece\n",
    "!head -20 ./spm-results/result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candlebria\n",
      "Dogen was a Zen priest in the early Kamakura period.\n",
      "He was the founder of the Soto sect.\n",
      "In his later years, he was also called Gengen.\n",
      "He was called the founder of the sect.\n",
      "His posthumous name was Togoku-bo (the eastern part of Japan, particularly Kanto region), and his posthumous Buddhist name was Jiku Daishi.\n",
      "Generally, he is called Dogen Zenji.\n",
      "It is said that they spread the custom of washing teeth and eating utensils at the time of meal.\n",
      "There is a theory that it was first brought back to the place where the first plant was brought back to Japan.\n",
      "There are many theories regarding the birth of Dogen, but there are various theories regarding the reason why he was born to Michichika TSUCHIMIKADO (or Michichika KOGA, who was the direct descendant of MINAMOTO no Michichika), who was the Naidaijin (minister of the center).\n"
     ]
    }
   ],
   "source": [
    "!grep \"^H-\" ./spm-results/result.txt | sort -V | cut -f3 > ./spm-results/result.en.txt\n",
    "!head ./spm-results/result.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(sys='./spm-results/result.en.txt', ref='kftt-data-1.0/data/orig/kyoto-test.en', order=4, ignore_case=False, sacrebleu=False, sentence_bleu=False)\n",
      "BLEU4 = 10.31, 33.1/13.9/6.8/3.6 (BP=1.000, ratio=1.101, syslen=24288, reflen=22063)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys ./spm-results/result.en.txt --ref kftt-data-1.0/data/orig/kyoto-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 18:44:14 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-spm/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:44:14 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:44:14 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:44:14 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:44:16 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 18:44:16 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 18:44:16 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 18:44:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:44:17 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:44:17 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:44:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:44:31 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 18:44:31 | INFO | fairseq_cli.generate | Translated 1,160 sentences (34,470 tokens) in 9.5s (122.48 sentences/s, 3639.50 tokens/s)\n",
      "2023-07-10 18:44:48 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-spm/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 2, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:44:48 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:44:48 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:44:48 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:44:50 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 18:44:50 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 18:44:50 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 18:44:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:44:51 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:44:51 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:44:51 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:45:06 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 18:45:06 | INFO | fairseq_cli.generate | Translated 1,160 sentences (33,602 tokens) in 9.9s (116.85 sentences/s, 3384.84 tokens/s)\n",
      "2023-07-10 18:45:22 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-spm/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 3, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:45:22 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:45:22 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:45:22 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:45:24 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 18:45:24 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 18:45:24 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 18:45:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:45:25 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:45:25 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:45:25 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:45:40 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 18:45:40 | INFO | fairseq_cli.generate | Translated 1,160 sentences (33,367 tokens) in 10.1s (115.01 sentences/s, 3308.26 tokens/s)\n",
      "2023-07-10 18:45:57 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-spm/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:45:57 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:45:57 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:45:57 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:45:59 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 18:45:59 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 18:45:59 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 18:46:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:46:00 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:46:00 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:46:00 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:46:15 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 18:46:15 | INFO | fairseq_cli.generate | Translated 1,160 sentences (32,736 tokens) in 10.1s (115.11 sentences/s, 3248.47 tokens/s)\n",
      "2023-07-10 18:46:31 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-spm/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:46:31 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:46:31 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:46:31 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-spm/checkpoint_best.pt\n",
      "2023-07-10 18:46:33 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 18:46:33 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 18:46:33 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 18:46:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:46:34 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:46:34 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:46:34 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:46:49 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 18:46:49 | INFO | fairseq_cli.generate | Translated 1,160 sentences (33,029 tokens) in 10.8s (107.89 sentences/s, 3072.04 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in 1 2 3 4 5\n",
    "do\n",
    "CUDA_VISIBLE_DEVICES=0 fairseq-generate data-spm-bin --path checkpoints-spm/checkpoint_best.pt --batch-size 128 --beam $i > ./spm-results/result.$i.txt --remove-bpe=sentencepiece\n",
    "grep \"^H-\" ./spm-results/result.$i.txt | sort -V | cut -f3 > ./spm-results/result.$i.en.txt\n",
    "fairseq-score --sys ./spm-results/result.$i.en.txt --ref kftt-data-1.0/data/orig/kyoto-test.en >> ./spm-results/BLEU.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.3, 9.9, 10.48, 10.37, 10.31]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "fname = './spm-results/BLEU.txt'\n",
    "def read_score(fname):\n",
    "    score = []\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('BLEU4'):\n",
    "                match = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', line)\n",
    "                score.append(float(match.group(0)))\n",
    "    print(score)\n",
    "    return score\n",
    "\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = read_score(fname)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"beam\")\n",
    "plt.ylabel(\"BLEU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.29' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "TensorBoard 2.13.0 at http://localhost:8000/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir tensorboard --host=localhost --port=8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.ハイパー・パラメータの調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-10 18:55:16 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:12671\n",
      "2023-07-10 18:55:16 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:12671\n",
      "2023-07-10 18:55:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "2023-07-10 18:55:17 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-07-10 18:55:17 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "2023-07-10 18:55:17 | INFO | fairseq.distributed.utils | initialized host opus3 as rank 0\n",
      "2023-07-10 18:55:17 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "2023-07-10 18:55:17 | INFO | fairseq.distributed.utils | initialized host opus3 as rank 1\n",
      "2023-07-10 18:55:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard-97', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:12671', 'distributed_port': 12671, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 8000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 8000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-97', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard-97', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=8000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=100, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=8000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=2, distributed_num_procs=2, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-97', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=100, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-spm-bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 18:55:19 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 18:55:19 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(34208, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(32008, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=32008, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | num. shared model params: 78,041,088 (num. trained: 78,041,088)\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-07-10 18:55:20 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-spm-bin/valid.ja-en.ja\n",
      "2023-07-10 18:55:20 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: data-spm-bin/valid.ja-en.en\n",
      "2023-07-10 18:55:20 | INFO | fairseq.tasks.translation | data-spm-bin valid ja-en 1166 examples\n",
      "2023-07-10 18:55:20 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-07-10 18:55:20 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "2023-07-10 18:55:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-07-10 18:55:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\n",
      "2023-07-10 18:55:20 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.651 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-10 18:55:20 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-10 18:55:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)\n",
      "2023-07-10 18:55:20 | INFO | fairseq_cli.train | max tokens per device = 8000 and max sentences per device = None\n",
      "2023-07-10 18:55:20 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 18:55:20 | INFO | fairseq.trainer | No existing checkpoint found checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 18:55:20 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-07-10 18:55:20 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-spm-bin/train.ja-en.ja\n",
      "2023-07-10 18:55:20 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: data-spm-bin/train.ja-en.en\n",
      "2023-07-10 18:55:20 | INFO | fairseq.tasks.translation | data-spm-bin train ja-en 440288 examples\n",
      "2023-07-10 18:55:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:55:20 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:55:20 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:55:20 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:55:21 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-07-10 18:55:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:55:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 18:55:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 18:55:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 18:55:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 001:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 18:55:25 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-07-10 18:55:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2023-07-10 18:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                              | 1/922 [00:05<1:23:59,  5.47s/it]2023-07-10 18:55:31 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.\n",
      "2023-07-10 18:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   4%|█▏                             | 36/922 [00:10<01:55,  7.67it/s]2023-07-10 18:55:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   9%|██▉                            | 87/922 [00:16<01:51,  7.47it/s]2023-07-10 18:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:  11%|███▎                          | 103/922 [00:18<01:47,  7.61it/s]2023-07-10 18:55:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:55:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:55:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.636 | nll_loss 11.16 | ppl 2288.72 | wps 262196 | wpb 7018.5 | bsz 290 | num_updates 100\n",
      "2023-07-10 18:55:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 100 updates\n",
      "2023-07-10 18:55:45 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_100.pt\n",
      "2023-07-10 18:55:48 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_100.pt\n",
      "2023-07-10 18:55:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_100.pt (epoch 1 @ 100 updates, score 11.636) (writing took 9.706617104122415 seconds)\n",
      "epoch 001:  22%|▏| 203/922 [00:42<01:34,  7.64it/s, loss=13.065, nll_loss=12.7642023-07-10 18:56:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:56:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 25.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:56:08 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.601 | nll_loss 9.91 | ppl 961.92 | wps 268707 | wpb 7018.5 | bsz 290 | num_updates 200 | best_loss 10.601\n",
      "2023-07-10 18:56:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 200 updates\n",
      "2023-07-10 18:56:08 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_200.pt\n",
      "2023-07-10 18:56:11 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_200.pt\n",
      "2023-07-10 18:56:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 10.601) (writing took 9.556549183093011 seconds)\n",
      "epoch 001:  33%|▎| 303/922 [01:04<01:20,  7.66it/s, loss=10.877, nll_loss=10.2682023-07-10 18:56:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:56:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:56:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.044 | nll_loss 9.252 | ppl 609.83 | wps 266645 | wpb 7018.5 | bsz 290 | num_updates 300 | best_loss 10.044\n",
      "2023-07-10 18:56:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 300 updates\n",
      "2023-07-10 18:56:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_300.pt\n",
      "2023-07-10 18:56:34 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_300.pt\n",
      "2023-07-10 18:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_300.pt (epoch 1 @ 300 updates, score 10.044) (writing took 9.42009000503458 seconds)\n",
      "epoch 001:  44%|▍| 403/922 [01:27<01:08,  7.57it/s, loss=10.126, nll_loss=9.372,2023-07-10 18:56:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:56:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 22.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:56:53 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.601 | nll_loss 8.758 | ppl 432.86 | wps 262047 | wpb 7018.5 | bsz 290 | num_updates 400 | best_loss 9.601\n",
      "2023-07-10 18:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 400 updates\n",
      "2023-07-10 18:56:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_400.pt\n",
      "2023-07-10 18:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_400.pt\n",
      "2023-07-10 18:57:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_400.pt (epoch 1 @ 400 updates, score 9.601) (writing took 9.760089579038322 seconds)\n",
      "epoch 001:  55%|▌| 503/922 [01:50<00:56,  7.38it/s, loss=9.567, nll_loss=8.729, 2023-07-10 18:57:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:57:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:57:17 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.268 | nll_loss 8.349 | ppl 325.95 | wps 267408 | wpb 7018.5 | bsz 290 | num_updates 500 | best_loss 9.268\n",
      "2023-07-10 18:57:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 500 updates\n",
      "2023-07-10 18:57:17 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_500.pt\n",
      "2023-07-10 18:57:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_500.pt\n",
      "2023-07-10 18:57:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_500.pt (epoch 1 @ 500 updates, score 9.268) (writing took 9.326668106019497 seconds)\n",
      "epoch 001:  65%|▋| 603/922 [02:13<00:40,  7.85it/s, loss=9.203, nll_loss=8.305, 2023-07-10 18:57:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:57:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:57:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.123 | nll_loss 8.196 | ppl 293.32 | wps 262619 | wpb 7018.5 | bsz 290 | num_updates 600 | best_loss 9.123\n",
      "2023-07-10 18:57:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 600 updates\n",
      "2023-07-10 18:57:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_600.pt\n",
      "2023-07-10 18:57:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_600.pt\n",
      "2023-07-10 18:57:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_600.pt (epoch 1 @ 600 updates, score 9.123) (writing took 9.716698969947174 seconds)\n",
      "epoch 001:  76%|▊| 703/922 [02:36<00:29,  7.55it/s, loss=8.896, nll_loss=7.947, 2023-07-10 18:58:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:58:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:58:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.735 | nll_loss 7.74 | ppl 213.83 | wps 259592 | wpb 7018.5 | bsz 290 | num_updates 700 | best_loss 8.735\n",
      "2023-07-10 18:58:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 700 updates\n",
      "2023-07-10 18:58:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_700.pt\n",
      "2023-07-10 18:58:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_700.pt\n",
      "2023-07-10 18:58:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_700.pt (epoch 1 @ 700 updates, score 8.735) (writing took 9.268496931996197 seconds)\n",
      "epoch 001:  87%|▊| 803/922 [02:59<00:15,  7.58it/s, loss=8.566, nll_loss=7.561, 2023-07-10 18:58:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:58:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:58:25 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.515 | nll_loss 7.467 | ppl 176.99 | wps 250577 | wpb 7018.5 | bsz 290 | num_updates 800 | best_loss 8.515\n",
      "2023-07-10 18:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 800 updates\n",
      "2023-07-10 18:58:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_800.pt\n",
      "2023-07-10 18:58:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_800.pt\n",
      "2023-07-10 18:58:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_800.pt (epoch 1 @ 800 updates, score 8.515) (writing took 10.26767832506448 seconds)\n",
      "epoch 001:  98%|▉| 903/922 [03:23<00:02,  7.60it/s, loss=8.375, nll_loss=7.337, 2023-07-10 18:58:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:58:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:58:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.33 | nll_loss 7.26 | ppl 153.29 | wps 261434 | wpb 7018.5 | bsz 290 | num_updates 900 | best_loss 8.33\n",
      "2023-07-10 18:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 900 updates\n",
      "2023-07-10 18:58:49 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_900.pt\n",
      "2023-07-10 18:58:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_1_900.pt\n",
      "2023-07-10 18:58:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_1_900.pt (epoch 1 @ 900 updates, score 8.33) (writing took 9.417037288891152 seconds)\n",
      "epoch 001: 100%|▉| 921/922 [03:35<00:00,  7.35it/s, loss=8.192, nll_loss=7.121, 2023-07-10 18:59:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:59:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.90it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:59:01 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.253 | nll_loss 7.171 | ppl 144.09 | wps 271520 | wpb 7018.5 | bsz 290 | num_updates 918 | best_loss 8.253\n",
      "2023-07-10 18:59:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 918 updates\n",
      "2023-07-10 18:59:01 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 18:59:04 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 18:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_best.pt (epoch 1 @ 918 updates, score 8.253) (writing took 7.1699869921430945 seconds)\n",
      "2023-07-10 18:59:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-07-10 18:59:08 | INFO | train | epoch 001 | loss 9.62 | nll_loss 8.786 | ppl 441.49 | wps 60602.1 | ups 4.23 | wpb 14320.2 | bsz 473.6 | num_updates 918 | lr 0.000459 | gnorm 1.547 | clip 83.7 | loss_scale 8 | train_wall 120 | gb_free 18.2 | wall 228\n",
      "2023-07-10 18:59:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 18:59:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 002:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 18:59:08 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-07-10 18:59:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:   9%|██▋                            | 81/922 [00:10<01:56,  7.25it/s]2023-07-10 18:59:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:59:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:59:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.154 | nll_loss 7.057 | ppl 133.19 | wps 270908 | wpb 7018.5 | bsz 290 | num_updates 1000 | best_loss 8.154\n",
      "2023-07-10 18:59:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1000 updates\n",
      "2023-07-10 18:59:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1000.pt\n",
      "2023-07-10 18:59:23 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1000.pt\n",
      "2023-07-10 18:59:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1000.pt (epoch 2 @ 1000 updates, score 8.154) (writing took 9.463808428030461 seconds)\n",
      "epoch 002:  20%|▏| 181/922 [00:33<01:37,  7.64it/s, loss=7.945, nll_loss=6.834, 2023-07-10 18:59:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 18:59:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.52it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 18:59:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.99 | nll_loss 6.837 | ppl 114.31 | wps 268224 | wpb 7018.5 | bsz 290 | num_updates 1100 | best_loss 7.99\n",
      "2023-07-10 18:59:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1100 updates\n",
      "2023-07-10 18:59:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1100.pt\n",
      "2023-07-10 18:59:46 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1100.pt\n",
      "2023-07-10 18:59:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1100.pt (epoch 2 @ 1100 updates, score 7.99) (writing took 9.46965170884505 seconds)\n",
      "epoch 002:  30%|▎| 281/922 [00:56<01:24,  7.56it/s, loss=7.802, nll_loss=6.664, 2023-07-10 19:00:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:00:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:00:05 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.84 | nll_loss 6.678 | ppl 102.43 | wps 271892 | wpb 7018.5 | bsz 290 | num_updates 1200 | best_loss 7.84\n",
      "2023-07-10 19:00:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1200 updates\n",
      "2023-07-10 19:00:06 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1200.pt\n",
      "2023-07-10 19:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1200.pt\n",
      "2023-07-10 19:00:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1200.pt (epoch 2 @ 1200 updates, score 7.84) (writing took 9.830160016193986 seconds)\n",
      "epoch 002:  41%|▍| 381/922 [01:20<01:12,  7.47it/s, loss=7.663, nll_loss=6.5, pp2023-07-10 19:00:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:00:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:00:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.65 | nll_loss 6.455 | ppl 87.75 | wps 267210 | wpb 7018.5 | bsz 290 | num_updates 1300 | best_loss 7.65\n",
      "2023-07-10 19:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1300 updates\n",
      "2023-07-10 19:00:29 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1300.pt\n",
      "2023-07-10 19:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1300.pt\n",
      "2023-07-10 19:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1300.pt (epoch 2 @ 1300 updates, score 7.65) (writing took 9.554138910025358 seconds)\n",
      "epoch 002:  52%|▌| 481/922 [01:43<00:58,  7.50it/s, loss=7.544, nll_loss=6.359, 2023-07-10 19:00:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:00:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:00:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.612 | nll_loss 6.414 | ppl 85.3 | wps 267806 | wpb 7018.5 | bsz 290 | num_updates 1400 | best_loss 7.612\n",
      "2023-07-10 19:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1400 updates\n",
      "2023-07-10 19:00:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1400.pt\n",
      "2023-07-10 19:00:55 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1400.pt\n",
      "2023-07-10 19:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1400.pt (epoch 2 @ 1400 updates, score 7.612) (writing took 9.440287256147712 seconds)\n",
      "epoch 002:  63%|▋| 581/922 [02:06<00:45,  7.50it/s, loss=7.472, nll_loss=6.271, 2023-07-10 19:01:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:01:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.50it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:01:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.461 | nll_loss 6.224 | ppl 74.75 | wps 263854 | wpb 7018.5 | bsz 290 | num_updates 1500 | best_loss 7.461\n",
      "2023-07-10 19:01:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1500 updates\n",
      "2023-07-10 19:01:15 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1500.pt\n",
      "2023-07-10 19:01:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1500.pt\n",
      "2023-07-10 19:01:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1500.pt (epoch 2 @ 1500 updates, score 7.461) (writing took 11.576995595125481 seconds)\n",
      "epoch 002:  74%|▋| 681/922 [02:31<00:31,  7.70it/s, loss=7.427, nll_loss=6.218, 2023-07-10 19:01:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:01:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:01:40 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.403 | nll_loss 6.174 | ppl 72.2 | wps 262517 | wpb 7018.5 | bsz 290 | num_updates 1600 | best_loss 7.403\n",
      "2023-07-10 19:01:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1600 updates\n",
      "2023-07-10 19:01:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1600.pt\n",
      "2023-07-10 19:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1600.pt\n",
      "2023-07-10 19:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1600.pt (epoch 2 @ 1600 updates, score 7.403) (writing took 9.787957132793963 seconds)\n",
      "epoch 002:  85%|▊| 781/922 [02:54<00:18,  7.58it/s, loss=7.224, nll_loss=5.985, 2023-07-10 19:02:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:02:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:02:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.42 | nll_loss 6.187 | ppl 72.85 | wps 267328 | wpb 7018.5 | bsz 290 | num_updates 1700 | best_loss 7.403\n",
      "2023-07-10 19:02:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1700 updates\n",
      "2023-07-10 19:02:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1700.pt\n",
      "2023-07-10 19:02:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1700.pt\n",
      "2023-07-10 19:02:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1700.pt (epoch 2 @ 1700 updates, score 7.42) (writing took 7.199426203966141 seconds)\n",
      "epoch 002:  96%|▉| 881/922 [03:15<00:05,  7.66it/s, loss=7.266, nll_loss=6.031, 2023-07-10 19:02:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:02:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:02:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.343 | nll_loss 6.072 | ppl 67.27 | wps 269868 | wpb 7018.5 | bsz 290 | num_updates 1800 | best_loss 7.343\n",
      "2023-07-10 19:02:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1800 updates\n",
      "2023-07-10 19:02:24 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1800.pt\n",
      "2023-07-10 19:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_2_1800.pt\n",
      "2023-07-10 19:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_2_1800.pt (epoch 2 @ 1800 updates, score 7.343) (writing took 9.37695285701193 seconds)\n",
      "epoch 002: 100%|▉| 921/922 [03:30<00:00,  7.67it/s, loss=7.134, nll_loss=5.88, p2023-07-10 19:02:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:02:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 22.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:02:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.262 | nll_loss 5.996 | ppl 63.82 | wps 259768 | wpb 7018.5 | bsz 290 | num_updates 1840 | best_loss 7.262\n",
      "2023-07-10 19:02:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1840 updates\n",
      "2023-07-10 19:02:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:02:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:02:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_best.pt (epoch 2 @ 1840 updates, score 7.262) (writing took 7.207397315185517 seconds)\n",
      "2023-07-10 19:02:47 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-07-10 19:02:47 | INFO | train | epoch 002 | loss 7.475 | nll_loss 6.278 | ppl 77.62 | wps 60486.8 | ups 4.22 | wpb 14321.4 | bsz 477.5 | num_updates 1840 | lr 0.00092 | gnorm 0.937 | clip 26.5 | loss_scale 8 | train_wall 120 | gb_free 17.9 | wall 446\n",
      "2023-07-10 19:02:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:02:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 003:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:02:47 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-07-10 19:02:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:   6%|█▉                             | 59/922 [00:07<01:55,  7.47it/s]2023-07-10 19:02:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:02:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:02:55 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.25 | nll_loss 5.964 | ppl 62.44 | wps 270954 | wpb 7018.5 | bsz 290 | num_updates 1900 | best_loss 7.25\n",
      "2023-07-10 19:02:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1900 updates\n",
      "2023-07-10 19:02:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_1900.pt\n",
      "2023-07-10 19:02:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_1900.pt\n",
      "2023-07-10 19:03:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_1900.pt (epoch 3 @ 1900 updates, score 7.25) (writing took 9.474993051029742 seconds)\n",
      "epoch 003:  17%|▏| 159/922 [00:30<01:40,  7.57it/s, loss=7.02, nll_loss=5.75, pp2023-07-10 19:03:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:03:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:03:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.172 | nll_loss 5.886 | ppl 59.15 | wps 263266 | wpb 7018.5 | bsz 290 | num_updates 2000 | best_loss 7.172\n",
      "2023-07-10 19:03:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2000 updates\n",
      "2023-07-10 19:03:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2000.pt\n",
      "2023-07-10 19:03:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2000.pt\n",
      "2023-07-10 19:03:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2000.pt (epoch 3 @ 2000 updates, score 7.172) (writing took 9.54370461916551 seconds)\n",
      "epoch 003:  28%|▎| 259/922 [00:53<01:28,  7.47it/s, loss=6.938, nll_loss=5.651, 2023-07-10 19:03:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:03:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.96it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:03:41 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.037 | nll_loss 5.726 | ppl 52.94 | wps 264162 | wpb 7018.5 | bsz 290 | num_updates 2100 | best_loss 7.037\n",
      "2023-07-10 19:03:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2100 updates\n",
      "2023-07-10 19:03:41 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2100.pt\n",
      "2023-07-10 19:03:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2100.pt\n",
      "2023-07-10 19:03:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2100.pt (epoch 3 @ 2100 updates, score 7.037) (writing took 9.841090589994565 seconds)\n",
      "epoch 003:  39%|▍| 359/922 [01:17<01:14,  7.54it/s, loss=6.858, nll_loss=5.56, p2023-07-10 19:04:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:04:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:04:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.926 | nll_loss 5.606 | ppl 48.7 | wps 269470 | wpb 7018.5 | bsz 290 | num_updates 2200 | best_loss 6.926\n",
      "2023-07-10 19:04:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2200 updates\n",
      "2023-07-10 19:04:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2200.pt\n",
      "2023-07-10 19:04:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2200.pt\n",
      "2023-07-10 19:04:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2200.pt (epoch 3 @ 2200 updates, score 6.926) (writing took 9.352624895982444 seconds)\n",
      "epoch 003:  50%|▍| 459/922 [01:40<01:01,  7.50it/s, loss=6.764, nll_loss=5.452, 2023-07-10 19:04:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:04:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:04:28 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.85 | nll_loss 5.511 | ppl 45.6 | wps 263479 | wpb 7018.5 | bsz 290 | num_updates 2300 | best_loss 6.85\n",
      "2023-07-10 19:04:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2300 updates\n",
      "2023-07-10 19:04:28 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2300.pt\n",
      "2023-07-10 19:04:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2300.pt\n",
      "2023-07-10 19:04:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2300.pt (epoch 3 @ 2300 updates, score 6.85) (writing took 9.653360493015498 seconds)\n",
      "epoch 003:  61%|▌| 559/922 [02:03<00:48,  7.54it/s, loss=6.619, nll_loss=5.287, 2023-07-10 19:04:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:04:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.92it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:04:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.744 | nll_loss 5.387 | ppl 41.84 | wps 267824 | wpb 7018.5 | bsz 290 | num_updates 2400 | best_loss 6.744\n",
      "2023-07-10 19:04:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2400 updates\n",
      "2023-07-10 19:04:51 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2400.pt\n",
      "2023-07-10 19:04:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2400.pt\n",
      "2023-07-10 19:05:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2400.pt (epoch 3 @ 2400 updates, score 6.744) (writing took 9.650514485081658 seconds)\n",
      "epoch 003:  71%|▋| 659/922 [02:26<00:34,  7.57it/s, loss=6.603, nll_loss=5.269, 2023-07-10 19:05:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:05:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 25.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:05:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.62 | nll_loss 5.242 | ppl 37.85 | wps 261444 | wpb 7018.5 | bsz 290 | num_updates 2500 | best_loss 6.62\n",
      "2023-07-10 19:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2500 updates\n",
      "2023-07-10 19:05:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2500.pt\n",
      "2023-07-10 19:05:17 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2500.pt\n",
      "2023-07-10 19:05:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2500.pt (epoch 3 @ 2500 updates, score 6.62) (writing took 9.709772383794188 seconds)\n",
      "epoch 003:  82%|▊| 759/922 [02:50<00:22,  7.33it/s, loss=6.423, nll_loss=5.064, 2023-07-10 19:05:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:05:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.60it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:05:38 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.55 | nll_loss 5.167 | ppl 35.92 | wps 269233 | wpb 7018.5 | bsz 290 | num_updates 2600 | best_loss 6.55\n",
      "2023-07-10 19:05:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2600 updates\n",
      "2023-07-10 19:05:38 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2600.pt\n",
      "2023-07-10 19:05:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2600.pt\n",
      "2023-07-10 19:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2600.pt (epoch 3 @ 2600 updates, score 6.55) (writing took 10.932080355007201 seconds)\n",
      "epoch 003:  93%|▉| 859/922 [03:15<00:08,  7.55it/s, loss=6.352, nll_loss=4.983, 2023-07-10 19:06:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:06:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:06:02 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.505 | nll_loss 5.118 | ppl 34.73 | wps 269677 | wpb 7018.5 | bsz 290 | num_updates 2700 | best_loss 6.505\n",
      "2023-07-10 19:06:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2700 updates\n",
      "2023-07-10 19:06:02 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2700.pt\n",
      "2023-07-10 19:06:05 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_3_2700.pt\n",
      "2023-07-10 19:06:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_3_2700.pt (epoch 3 @ 2700 updates, score 6.505) (writing took 9.41928275115788 seconds)\n",
      "epoch 003: 100%|▉| 921/922 [03:32<00:00,  7.55it/s, loss=6.332, nll_loss=4.961, 2023-07-10 19:06:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:06:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:06:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.403 | nll_loss 5.001 | ppl 32.03 | wps 245391 | wpb 7018.5 | bsz 290 | num_updates 2762 | best_loss 6.403\n",
      "2023-07-10 19:06:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2762 updates\n",
      "2023-07-10 19:06:20 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:06:23 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:06:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_best.pt (epoch 3 @ 2762 updates, score 6.403) (writing took 7.392064342973754 seconds)\n",
      "2023-07-10 19:06:27 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-07-10 19:06:27 | INFO | train | epoch 003 | loss 6.612 | nll_loss 5.279 | ppl 38.84 | wps 59784.7 | ups 4.17 | wpb 14321.4 | bsz 477.5 | num_updates 2762 | lr 0.000850948 | gnorm 0.738 | clip 7.6 | loss_scale 8 | train_wall 120 | gb_free 17.9 | wall 667\n",
      "2023-07-10 19:06:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:06:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 004:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:06:28 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-07-10 19:06:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:   4%|█▏                             | 37/922 [00:04<01:57,  7.54it/s]2023-07-10 19:06:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:06:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:06:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.35 | nll_loss 4.929 | ppl 30.47 | wps 252398 | wpb 7018.5 | bsz 290 | num_updates 2800 | best_loss 6.35\n",
      "2023-07-10 19:06:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2800 updates\n",
      "2023-07-10 19:06:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_2800.pt\n",
      "2023-07-10 19:06:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_2800.pt\n",
      "2023-07-10 19:06:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_2800.pt (epoch 4 @ 2800 updates, score 6.35) (writing took 9.327678326051682 seconds)\n",
      "epoch 004:  15%|▏| 137/922 [00:27<01:41,  7.73it/s, loss=6.188, nll_loss=4.796, 2023-07-10 19:06:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:06:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:06:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.353 | nll_loss 4.93 | ppl 30.49 | wps 253769 | wpb 7018.5 | bsz 290 | num_updates 2900 | best_loss 6.35\n",
      "2023-07-10 19:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 2900 updates\n",
      "2023-07-10 19:06:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_2900.pt\n",
      "2023-07-10 19:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_2900.pt\n",
      "2023-07-10 19:07:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_2900.pt (epoch 4 @ 2900 updates, score 6.353) (writing took 6.830135829048231 seconds)\n",
      "epoch 004:  26%|▎| 237/922 [00:48<01:31,  7.50it/s, loss=6.037, nll_loss=4.622, 2023-07-10 19:07:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:07:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.14it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:07:16 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.284 | nll_loss 4.861 | ppl 29.05 | wps 271115 | wpb 7018.5 | bsz 290 | num_updates 3000 | best_loss 6.284\n",
      "2023-07-10 19:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3000 updates\n",
      "2023-07-10 19:07:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3000.pt\n",
      "2023-07-10 19:07:19 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3000.pt\n",
      "2023-07-10 19:07:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3000.pt (epoch 4 @ 3000 updates, score 6.284) (writing took 9.572349153924733 seconds)\n",
      "epoch 004:  37%|▎| 337/922 [01:11<01:16,  7.60it/s, loss=6.013, nll_loss=4.592, 2023-07-10 19:07:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:07:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:07:39 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.192 | nll_loss 4.75 | ppl 26.91 | wps 267427 | wpb 7018.5 | bsz 290 | num_updates 3100 | best_loss 6.192\n",
      "2023-07-10 19:07:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3100 updates\n",
      "2023-07-10 19:07:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3100.pt\n",
      "2023-07-10 19:07:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3100.pt\n",
      "2023-07-10 19:07:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3100.pt (epoch 4 @ 3100 updates, score 6.192) (writing took 9.130625762976706 seconds)\n",
      "epoch 004:  47%|▍| 437/922 [01:33<01:05,  7.42it/s, loss=5.89, nll_loss=4.452, p2023-07-10 19:08:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:08:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:08:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.12 | nll_loss 4.669 | ppl 25.43 | wps 268310 | wpb 7018.5 | bsz 290 | num_updates 3200 | best_loss 6.12\n",
      "2023-07-10 19:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3200 updates\n",
      "2023-07-10 19:08:02 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3200.pt\n",
      "2023-07-10 19:08:05 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3200.pt\n",
      "2023-07-10 19:08:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3200.pt (epoch 4 @ 3200 updates, score 6.12) (writing took 10.025647850940004 seconds)\n",
      "epoch 004:  58%|▌| 537/922 [01:57<00:50,  7.60it/s, loss=5.888, nll_loss=4.449, 2023-07-10 19:08:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:08:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:08:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.039 | nll_loss 4.573 | ppl 23.8 | wps 260658 | wpb 7018.5 | bsz 290 | num_updates 3300 | best_loss 6.039\n",
      "2023-07-10 19:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3300 updates\n",
      "2023-07-10 19:08:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3300.pt\n",
      "2023-07-10 19:08:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3300.pt\n",
      "2023-07-10 19:08:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3300.pt (epoch 4 @ 3300 updates, score 6.039) (writing took 9.308421703055501 seconds)\n",
      "epoch 004:  69%|▋| 637/922 [02:20<00:39,  7.19it/s, loss=5.894, nll_loss=4.456, 2023-07-10 19:08:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:08:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.53it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:08:48 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.975 | nll_loss 4.495 | ppl 22.56 | wps 263673 | wpb 7018.5 | bsz 290 | num_updates 3400 | best_loss 5.975\n",
      "2023-07-10 19:08:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3400 updates\n",
      "2023-07-10 19:08:48 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3400.pt\n",
      "2023-07-10 19:08:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3400.pt\n",
      "2023-07-10 19:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3400.pt (epoch 4 @ 3400 updates, score 5.975) (writing took 9.618670178111643 seconds)\n",
      "epoch 004:  80%|▊| 737/922 [02:43<00:24,  7.53it/s, loss=5.771, nll_loss=4.317, 2023-07-10 19:09:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:09:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:09:12 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.98 | nll_loss 4.499 | ppl 22.61 | wps 261263 | wpb 7018.5 | bsz 290 | num_updates 3500 | best_loss 5.975\n",
      "2023-07-10 19:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3500 updates\n",
      "2023-07-10 19:09:12 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3500.pt\n",
      "2023-07-10 19:09:15 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3500.pt\n",
      "2023-07-10 19:09:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3500.pt (epoch 4 @ 3500 updates, score 5.98) (writing took 7.7773802110459656 seconds)\n",
      "epoch 004:  91%|▉| 837/922 [03:05<00:11,  7.43it/s, loss=5.722, nll_loss=4.261, 2023-07-10 19:09:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:09:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:09:34 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.933 | nll_loss 4.448 | ppl 21.82 | wps 270033 | wpb 7018.5 | bsz 290 | num_updates 3600 | best_loss 5.933\n",
      "2023-07-10 19:09:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3600 updates\n",
      "2023-07-10 19:09:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3600.pt\n",
      "2023-07-10 19:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_4_3600.pt\n",
      "2023-07-10 19:09:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_4_3600.pt (epoch 4 @ 3600 updates, score 5.933) (writing took 9.392742190044373 seconds)\n",
      "epoch 004: 100%|▉| 921/922 [03:26<00:00,  7.92it/s, loss=5.649, nll_loss=4.177, 2023-07-10 19:09:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:09:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:09:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.781 | nll_loss 4.28 | ppl 19.42 | wps 268097 | wpb 7018.5 | bsz 290 | num_updates 3684 | best_loss 5.781\n",
      "2023-07-10 19:09:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 3684 updates\n",
      "2023-07-10 19:09:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:09:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:10:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_best.pt (epoch 4 @ 3684 updates, score 5.781) (writing took 7.145758340833709 seconds)\n",
      "2023-07-10 19:10:02 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-07-10 19:10:02 | INFO | train | epoch 004 | loss 5.836 | nll_loss 4.391 | ppl 20.98 | wps 61640 | ups 4.3 | wpb 14321.4 | bsz 477.5 | num_updates 3684 | lr 0.000736809 | gnorm 0.665 | clip 3.3 | loss_scale 8 | train_wall 120 | gb_free 18.1 | wall 881\n",
      "2023-07-10 19:10:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:10:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 005:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:10:02 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-07-10 19:10:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:   2%|▌                              | 15/922 [00:02<02:00,  7.50it/s]2023-07-10 19:10:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:10:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:10:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.804 | nll_loss 4.303 | ppl 19.74 | wps 262185 | wpb 7018.5 | bsz 290 | num_updates 3700 | best_loss 5.781\n",
      "2023-07-10 19:10:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3700 updates\n",
      "2023-07-10 19:10:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_3700.pt\n",
      "2023-07-10 19:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_3700.pt\n",
      "2023-07-10 19:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_3700.pt (epoch 5 @ 3700 updates, score 5.804) (writing took 7.046911215875298 seconds)\n",
      "epoch 005:  12%| | 115/922 [00:22<01:43,  7.80it/s, loss=5.548, nll_loss=4.063, 2023-07-10 19:10:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:10:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.79it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:10:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.805 | nll_loss 4.298 | ppl 19.67 | wps 265552 | wpb 7018.5 | bsz 290 | num_updates 3800 | best_loss 5.781\n",
      "2023-07-10 19:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3800 updates\n",
      "2023-07-10 19:10:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_3800.pt\n",
      "2023-07-10 19:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_3800.pt\n",
      "2023-07-10 19:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_3800.pt (epoch 5 @ 3800 updates, score 5.805) (writing took 6.801266340073198 seconds)\n",
      "epoch 005:  23%|▏| 215/922 [00:43<01:32,  7.64it/s, loss=5.399, nll_loss=3.89, p2023-07-10 19:10:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:10:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.98it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:10:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.739 | nll_loss 4.22 | ppl 18.64 | wps 269213 | wpb 7018.5 | bsz 290 | num_updates 3900 | best_loss 5.739\n",
      "2023-07-10 19:10:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 3900 updates\n",
      "2023-07-10 19:10:46 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_3900.pt\n",
      "2023-07-10 19:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_3900.pt\n",
      "2023-07-10 19:10:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_3900.pt (epoch 5 @ 3900 updates, score 5.739) (writing took 9.323719182051718 seconds)\n",
      "epoch 005:  34%|▎| 315/922 [01:06<01:20,  7.53it/s, loss=5.44, nll_loss=3.936, p2023-07-10 19:11:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:11:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:11:09 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.678 | nll_loss 4.145 | ppl 17.69 | wps 262088 | wpb 7018.5 | bsz 290 | num_updates 4000 | best_loss 5.678\n",
      "2023-07-10 19:11:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4000 updates\n",
      "2023-07-10 19:11:09 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4000.pt\n",
      "2023-07-10 19:11:12 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4000.pt\n",
      "2023-07-10 19:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4000.pt (epoch 5 @ 4000 updates, score 5.678) (writing took 9.323587594088167 seconds)\n",
      "epoch 005:  45%|▍| 415/922 [01:29<01:06,  7.62it/s, loss=5.359, nll_loss=3.844, 2023-07-10 19:11:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:11:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:11:32 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.619 | nll_loss 4.083 | ppl 16.95 | wps 267432 | wpb 7018.5 | bsz 290 | num_updates 4100 | best_loss 5.619\n",
      "2023-07-10 19:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4100 updates\n",
      "2023-07-10 19:11:32 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4100.pt\n",
      "2023-07-10 19:11:35 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4100.pt\n",
      "2023-07-10 19:11:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4100.pt (epoch 5 @ 4100 updates, score 5.619) (writing took 9.827650321880355 seconds)\n",
      "epoch 005:  56%|▌| 515/922 [01:52<00:53,  7.65it/s, loss=5.336, nll_loss=3.819, 2023-07-10 19:11:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:11:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.57it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:11:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.583 | nll_loss 4.049 | ppl 16.55 | wps 270135 | wpb 7018.5 | bsz 290 | num_updates 4200 | best_loss 5.583\n",
      "2023-07-10 19:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4200 updates\n",
      "2023-07-10 19:11:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4200.pt\n",
      "2023-07-10 19:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4200.pt\n",
      "2023-07-10 19:12:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4200.pt (epoch 5 @ 4200 updates, score 5.583) (writing took 9.317590224090964 seconds)\n",
      "epoch 005:  67%|▋| 615/922 [02:15<00:39,  7.74it/s, loss=5.304, nll_loss=3.784, 2023-07-10 19:12:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:12:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.37it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:12:18 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.581 | nll_loss 4.038 | ppl 16.43 | wps 268170 | wpb 7018.5 | bsz 290 | num_updates 4300 | best_loss 5.581\n",
      "2023-07-10 19:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4300 updates\n",
      "2023-07-10 19:12:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4300.pt\n",
      "2023-07-10 19:12:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4300.pt\n",
      "2023-07-10 19:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4300.pt (epoch 5 @ 4300 updates, score 5.581) (writing took 9.947501020971686 seconds)\n",
      "epoch 005:  78%|▊| 715/922 [02:39<00:26,  7.69it/s, loss=5.314, nll_loss=3.795, 2023-07-10 19:12:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:12:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:12:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.524 | nll_loss 3.983 | ppl 15.81 | wps 257373 | wpb 7018.5 | bsz 290 | num_updates 4400 | best_loss 5.524\n",
      "2023-07-10 19:12:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4400 updates\n",
      "2023-07-10 19:12:41 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4400.pt\n",
      "2023-07-10 19:12:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4400.pt\n",
      "2023-07-10 19:12:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4400.pt (epoch 5 @ 4400 updates, score 5.524) (writing took 9.247878782916814 seconds)\n",
      "epoch 005:  88%|▉| 815/922 [03:01<00:14,  7.53it/s, loss=5.233, nll_loss=3.704, 2023-07-10 19:13:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:13:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:13:04 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.462 | nll_loss 3.921 | ppl 15.15 | wps 270591 | wpb 7018.5 | bsz 290 | num_updates 4500 | best_loss 5.462\n",
      "2023-07-10 19:13:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4500 updates\n",
      "2023-07-10 19:13:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4500.pt\n",
      "2023-07-10 19:13:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4500.pt\n",
      "2023-07-10 19:13:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4500.pt (epoch 5 @ 4500 updates, score 5.462) (writing took 9.783889372833073 seconds)\n",
      "epoch 005:  99%|▉| 915/922 [03:25<00:00,  7.61it/s, loss=5.244, nll_loss=3.717, 2023-07-10 19:13:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:13:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 18.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:13:27 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.442 | nll_loss 3.9 | ppl 14.92 | wps 269432 | wpb 7018.5 | bsz 290 | num_updates 4600 | best_loss 5.442\n",
      "2023-07-10 19:13:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4600 updates\n",
      "2023-07-10 19:13:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4600.pt\n",
      "2023-07-10 19:13:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_5_4600.pt\n",
      "2023-07-10 19:13:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_5_4600.pt (epoch 5 @ 4600 updates, score 5.442) (writing took 9.975100602023304 seconds)\n",
      "epoch 005: 100%|▉| 921/922 [03:36<00:00,  1.53it/s, loss=5.263, nll_loss=3.739, 2023-07-10 19:13:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:13:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:13:39 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.449 | nll_loss 3.888 | ppl 14.8 | wps 256011 | wpb 7018.5 | bsz 290 | num_updates 4606 | best_loss 5.442\n",
      "2023-07-10 19:13:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 4606 updates\n",
      "2023-07-10 19:13:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:13:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:13:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_last.pt (epoch 5 @ 4606 updates, score 5.449) (writing took 3.4773686011321843 seconds)\n",
      "2023-07-10 19:13:42 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-07-10 19:13:42 | INFO | train | epoch 005 | loss 5.323 | nll_loss 3.805 | ppl 13.98 | wps 59936.5 | ups 4.19 | wpb 14321.4 | bsz 477.5 | num_updates 4606 | lr 0.000658951 | gnorm 0.653 | clip 2 | loss_scale 8 | train_wall 120 | gb_free 18.1 | wall 1102\n",
      "2023-07-10 19:13:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:13:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 006:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:13:42 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-07-10 19:13:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  10%|███▏                           | 93/922 [00:12<01:47,  7.74it/s]2023-07-10 19:13:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:13:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:13:55 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.458 | nll_loss 3.899 | ppl 14.92 | wps 271252 | wpb 7018.5 | bsz 290 | num_updates 4700 | best_loss 5.442\n",
      "2023-07-10 19:13:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4700 updates\n",
      "2023-07-10 19:13:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_4700.pt\n",
      "2023-07-10 19:13:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_4700.pt\n",
      "2023-07-10 19:14:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_4700.pt (epoch 6 @ 4700 updates, score 5.458) (writing took 6.910492194117978 seconds)\n",
      "epoch 006:  21%|▏| 193/922 [00:32<01:34,  7.72it/s, loss=5.02, nll_loss=3.461, p2023-07-10 19:14:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:14:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:14:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.435 | nll_loss 3.879 | ppl 14.72 | wps 264037 | wpb 7018.5 | bsz 290 | num_updates 4800 | best_loss 5.435\n",
      "2023-07-10 19:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4800 updates\n",
      "2023-07-10 19:14:15 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_4800.pt\n",
      "2023-07-10 19:14:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_4800.pt\n",
      "2023-07-10 19:14:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_4800.pt (epoch 6 @ 4800 updates, score 5.435) (writing took 12.668827930931002 seconds)\n",
      "epoch 006:  32%|▎| 293/922 [00:59<01:20,  7.83it/s, loss=4.973, nll_loss=3.407, 2023-07-10 19:14:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:14:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 25.31it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:14:42 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.407 | nll_loss 3.847 | ppl 14.39 | wps 264438 | wpb 7018.5 | bsz 290 | num_updates 4900 | best_loss 5.407\n",
      "2023-07-10 19:14:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 4900 updates\n",
      "2023-07-10 19:14:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_4900.pt\n",
      "2023-07-10 19:14:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_4900.pt\n",
      "2023-07-10 19:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_4900.pt (epoch 6 @ 4900 updates, score 5.407) (writing took 9.110631824005395 seconds)\n",
      "epoch 006:  43%|▍| 393/922 [01:21<01:10,  7.52it/s, loss=4.989, nll_loss=3.424, 2023-07-10 19:15:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:15:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:15:04 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.376 | nll_loss 3.807 | ppl 14 | wps 261562 | wpb 7018.5 | bsz 290 | num_updates 5000 | best_loss 5.376\n",
      "2023-07-10 19:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5000 updates\n",
      "2023-07-10 19:15:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5000.pt\n",
      "2023-07-10 19:15:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5000.pt\n",
      "2023-07-10 19:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_5000.pt (epoch 6 @ 5000 updates, score 5.376) (writing took 9.563470508903265 seconds)\n",
      "epoch 006:  53%|▌| 493/922 [01:44<00:55,  7.68it/s, loss=5.04, nll_loss=3.484, p2023-07-10 19:15:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:15:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:15:27 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.339 | nll_loss 3.771 | ppl 13.65 | wps 264546 | wpb 7018.5 | bsz 290 | num_updates 5100 | best_loss 5.339\n",
      "2023-07-10 19:15:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5100 updates\n",
      "2023-07-10 19:15:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5100.pt\n",
      "2023-07-10 19:15:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5100.pt\n",
      "2023-07-10 19:15:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_5100.pt (epoch 6 @ 5100 updates, score 5.339) (writing took 9.238585270941257 seconds)\n",
      "epoch 006:  64%|▋| 593/922 [02:07<00:43,  7.64it/s, loss=5.025, nll_loss=3.467, 2023-07-10 19:15:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:15:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 22.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:15:50 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.344 | nll_loss 3.769 | ppl 13.63 | wps 253973 | wpb 7018.5 | bsz 290 | num_updates 5200 | best_loss 5.339\n",
      "2023-07-10 19:15:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5200 updates\n",
      "2023-07-10 19:15:50 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5200.pt\n",
      "2023-07-10 19:15:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5200.pt\n",
      "2023-07-10 19:15:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_5200.pt (epoch 6 @ 5200 updates, score 5.344) (writing took 7.581776347011328 seconds)\n",
      "epoch 006:  75%|▊| 693/922 [02:28<00:31,  7.30it/s, loss=5.037, nll_loss=3.481, 2023-07-10 19:16:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:16:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:16:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.315 | nll_loss 3.744 | ppl 13.4 | wps 268819 | wpb 7018.5 | bsz 290 | num_updates 5300 | best_loss 5.315\n",
      "2023-07-10 19:16:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5300 updates\n",
      "2023-07-10 19:16:11 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5300.pt\n",
      "2023-07-10 19:16:14 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5300.pt\n",
      "2023-07-10 19:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_5300.pt (epoch 6 @ 5300 updates, score 5.315) (writing took 9.203952997922897 seconds)\n",
      "epoch 006:  86%|▊| 793/922 [02:51<00:16,  7.70it/s, loss=4.975, nll_loss=3.412, 2023-07-10 19:16:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:16:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:16:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.326 | nll_loss 3.756 | ppl 13.51 | wps 269104 | wpb 7018.5 | bsz 290 | num_updates 5400 | best_loss 5.315\n",
      "2023-07-10 19:16:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5400 updates\n",
      "2023-07-10 19:16:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5400.pt\n",
      "2023-07-10 19:16:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5400.pt\n",
      "2023-07-10 19:16:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_5400.pt (epoch 6 @ 5400 updates, score 5.326) (writing took 6.985630593029782 seconds)\n",
      "epoch 006:  97%|▉| 893/922 [03:12<00:03,  7.53it/s, loss=4.997, nll_loss=3.437, 2023-07-10 19:16:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:16:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.49it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:16:55 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.248 | nll_loss 3.667 | ppl 12.71 | wps 264830 | wpb 7018.5 | bsz 290 | num_updates 5500 | best_loss 5.248\n",
      "2023-07-10 19:16:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5500 updates\n",
      "2023-07-10 19:16:55 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5500.pt\n",
      "2023-07-10 19:16:58 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_6_5500.pt\n",
      "2023-07-10 19:17:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_6_5500.pt (epoch 6 @ 5500 updates, score 5.248) (writing took 9.663150429027155 seconds)\n",
      "epoch 006: 100%|▉| 921/922 [03:25<00:00,  7.59it/s, loss=4.925, nll_loss=3.357, 2023-07-10 19:17:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:17:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:17:08 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.268 | nll_loss 3.696 | ppl 12.96 | wps 253770 | wpb 7018.5 | bsz 290 | num_updates 5528 | best_loss 5.248\n",
      "2023-07-10 19:17:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 5528 updates\n",
      "2023-07-10 19:17:08 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:17:12 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_last.pt (epoch 6 @ 5528 updates, score 5.268) (writing took 3.6160643550101668 seconds)\n",
      "2023-07-10 19:17:12 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-07-10 19:17:12 | INFO | train | epoch 006 | loss 4.996 | nll_loss 3.435 | ppl 10.82 | wps 62933.3 | ups 4.39 | wpb 14321.4 | bsz 477.5 | num_updates 5528 | lr 0.000601494 | gnorm 0.639 | clip 2 | loss_scale 8 | train_wall 120 | gb_free 18 | wall 1311\n",
      "2023-07-10 19:17:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:17:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 007:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:17:12 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-07-10 19:17:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:   8%|██▍                            | 71/922 [00:09<01:55,  7.37it/s]2023-07-10 19:17:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:17:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:17:22 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.252 | nll_loss 3.669 | ppl 12.72 | wps 269917 | wpb 7018.5 | bsz 290 | num_updates 5600 | best_loss 5.248\n",
      "2023-07-10 19:17:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5600 updates\n",
      "2023-07-10 19:17:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5600.pt\n",
      "2023-07-10 19:17:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5600.pt\n",
      "2023-07-10 19:17:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_5600.pt (epoch 7 @ 5600 updates, score 5.252) (writing took 6.94425416784361 seconds)\n",
      "epoch 007:  19%|▏| 171/922 [00:29<01:40,  7.48it/s, loss=4.836, nll_loss=3.253, 2023-07-10 19:17:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:17:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.32it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:17:42 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.24 | nll_loss 3.651 | ppl 12.56 | wps 266020 | wpb 7018.5 | bsz 290 | num_updates 5700 | best_loss 5.24\n",
      "2023-07-10 19:17:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5700 updates\n",
      "2023-07-10 19:17:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5700.pt\n",
      "2023-07-10 19:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5700.pt\n",
      "2023-07-10 19:17:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_5700.pt (epoch 7 @ 5700 updates, score 5.24) (writing took 9.28576824394986 seconds)\n",
      "epoch 007:  29%|▎| 271/922 [00:52<01:24,  7.69it/s, loss=4.744, nll_loss=3.148, 2023-07-10 19:18:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:18:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:18:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.248 | nll_loss 3.661 | ppl 12.65 | wps 269679 | wpb 7018.5 | bsz 290 | num_updates 5800 | best_loss 5.24\n",
      "2023-07-10 19:18:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5800 updates\n",
      "2023-07-10 19:18:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5800.pt\n",
      "2023-07-10 19:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5800.pt\n",
      "2023-07-10 19:18:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_5800.pt (epoch 7 @ 5800 updates, score 5.248) (writing took 6.940024425042793 seconds)\n",
      "epoch 007:  40%|▍| 371/922 [01:13<01:11,  7.67it/s, loss=4.781, nll_loss=3.19, p2023-07-10 19:18:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:18:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.51it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:18:26 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.199 | nll_loss 3.615 | ppl 12.25 | wps 268180 | wpb 7018.5 | bsz 290 | num_updates 5900 | best_loss 5.199\n",
      "2023-07-10 19:18:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 5900 updates\n",
      "2023-07-10 19:18:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5900.pt\n",
      "2023-07-10 19:18:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_5900.pt\n",
      "2023-07-10 19:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_5900.pt (epoch 7 @ 5900 updates, score 5.199) (writing took 9.588654892053455 seconds)\n",
      "epoch 007:  51%|▌| 471/922 [01:36<01:01,  7.27it/s, loss=4.737, nll_loss=3.14, p2023-07-10 19:18:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:18:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:18:49 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.196 | nll_loss 3.602 | ppl 12.14 | wps 254194 | wpb 7018.5 | bsz 290 | num_updates 6000 | best_loss 5.196\n",
      "2023-07-10 19:18:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6000 updates\n",
      "2023-07-10 19:18:49 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6000.pt\n",
      "2023-07-10 19:18:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6000.pt\n",
      "2023-07-10 19:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_6000.pt (epoch 7 @ 6000 updates, score 5.196) (writing took 9.66576707502827 seconds)\n",
      "epoch 007:  62%|▌| 571/922 [02:00<00:46,  7.55it/s, loss=4.786, nll_loss=3.197, 2023-07-10 19:19:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:19:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.08it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:19:12 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.189 | nll_loss 3.6 | ppl 12.13 | wps 267592 | wpb 7018.5 | bsz 290 | num_updates 6100 | best_loss 5.189\n",
      "2023-07-10 19:19:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6100 updates\n",
      "2023-07-10 19:19:12 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6100.pt\n",
      "2023-07-10 19:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6100.pt\n",
      "2023-07-10 19:19:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_6100.pt (epoch 7 @ 6100 updates, score 5.189) (writing took 12.074584425194189 seconds)\n",
      "epoch 007:  73%|▋| 671/922 [02:25<00:33,  7.54it/s, loss=4.778, nll_loss=3.189, 2023-07-10 19:19:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:19:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:19:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.183 | nll_loss 3.596 | ppl 12.09 | wps 269212 | wpb 7018.5 | bsz 290 | num_updates 6200 | best_loss 5.183\n",
      "2023-07-10 19:19:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6200 updates\n",
      "2023-07-10 19:19:38 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6200.pt\n",
      "2023-07-10 19:19:41 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6200.pt\n",
      "2023-07-10 19:19:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_6200.pt (epoch 7 @ 6200 updates, score 5.183) (writing took 9.599696306977421 seconds)\n",
      "epoch 007:  84%|▊| 771/922 [02:48<00:20,  7.44it/s, loss=4.839, nll_loss=3.259, 2023-07-10 19:20:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:20:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:20:01 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.185 | nll_loss 3.596 | ppl 12.09 | wps 267260 | wpb 7018.5 | bsz 290 | num_updates 6300 | best_loss 5.183\n",
      "2023-07-10 19:20:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6300 updates\n",
      "2023-07-10 19:20:01 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6300.pt\n",
      "2023-07-10 19:20:04 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6300.pt\n",
      "2023-07-10 19:20:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_6300.pt (epoch 7 @ 6300 updates, score 5.185) (writing took 7.034789331024513 seconds)\n",
      "epoch 007:  94%|▉| 871/922 [03:09<00:06,  7.55it/s, loss=4.783, nll_loss=3.196, 2023-07-10 19:20:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:20:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.11it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:20:22 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.145 | nll_loss 3.545 | ppl 11.67 | wps 264982 | wpb 7018.5 | bsz 290 | num_updates 6400 | best_loss 5.145\n",
      "2023-07-10 19:20:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6400 updates\n",
      "2023-07-10 19:20:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6400.pt\n",
      "2023-07-10 19:20:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_7_6400.pt\n",
      "2023-07-10 19:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_7_6400.pt (epoch 7 @ 6400 updates, score 5.145) (writing took 9.848303301958367 seconds)\n",
      "epoch 007: 100%|▉| 921/922 [03:26<00:00,  7.67it/s, loss=4.784, nll_loss=3.198, 2023-07-10 19:20:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:20:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:20:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.137 | nll_loss 3.546 | ppl 11.68 | wps 255922 | wpb 7018.5 | bsz 290 | num_updates 6450 | best_loss 5.137\n",
      "2023-07-10 19:20:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 6450 updates\n",
      "2023-07-10 19:20:38 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:20:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 19:20:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_best.pt (epoch 7 @ 6450 updates, score 5.137) (writing took 8.10874022799544 seconds)\n",
      "2023-07-10 19:20:46 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-07-10 19:20:46 | INFO | train | epoch 007 | loss 4.775 | nll_loss 3.185 | ppl 9.1 | wps 61526.3 | ups 4.3 | wpb 14321.4 | bsz 477.5 | num_updates 6450 | lr 0.000556846 | gnorm 0.628 | clip 2.1 | loss_scale 8 | train_wall 120 | gb_free 17.9 | wall 1526\n",
      "2023-07-10 19:20:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:20:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 008:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:20:47 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-07-10 19:20:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:   5%|█▋                             | 49/922 [00:06<01:53,  7.66it/s]2023-07-10 19:20:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:20:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.03it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:20:54 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.134 | nll_loss 3.544 | ppl 11.66 | wps 264145 | wpb 7018.5 | bsz 290 | num_updates 6500 | best_loss 5.134\n",
      "2023-07-10 19:20:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6500 updates\n",
      "2023-07-10 19:20:54 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6500.pt\n",
      "2023-07-10 19:20:57 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6500.pt\n",
      "2023-07-10 19:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_6500.pt (epoch 8 @ 6500 updates, score 5.134) (writing took 9.435249325120822 seconds)\n",
      "epoch 008:  16%|▏| 149/922 [00:29<01:42,  7.55it/s, loss=4.632, nll_loss=3.023, 2023-07-10 19:21:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:21:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.63it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:21:17 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.122 | nll_loss 3.523 | ppl 11.49 | wps 264573 | wpb 7018.5 | bsz 290 | num_updates 6600 | best_loss 5.122\n",
      "2023-07-10 19:21:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6600 updates\n",
      "2023-07-10 19:21:17 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6600.pt\n",
      "2023-07-10 19:21:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6600.pt\n",
      "2023-07-10 19:21:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_6600.pt (epoch 8 @ 6600 updates, score 5.122) (writing took 9.861951495986432 seconds)\n",
      "epoch 008:  27%|▎| 249/922 [00:52<01:31,  7.38it/s, loss=4.597, nll_loss=2.982, 2023-07-10 19:21:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:21:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:21:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.104 | nll_loss 3.502 | ppl 11.33 | wps 258654 | wpb 7018.5 | bsz 290 | num_updates 6700 | best_loss 5.104\n",
      "2023-07-10 19:21:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6700 updates\n",
      "2023-07-10 19:21:40 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6700.pt\n",
      "2023-07-10 19:21:43 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6700.pt\n",
      "2023-07-10 19:21:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_6700.pt (epoch 8 @ 6700 updates, score 5.104) (writing took 9.22449073009193 seconds)\n",
      "epoch 008:  38%|▍| 349/922 [01:15<01:13,  7.80it/s, loss=4.61, nll_loss=2.997, p2023-07-10 19:22:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:22:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 22.98it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:22:03 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.106 | nll_loss 3.512 | ppl 11.41 | wps 262364 | wpb 7018.5 | bsz 290 | num_updates 6800 | best_loss 5.104\n",
      "2023-07-10 19:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6800 updates\n",
      "2023-07-10 19:22:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6800.pt\n",
      "2023-07-10 19:22:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6800.pt\n",
      "2023-07-10 19:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_6800.pt (epoch 8 @ 6800 updates, score 5.106) (writing took 7.63117518206127 seconds)\n",
      "epoch 008:  49%|▍| 449/922 [01:37<01:01,  7.75it/s, loss=4.615, nll_loss=3.004, 2023-07-10 19:22:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:22:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.07it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:22:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.104 | nll_loss 3.5 | ppl 11.32 | wps 269847 | wpb 7018.5 | bsz 290 | num_updates 6900 | best_loss 5.104\n",
      "2023-07-10 19:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 6900 updates\n",
      "2023-07-10 19:22:24 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6900.pt\n",
      "2023-07-10 19:22:27 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_6900.pt\n",
      "2023-07-10 19:22:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_6900.pt (epoch 8 @ 6900 updates, score 5.104) (writing took 9.130513918120414 seconds)\n",
      "epoch 008:  60%|▌| 549/922 [01:59<00:49,  7.49it/s, loss=4.628, nll_loss=3.019, 2023-07-10 19:22:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:22:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.01it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:22:47 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.089 | nll_loss 3.488 | ppl 11.22 | wps 263485 | wpb 7018.5 | bsz 290 | num_updates 7000 | best_loss 5.089\n",
      "2023-07-10 19:22:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7000 updates\n",
      "2023-07-10 19:22:47 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7000.pt\n",
      "2023-07-10 19:22:50 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7000.pt\n",
      "2023-07-10 19:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_7000.pt (epoch 8 @ 7000 updates, score 5.089) (writing took 9.155222109053284 seconds)\n",
      "epoch 008:  70%|▋| 649/922 [02:22<00:36,  7.44it/s, loss=4.623, nll_loss=3.014, 2023-07-10 19:23:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:23:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.34it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:23:10 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.074 | nll_loss 3.473 | ppl 11.1 | wps 265720 | wpb 7018.5 | bsz 290 | num_updates 7100 | best_loss 5.074\n",
      "2023-07-10 19:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7100 updates\n",
      "2023-07-10 19:23:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7100.pt\n",
      "2023-07-10 19:23:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7100.pt\n",
      "2023-07-10 19:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_7100.pt (epoch 8 @ 7100 updates, score 5.074) (writing took 9.304161773063242 seconds)\n",
      "epoch 008:  81%|▊| 749/922 [02:45<00:23,  7.49it/s, loss=4.582, nll_loss=2.968, 2023-07-10 19:23:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:23:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.99it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:23:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.057 | nll_loss 3.452 | ppl 10.94 | wps 244684 | wpb 7018.5 | bsz 290 | num_updates 7200 | best_loss 5.057\n",
      "2023-07-10 19:23:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7200 updates\n",
      "2023-07-10 19:23:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7200.pt\n",
      "2023-07-10 19:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7200.pt\n",
      "2023-07-10 19:23:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_7200.pt (epoch 8 @ 7200 updates, score 5.057) (writing took 10.017965239007026 seconds)\n",
      "epoch 008:  92%|▉| 849/922 [03:09<00:09,  7.56it/s, loss=4.615, nll_loss=3.007, 2023-07-10 19:23:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:23:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.74it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:23:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.075 | nll_loss 3.475 | ppl 11.12 | wps 255616 | wpb 7018.5 | bsz 290 | num_updates 7300 | best_loss 5.057\n",
      "2023-07-10 19:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7300 updates\n",
      "2023-07-10 19:23:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7300.pt\n",
      "2023-07-10 19:24:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_8_7300.pt\n",
      "2023-07-10 19:24:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_8_7300.pt (epoch 8 @ 7300 updates, score 5.075) (writing took 8.067693331977352 seconds)\n",
      "epoch 008: 100%|▉| 921/922 [03:26<00:00,  7.76it/s, loss=4.628, nll_loss=3.022, 2023-07-10 19:24:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:24:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 008 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 008 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.09it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:24:14 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.059 | nll_loss 3.459 | ppl 11 | wps 268871 | wpb 7018.5 | bsz 290 | num_updates 7372 | best_loss 5.057\n",
      "2023-07-10 19:24:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 7372 updates\n",
      "2023-07-10 19:24:14 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:24:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_last.pt (epoch 8 @ 7372 updates, score 5.059) (writing took 4.16367650590837 seconds)\n",
      "2023-07-10 19:24:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
      "2023-07-10 19:24:18 | INFO | train | epoch 008 | loss 4.613 | nll_loss 3.002 | ppl 8.01 | wps 62393.4 | ups 4.36 | wpb 14321.4 | bsz 477.5 | num_updates 7372 | lr 0.000520862 | gnorm 0.622 | clip 1.1 | loss_scale 8 | train_wall 120 | gb_free 18.1 | wall 1738\n",
      "2023-07-10 19:24:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:24:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 009:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:24:18 | INFO | fairseq.trainer | begin training epoch 9\n",
      "2023-07-10 19:24:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 009:   3%|▉                              | 27/922 [00:03<02:02,  7.33it/s]2023-07-10 19:24:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:24:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.62it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:24:22 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.069 | nll_loss 3.459 | ppl 10.99 | wps 267920 | wpb 7018.5 | bsz 290 | num_updates 7400 | best_loss 5.057\n",
      "2023-07-10 19:24:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7400 updates\n",
      "2023-07-10 19:24:22 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7400.pt\n",
      "2023-07-10 19:24:25 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7400.pt\n",
      "2023-07-10 19:24:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_7400.pt (epoch 9 @ 7400 updates, score 5.069) (writing took 6.790127765852958 seconds)\n",
      "epoch 009:  14%|▏| 127/922 [00:23<01:44,  7.62it/s, loss=4.575, nll_loss=2.961, 2023-07-10 19:24:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:24:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.04it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:24:42 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.055 | nll_loss 3.442 | ppl 10.87 | wps 265472 | wpb 7018.5 | bsz 290 | num_updates 7500 | best_loss 5.055\n",
      "2023-07-10 19:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7500 updates\n",
      "2023-07-10 19:24:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7500.pt\n",
      "2023-07-10 19:24:46 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7500.pt\n",
      "2023-07-10 19:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_7500.pt (epoch 9 @ 7500 updates, score 5.055) (writing took 9.459960355889052 seconds)\n",
      "epoch 009:  25%|▏| 227/922 [00:46<01:33,  7.46it/s, loss=4.432, nll_loss=2.794, 2023-07-10 19:25:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:25:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.89it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:25:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.076 | nll_loss 3.469 | ppl 11.07 | wps 263570 | wpb 7018.5 | bsz 290 | num_updates 7600 | best_loss 5.055\n",
      "2023-07-10 19:25:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7600 updates\n",
      "2023-07-10 19:25:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7600.pt\n",
      "2023-07-10 19:25:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7600.pt\n",
      "2023-07-10 19:25:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_7600.pt (epoch 9 @ 7600 updates, score 5.076) (writing took 7.045908074826002 seconds)\n",
      "epoch 009:  35%|▎| 327/922 [01:07<01:18,  7.56it/s, loss=4.467, nll_loss=2.836, 2023-07-10 19:25:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:25:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.54it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:25:26 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.047 | nll_loss 3.43 | ppl 10.78 | wps 258438 | wpb 7018.5 | bsz 290 | num_updates 7700 | best_loss 5.047\n",
      "2023-07-10 19:25:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7700 updates\n",
      "2023-07-10 19:25:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7700.pt\n",
      "2023-07-10 19:25:29 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7700.pt\n",
      "2023-07-10 19:25:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_7700.pt (epoch 9 @ 7700 updates, score 5.047) (writing took 9.494089444866404 seconds)\n",
      "epoch 009:  46%|▍| 427/922 [01:30<01:04,  7.72it/s, loss=4.52, nll_loss=2.895, p2023-07-10 19:25:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:25:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.94it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:25:49 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.059 | nll_loss 3.454 | ppl 10.96 | wps 265145 | wpb 7018.5 | bsz 290 | num_updates 7800 | best_loss 5.047\n",
      "2023-07-10 19:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7800 updates\n",
      "2023-07-10 19:25:49 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7800.pt\n",
      "2023-07-10 19:25:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7800.pt\n",
      "2023-07-10 19:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_7800.pt (epoch 9 @ 7800 updates, score 5.059) (writing took 7.368287987075746 seconds)\n",
      "epoch 009:  57%|▌| 527/922 [01:51<00:54,  7.29it/s, loss=4.492, nll_loss=2.865, 2023-07-10 19:26:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:26:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:26:10 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.025 | nll_loss 3.414 | ppl 10.66 | wps 264309 | wpb 7018.5 | bsz 290 | num_updates 7900 | best_loss 5.025\n",
      "2023-07-10 19:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 7900 updates\n",
      "2023-07-10 19:26:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7900.pt\n",
      "2023-07-10 19:26:14 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_7900.pt\n",
      "2023-07-10 19:26:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_7900.pt (epoch 9 @ 7900 updates, score 5.025) (writing took 10.096625962061808 seconds)\n",
      "epoch 009:  68%|▋| 627/922 [02:15<00:38,  7.64it/s, loss=4.466, nll_loss=2.837, 2023-07-10 19:26:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:26:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.21it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:26:34 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.999 | nll_loss 3.385 | ppl 10.45 | wps 269159 | wpb 7018.5 | bsz 290 | num_updates 8000 | best_loss 4.999\n",
      "2023-07-10 19:26:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 8000 updates\n",
      "2023-07-10 19:26:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_8000.pt\n",
      "2023-07-10 19:26:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_8000.pt\n",
      "2023-07-10 19:26:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_8000.pt (epoch 9 @ 8000 updates, score 4.999) (writing took 9.324818004155532 seconds)\n",
      "epoch 009:  79%|▊| 727/922 [02:37<00:26,  7.33it/s, loss=4.484, nll_loss=2.857, 2023-07-10 19:26:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:26:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:26:57 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.01 | nll_loss 3.404 | ppl 10.59 | wps 268468 | wpb 7018.5 | bsz 290 | num_updates 8100 | best_loss 4.999\n",
      "2023-07-10 19:26:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 8100 updates\n",
      "2023-07-10 19:26:57 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_8100.pt\n",
      "2023-07-10 19:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_8100.pt\n",
      "2023-07-10 19:27:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_8100.pt (epoch 9 @ 8100 updates, score 5.01) (writing took 6.9193142300937325 seconds)\n",
      "epoch 009:  90%|▉| 827/922 [02:58<00:12,  7.48it/s, loss=4.477, nll_loss=2.851, 2023-07-10 19:27:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:27:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.17it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:27:17 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.994 | nll_loss 3.377 | ppl 10.39 | wps 266905 | wpb 7018.5 | bsz 290 | num_updates 8200 | best_loss 4.994\n",
      "2023-07-10 19:27:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 8200 updates\n",
      "2023-07-10 19:27:17 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_8200.pt\n",
      "2023-07-10 19:27:20 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_9_8200.pt\n",
      "2023-07-10 19:27:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_9_8200.pt (epoch 9 @ 8200 updates, score 4.994) (writing took 9.154879455920309 seconds)\n",
      "epoch 009: 100%|▉| 921/922 [03:20<00:00,  7.82it/s, loss=4.501, nll_loss=2.878, 2023-07-10 19:27:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:27:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 009 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 009 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.58it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:27:39 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.998 | nll_loss 3.389 | ppl 10.48 | wps 255730 | wpb 7018.5 | bsz 290 | num_updates 8294 | best_loss 4.994\n",
      "2023-07-10 19:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 8294 updates\n",
      "2023-07-10 19:27:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:27:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:27:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_last.pt (epoch 9 @ 8294 updates, score 4.998) (writing took 3.5261668669991195 seconds)\n",
      "2023-07-10 19:27:42 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
      "2023-07-10 19:27:42 | INFO | train | epoch 009 | loss 4.482 | nll_loss 2.854 | ppl 7.23 | wps 64607.5 | ups 4.51 | wpb 14321.4 | bsz 477.5 | num_updates 8294 | lr 0.000491058 | gnorm 0.617 | clip 1.2 | loss_scale 16 | train_wall 120 | gb_free 17.9 | wall 1942\n",
      "2023-07-10 19:27:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:27:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 922\n",
      "epoch 010:   0%|                                        | 0/922 [00:00<?, ?it/s]2023-07-10 19:27:43 | INFO | fairseq.trainer | begin training epoch 10\n",
      "2023-07-10 19:27:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 010:   1%|▏                               | 5/922 [00:00<02:03,  7.41it/s]2023-07-10 19:27:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:27:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:27:44 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.016 | nll_loss 3.4 | ppl 10.55 | wps 255152 | wpb 7018.5 | bsz 290 | num_updates 8300 | best_loss 4.994\n",
      "2023-07-10 19:27:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8300 updates\n",
      "2023-07-10 19:27:44 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8300.pt\n",
      "2023-07-10 19:27:47 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8300.pt\n",
      "2023-07-10 19:27:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8300.pt (epoch 10 @ 8300 updates, score 5.016) (writing took 7.266000158153474 seconds)\n",
      "epoch 010:  11%| | 105/922 [00:21<01:48,  7.52it/s, loss=4.526, nll_loss=2.906, 2023-07-10 19:28:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:28:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.65it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:28:05 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.026 | nll_loss 3.407 | ppl 10.61 | wps 267165 | wpb 7018.5 | bsz 290 | num_updates 8400 | best_loss 4.994\n",
      "2023-07-10 19:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8400 updates\n",
      "2023-07-10 19:28:05 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8400.pt\n",
      "2023-07-10 19:28:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8400.pt\n",
      "2023-07-10 19:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8400.pt (epoch 10 @ 8400 updates, score 5.026) (writing took 6.772058645030484 seconds)\n",
      "epoch 010:  22%|▏| 205/922 [00:41<01:33,  7.65it/s, loss=4.29, nll_loss=2.634, p2023-07-10 19:28:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:28:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.19it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:28:25 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.006 | nll_loss 3.392 | ppl 10.5 | wps 259383 | wpb 7018.5 | bsz 290 | num_updates 8500 | best_loss 4.994\n",
      "2023-07-10 19:28:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8500 updates\n",
      "2023-07-10 19:28:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8500.pt\n",
      "2023-07-10 19:28:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8500.pt\n",
      "2023-07-10 19:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8500.pt (epoch 10 @ 8500 updates, score 5.006) (writing took 6.930556170176715 seconds)\n",
      "epoch 010:  33%|▎| 305/922 [01:02<01:22,  7.51it/s, loss=4.355, nll_loss=2.708, 2023-07-10 19:28:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:28:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.77it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:28:45 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.005 | nll_loss 3.385 | ppl 10.45 | wps 267157 | wpb 7018.5 | bsz 290 | num_updates 8600 | best_loss 4.994\n",
      "2023-07-10 19:28:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8600 updates\n",
      "2023-07-10 19:28:45 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8600.pt\n",
      "2023-07-10 19:28:49 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8600.pt\n",
      "2023-07-10 19:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8600.pt (epoch 10 @ 8600 updates, score 5.005) (writing took 6.788470115046948 seconds)\n",
      "epoch 010:  44%|▍| 405/922 [01:22<01:07,  7.61it/s, loss=4.317, nll_loss=2.665, 2023-07-10 19:29:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:29:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.67it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:29:06 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.994 | nll_loss 3.384 | ppl 10.44 | wps 259724 | wpb 7018.5 | bsz 290 | num_updates 8700 | best_loss 4.994\n",
      "2023-07-10 19:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8700 updates\n",
      "2023-07-10 19:29:06 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8700.pt\n",
      "2023-07-10 19:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8700.pt\n",
      "2023-07-10 19:29:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8700.pt (epoch 10 @ 8700 updates, score 4.994) (writing took 9.432313706958666 seconds)\n",
      "epoch 010:  55%|▌| 505/922 [01:45<00:55,  7.49it/s, loss=4.387, nll_loss=2.746, 2023-07-10 19:29:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:29:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:29:29 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.978 | nll_loss 3.362 | ppl 10.28 | wps 265545 | wpb 7018.5 | bsz 290 | num_updates 8800 | best_loss 4.978\n",
      "2023-07-10 19:29:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8800 updates\n",
      "2023-07-10 19:29:29 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8800.pt\n",
      "2023-07-10 19:29:32 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8800.pt\n",
      "2023-07-10 19:29:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8800.pt (epoch 10 @ 8800 updates, score 4.978) (writing took 9.625341167207807 seconds)\n",
      "epoch 010:  66%|▋| 605/922 [02:09<00:41,  7.71it/s, loss=4.392, nll_loss=2.752, 2023-07-10 19:29:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:29:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.24it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:29:52 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.97 | nll_loss 3.35 | ppl 10.2 | wps 268638 | wpb 7018.5 | bsz 290 | num_updates 8900 | best_loss 4.97\n",
      "2023-07-10 19:29:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 8900 updates\n",
      "2023-07-10 19:29:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8900.pt\n",
      "2023-07-10 19:29:56 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_8900.pt\n",
      "2023-07-10 19:30:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_8900.pt (epoch 10 @ 8900 updates, score 4.97) (writing took 9.30298204999417 seconds)\n",
      "epoch 010:  75%|▋| 687/922 [02:30<00:31,  7.53it/s, loss=4.427, nll_loss=2.792, 2023-07-10 19:30:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 010:  77%|▊| 706/922 [02:32<00:28,  7.50it/s, loss=4.427, nll_loss=2.792, 2023-07-10 19:30:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:30:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 24.97it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:30:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.984 | nll_loss 3.369 | ppl 10.33 | wps 257290 | wpb 7018.5 | bsz 290 | num_updates 9000 | best_loss 4.97\n",
      "2023-07-10 19:30:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 9000 updates\n",
      "2023-07-10 19:30:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_9000.pt\n",
      "2023-07-10 19:30:19 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_9000.pt\n",
      "2023-07-10 19:30:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_9000.pt (epoch 10 @ 9000 updates, score 4.984) (writing took 6.726520982105285 seconds)\n",
      "epoch 010:  87%|▊| 806/922 [02:53<00:15,  7.50it/s, loss=4.367, nll_loss=2.725, 2023-07-10 19:30:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:30:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.20it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:30:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.948 | nll_loss 3.327 | ppl 10.03 | wps 261846 | wpb 7018.5 | bsz 290 | num_updates 9100 | best_loss 4.948\n",
      "2023-07-10 19:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 9100 updates\n",
      "2023-07-10 19:30:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_9100.pt\n",
      "2023-07-10 19:30:39 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_9100.pt\n",
      "2023-07-10 19:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_9100.pt (epoch 10 @ 9100 updates, score 4.948) (writing took 9.286096862051636 seconds)\n",
      "epoch 010:  98%|▉| 906/922 [03:16<00:02,  7.14it/s, loss=4.406, nll_loss=2.77, p2023-07-10 19:30:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:30:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  75%|██████  | 3/4 [00:00<00:00, 23.82it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:30:59 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.946 | nll_loss 3.333 | ppl 10.08 | wps 262416 | wpb 7018.5 | bsz 290 | num_updates 9200 | best_loss 4.946\n",
      "2023-07-10 19:30:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 9200 updates\n",
      "2023-07-10 19:30:59 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_9200.pt\n",
      "2023-07-10 19:31:02 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_10_9200.pt\n",
      "2023-07-10 19:31:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_10_9200.pt (epoch 10 @ 9200 updates, score 4.946) (writing took 9.312537362100556 seconds)\n",
      "epoch 010: 100%|▉| 921/922 [03:27<00:00,  6.76it/s, loss=4.419, nll_loss=2.785, 2023-07-10 19:31:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-10 19:31:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 010 | valid on 'valid' subset:   0%|                | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 010 | valid on 'valid' subset:  50%|████    | 2/4 [00:00<00:00, 19.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-10 19:31:11 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.952 | nll_loss 3.333 | ppl 10.08 | wps 257946 | wpb 7018.5 | bsz 290 | num_updates 9215 | best_loss 4.946\n",
      "2023-07-10 19:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 9215 updates\n",
      "2023-07-10 19:31:11 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:31:14 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-97/checkpoint_last.pt\n",
      "2023-07-10 19:31:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-97/checkpoint_last.pt (epoch 10 @ 9215 updates, score 4.952) (writing took 3.716445561964065 seconds)\n",
      "2023-07-10 19:31:14 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
      "2023-07-10 19:31:14 | INFO | train | epoch 010 | loss 4.376 | nll_loss 2.734 | ppl 6.66 | wps 62248.6 | ups 4.35 | wpb 14322.3 | bsz 475.1 | num_updates 9215 | lr 0.000465873 | gnorm 0.618 | clip 1.4 | loss_scale 8 | train_wall 120 | gb_free 17.9 | wall 2154\n",
      "2023-07-10 19:31:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 19:31:15 | INFO | fairseq_cli.train | done training in 2149.1 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 fairseq-train data-spm-bin \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.1 --weight-decay 1e-4 --clip-norm 1.0 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 --update-freq 1 \\\n",
    "    --save-dir checkpoints-97 \\\n",
    "    --save-interval-updates 100 --validate-interval-updates 100 \\\n",
    "    --keep-interval-updates 10 --no-epoch-checkpoints \\\n",
    "    --fp16 \\\n",
    "    --tensorboard-logdir tensorboard-97 \\\n",
    "    --max-epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-10 20:04:51 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-97/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'data-spm-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-10 20:04:51 | INFO | fairseq.tasks.translation | [ja] dictionary: 34208 types\n",
      "2023-07-10 20:04:51 | INFO | fairseq.tasks.translation | [en] dictionary: 32008 types\n",
      "2023-07-10 20:04:51 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-97/checkpoint_best.pt\n",
      "2023-07-10 20:04:52 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.ja\n",
      "2023-07-10 20:04:52 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: data-spm-bin/test.ja-en.en\n",
      "2023-07-10 20:04:52 | INFO | fairseq.tasks.translation | data-spm-bin test ja-en 1160 examples\n",
      "2023-07-10 20:04:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-10 20:04:53 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-10 20:04:53 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-10 20:04:53 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-10 20:05:08 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-10 20:05:08 | INFO | fairseq_cli.generate | Translated 1,160 sentences (30,147 tokens) in 9.8s (118.13 sentences/s, 3070.08 tokens/s)\n",
      "S-347\t大比叡と四明岳\n",
      "T-347\tDaihiei and Shimeigatake\n",
      "H-347\t-0.4359646141529083\tMt. Hiei and Mt. Shimyo\n",
      "D-347\t-0.4359646141529083\tMt. Hiei and Mt. Shimyo\n",
      "P-347\t-1.2140 -0.1311 -0.1427 -0.2065 -0.1489 -0.0815 -0.5094 -1.2790 -0.2106\n",
      "S-211\tそのため、伝授は困難を極めた。\n",
      "T-211\tTherefore, its initiation was extremely difficult.\n",
      "H-211\t-1.059277057647705\tTherefore, it was extremely difficult to teach.\n",
      "D-211\t-1.059277057647705\tTherefore, it was extremely difficult to teach.\n",
      "P-211\t-1.1445 -0.3877 -1.2235 -0.8186 -2.5518 -0.0636 -0.4679 -2.6423 -1.1647 -0.1282\n",
      "S-109\t1987年静態保存。\n",
      "T-109\tPut into static preservation in 1987.\n",
      "H-109\t-0.9481772184371948\t1987: Seika-cho was preserved.\n",
      "D-109\t-0.9481772184371948\t1987: Seika-cho was preserved.\n",
      "P-109\t-0.0782 -0.4684 -2.7661 -1.6087 -1.2147 -1.0301 -0.9647 -0.2689 -0.1338\n",
      "S-1157\t親藩、譜代大名、外様大名\n",
      "T-1157\tShinpan, Fudai Daimyo, Tozama Daimyo\n",
      "H-1157\t-0.3913463056087494\tShinpan (Tokugawa's relatives), Fudai daimyo (a daimyo in hereditary vassal to the Tokugawa family), and Tozama daimyo (a daimyo in hereditary vassal to the Tokugawa family)\n",
      "D-1157\t-0.3913463056087494\tShinpan (Tokugawa's relatives), Fudai daimyo (a daimyo in hereditary vassal to the Tokugawa family), and Tozama daimyo (a daimyo in hereditary vassal to the Tokugawa family)\n",
      "P-1157\t-0.0878 -0.6295 -1.6194 -0.8589 -0.1084 -0.0679 -0.1253 -0.3655 -0.6241 -0.1996 -0.8175 -0.1060 -1.5198 -0.0489 -0.0584 -0.1401 -0.1818 -0.0136 -0.4165 -0.3905 -0.6428 -0.1045 -0.3261 -0.1880 -1.1376 -0.1778 -1.1688 -0.1996 -0.0653 -0.1299 -0.1470 -0.0259 -0.6534 -0.1998 -0.1512\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-generate data-spm-bin --path checkpoints-97/checkpoint_best.pt --batch-size 128 --beam 5 > ./97-results/result.txt --remove-bpe=sentencepiece\n",
    "!head -20 ./97-results/result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilp: Bergent\n",
      "Dogen was a Zen priest in the early Kamakura period.\n",
      "He was the founder of the Soto sect.\n",
      "In his later years, he also used the different name, Kigen.\n",
      "In the same sect, he was called Koso.\n",
      "His shi (posthumous name) was Busshoden Togokushi (Master of the Eastern Capital Offices) and Shoyo Daishi (priest).\n",
      "Generally, he is called Dogen Zenji.\n",
      "It is said that he spread the manner and custom of cleaning teeth in Japan.\n",
      "There is a theory that at first he brought back a green grass.\n",
      "There are many theories about Dogen's birth, but there are several theories on the origin of Dogen's birth; one is that he was born in the direct line of Michichika TSUCHIMIKADO (MINAMOTO no Michichika or Michichika KOGA), Naidaijin (Minister of the Interior).\n"
     ]
    }
   ],
   "source": [
    "!grep \"^H-\" ./97-results/result.txt | sort -V | cut -f3 > ./97-results/result.en.txt\n",
    "!head ./97-results/result.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(sys='./97-results/result.en.txt', ref='kftt-data-1.0/data/orig/kyoto-test.en', order=4, ignore_case=False, sacrebleu=False, sentence_bleu=False)\n",
      "BLEU4 = 15.57, 43.6/20.8/11.1/6.3 (BP=0.980, ratio=0.981, syslen=21636, reflen=22063)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys ./97-results/result.en.txt --ref kftt-data-1.0/data/orig/kyoto-test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "98.ドメイン適応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-10 20:08:32--  http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/pretrained_models/ja-en/base.tar.gz\n",
      "www.kecl.ntt.co.jp (www.kecl.ntt.co.jp) をDNSに問いあわせています... 163.137.218.58\n",
      "www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)|163.137.218.58|:80 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 943815658 (900M) [application/x-gzip]\n",
      "`base.tar.gz' に保存中\n",
      "\n",
      "base.tar.gz         100%[===================>] 900.09M  5.07MB/s    時間 3m 24s  \n",
      "\n",
      "2023-07-10 20:11:56 (4.41 MB/s) - `base.tar.gz' へ保存完了 [943815658/943815658]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/pretrained_models/ja-en/base.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base/\n",
      "base/LICENSE\n",
      "base/dict.en.txt\n",
      "base/base.pretrain.pt\n",
      "base/dict.ja.txt\n"
     ]
    }
   ],
   "source": [
    "!tar zxvf base.tar.gz\n",
    "!rm -r base.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-12 12:52:01--  https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/spm_models/en-ja_spm.tar.gz\n",
      "www.kecl.ntt.co.jp (www.kecl.ntt.co.jp) をDNSに問いあわせています... 163.137.218.58\n",
      "www.kecl.ntt.co.jp (www.kecl.ntt.co.jp)|163.137.218.58|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 1138921 (1.1M) [application/x-gzip]\n",
      "`en-ja_spm.tar.gz' に保存中\n",
      "\n",
      "en-ja_spm.tar.gz    100%[===================>]   1.09M  4.01MB/s    時間 0.3s    \n",
      "\n",
      "2023-07-12 12:52:01 (4.01 MB/s) - `en-ja_spm.tar.gz' へ保存完了 [1138921/1138921]\n",
      "\n",
      "enja_spm_models/\n",
      "enja_spm_models/spm.en.nopretok.vocab\n",
      "enja_spm_models/spm.en.nopretok.model\n",
      "enja_spm_models/spm.ja.nopretok.model\n",
      "enja_spm_models/spm.ja.nopretok.vocab\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/spm_models/en-ja_spm.tar.gz\n",
    "!tar zxvf en-ja_spm.tar.gz\n",
    "!rm -r en-ja_spm.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./pretrain-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp_pre_ja = spm.SentencePieceProcessor(model_file='./enja_spm_models/spm.ja.nopretok.model')\n",
    "sp_pre_en = spm.SentencePieceProcessor(model_file='./enja_spm_models/spm.en.nopretok.model')\n",
    "\n",
    "def make_corpus(fname_unigram, fname_raw, model):\n",
    "    with open(fname_unigram, 'w') as f_uni, open(fname_raw, 'r') as f_raw:\n",
    "        for line in f_raw:\n",
    "            f_uni.write(' '.join(model.encode(line, out_type=str)))\n",
    "            f_uni.write('\\n')\n",
    "\n",
    "make_corpus('./pretrain-corpus/train.ja', './kftt-data-1.0/data/orig/kyoto-train.ja', sp_pre_ja)\n",
    "make_corpus('./pretrain-corpus/dev.ja', './kftt-data-1.0/data/orig/kyoto-dev.ja', sp_pre_ja)\n",
    "make_corpus('./pretrain-corpus/test.ja', './kftt-data-1.0/data/orig/kyoto-test.ja', sp_pre_ja)\n",
    "make_corpus('./pretrain-corpus/train.en', './kftt-data-1.0/data/orig/kyoto-train.en', sp_pre_en)\n",
    "make_corpus('./pretrain-corpus/dev.en', './kftt-data-1.0/data/orig/kyoto-dev.en', sp_pre_en)\n",
    "make_corpus('./pretrain-corpus/test.en', './kftt-data-1.0/data/orig/kyoto-test.en', sp_pre_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-12 13:07:09 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='ja', target_lang='en', trainpref='./pretrain-corpus/train', validpref='./pretrain-corpus/dev', testpref='./pretrain-corpus/test', align_suffix=None, destdir='pretrain-bin', thresholdtgt=0, thresholdsrc=0, tgtdict='./base/dict.en.txt', srcdict='./base/dict.ja.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
      "2023-07-12 13:07:09 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 31984 types\n",
      "2023-07-12 13:08:37 | INFO | fairseq_cli.preprocess | [ja] ./pretrain-corpus/train.ja: 440288 sents, 12070521 tokens, 0.0535% replaced (by <unk>)\n",
      "2023-07-12 13:08:37 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 31984 types\n",
      "2023-07-12 13:08:38 | INFO | fairseq_cli.preprocess | [ja] ./pretrain-corpus/dev.ja: 1166 sents, 28269 tokens, 0.184% replaced (by <unk>)\n",
      "2023-07-12 13:08:38 | INFO | fairseq_cli.preprocess | [ja] Dictionary: 31984 types\n",
      "2023-07-12 13:08:38 | INFO | fairseq_cli.preprocess | [ja] ./pretrain-corpus/test.ja: 1160 sents, 30239 tokens, 0.324% replaced (by <unk>)\n",
      "2023-07-12 13:08:38 | INFO | fairseq_cli.preprocess | [en] Dictionary: 32024 types\n",
      "2023-07-12 13:10:10 | INFO | fairseq_cli.preprocess | [en] ./pretrain-corpus/train.en: 440288 sents, 15185034 tokens, 0.005% replaced (by <unk>)\n",
      "2023-07-12 13:10:10 | INFO | fairseq_cli.preprocess | [en] Dictionary: 32024 types\n",
      "2023-07-12 13:10:11 | INFO | fairseq_cli.preprocess | [en] ./pretrain-corpus/dev.en: 1166 sents, 32830 tokens, 0.11% replaced (by <unk>)\n",
      "2023-07-12 13:10:11 | INFO | fairseq_cli.preprocess | [en] Dictionary: 32024 types\n",
      "2023-07-12 13:10:11 | INFO | fairseq_cli.preprocess | [en] ./pretrain-corpus/test.en: 1160 sents, 35662 tokens, 0.0028% replaced (by <unk>)\n",
      "2023-07-12 13:10:11 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to pretrain-bin\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-preprocess \\\n",
    "    --source-lang ja \\\n",
    "    --target-lang en \\\n",
    "    --trainpref ./pretrain-corpus/train \\\n",
    "    --validpref ./pretrain-corpus/dev \\\n",
    "    --testpref ./pretrain-corpus/test \\\n",
    "    --srcdict ./base/dict.ja.txt \\\n",
    "    --tgtdict ./base/dict.en.txt \\\n",
    "    --destdir pretrain-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-13 15:41:10 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:19495\n",
      "2023-07-13 15:41:10 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:19495\n",
      "2023-07-13 15:41:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "2023-07-13 15:41:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "2023-07-13 15:41:10 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "2023-07-13 15:41:10 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
      "2023-07-13 15:41:10 | INFO | fairseq.distributed.utils | initialized host opus1 as rank 0\n",
      "2023-07-13 15:41:10 | INFO | fairseq.distributed.utils | initialized host opus1 as rank 1\n",
      "2023-07-13 15:41:12 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': 'tensorboard-98', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:19495', 'distributed_port': 19495, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 8000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 8000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 3, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.001], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints-98', 'restore_file': './base/base.pretrain.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': 10, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir='tensorboard-98', wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=8000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=100, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=8000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=2, distributed_num_procs=2, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=2, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=3, max_update=0, stop_time_hours=0, clip_norm=1.0, sentence_avg=False, update_freq=[1], lr=[0.001], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='checkpoints-98', restore_file='./base/base.pretrain.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=100, keep_interval_updates=10, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='pretrain-bin', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=2000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, merge_src_tgt_embed=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'pretrain-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2000, 'warmup_init_lr': -1.0, 'lr': [0.001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-13 15:41:12 | INFO | fairseq.tasks.translation | [ja] dictionary: 31984 types\n",
      "2023-07-13 15:41:12 | INFO | fairseq.tasks.translation | [en] dictionary: 32024 types\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | TransformerModel(\n",
      "  (encoder): TransformerEncoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(31984, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayerBase(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoderBase(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(32024, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=32024, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | task: TranslationTask\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | model: TransformerModel\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | num. shared model params: 76,910,592 (num. trained: 76,910,592)\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-07-13 15:41:13 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: pretrain-bin/valid.ja-en.ja\n",
      "2023-07-13 15:41:13 | INFO | fairseq.data.data_utils | loaded 1,166 examples from: pretrain-bin/valid.ja-en.en\n",
      "2023-07-13 15:41:13 | INFO | fairseq.tasks.translation | pretrain-bin valid ja-en 1166 examples\n",
      "2023-07-13 15:41:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0\n",
      "2023-07-13 15:41:13 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
      "2023-07-13 15:41:13 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-07-13 15:41:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\n",
      "2023-07-13 15:41:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.651 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-13 15:41:13 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        \n",
      "2023-07-13 15:41:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 2 workers***********************\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | training on 2 devices (GPUs/TPUs)\n",
      "2023-07-13 15:41:13 | INFO | fairseq_cli.train | max tokens per device = 8000 and max sentences per device = None\n",
      "2023-07-13 15:41:13 | INFO | fairseq.trainer | Preparing to load checkpoint ./base/base.pretrain.pt\n",
      "2023-07-13 15:41:16 | INFO | fairseq.trainer | Loaded checkpoint ./base/base.pretrain.pt (epoch 16 @ 0 updates)\n",
      "2023-07-13 15:41:16 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-07-13 15:41:16 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: pretrain-bin/train.ja-en.ja\n",
      "2023-07-13 15:41:16 | INFO | fairseq.data.data_utils | loaded 440,288 examples from: pretrain-bin/train.ja-en.en\n",
      "2023-07-13 15:41:16 | INFO | fairseq.tasks.translation | pretrain-bin train ja-en 440288 examples\n",
      "2023-07-13 15:41:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-13 15:41:16 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-13 15:41:16 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-13 15:41:16 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-13 15:41:17 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-07-13 15:41:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-13 15:41:17 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-13 15:41:17 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-13 15:41:17 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-13 15:41:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1072\n",
      "epoch 001:   0%|                                       | 0/1072 [00:00<?, ?it/s]2023-07-13 15:41:21 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-07-13 15:41:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2023-07-13 15:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                             | 1/1072 [00:04<1:21:28,  4.56s/it]2023-07-13 15:41:26 | INFO | torch.nn.parallel.distributed | Reducer buckets have been rebuilt in this iteration.\n",
      "2023-07-13 15:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   0%|                               | 2/1072 [00:04<35:23,  1.98s/it]2023-07-13 15:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   0%|                               | 3/1072 [00:04<20:38,  1.16s/it]2023-07-13 15:41:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:   0%|                               | 4/1072 [00:05<13:44,  1.30it/s]2023-07-13 15:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
      "epoch 001:   0%|▏                              | 5/1072 [00:05<09:57,  1.78it/s]2023-07-13 15:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
      "epoch 001:   1%|▏                              | 6/1072 [00:05<07:37,  2.33it/s]2023-07-13 15:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
      "epoch 001:   2%|▋                             | 24/1072 [00:07<02:20,  7.48it/s]2023-07-13 15:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
      "epoch 001:  10%|██▉                          | 107/1072 [00:18<02:09,  7.46it/s]2023-07-13 15:41:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:41:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 18.76it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:41:41 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.425 | nll_loss 6.211 | ppl 74.06 | wps 227520 | wpb 6386 | bsz 232 | num_updates 100\n",
      "2023-07-13 15:41:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 100 updates\n",
      "2023-07-13 15:41:41 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_100.pt\n",
      "2023-07-13 15:41:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_100.pt\n",
      "2023-07-13 15:41:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_100.pt (epoch 1 @ 100 updates, score 7.425) (writing took 9.035214406903833 seconds)\n",
      "epoch 001:  19%|▏| 207/1072 [00:41<01:52,  7.70it/s, loss=9.72, nll_loss=8.824, 2023-07-13 15:42:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:42:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 17.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:42:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.754 | nll_loss 4.309 | ppl 19.82 | wps 227852 | wpb 6386 | bsz 232 | num_updates 200 | best_loss 5.754\n",
      "2023-07-13 15:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 200 updates\n",
      "2023-07-13 15:42:03 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_200.pt\n",
      "2023-07-13 15:42:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_200.pt\n",
      "2023-07-13 15:42:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 5.754) (writing took 9.642358040204272 seconds)\n",
      "epoch 001:  29%|▎| 307/1072 [01:04<01:43,  7.43it/s, loss=6.821, nll_loss=5.54, 2023-07-13 15:42:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:42:26 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.62it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:42:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.996 | nll_loss 3.463 | ppl 11.02 | wps 218476 | wpb 6386 | bsz 232 | num_updates 300 | best_loss 4.996\n",
      "2023-07-13 15:42:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 300 updates\n",
      "2023-07-13 15:42:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_300.pt\n",
      "2023-07-13 15:42:30 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_300.pt\n",
      "2023-07-13 15:42:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_300.pt (epoch 1 @ 300 updates, score 4.996) (writing took 9.275760930031538 seconds)\n",
      "epoch 001:  38%|▍| 407/1072 [01:27<01:26,  7.69it/s, loss=5.584, nll_loss=4.142,2023-07-13 15:42:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:42:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:42:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.623 | nll_loss 3.039 | ppl 8.22 | wps 206948 | wpb 6386 | bsz 232 | num_updates 400 | best_loss 4.623\n",
      "2023-07-13 15:42:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 400 updates\n",
      "2023-07-13 15:42:49 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_400.pt\n",
      "2023-07-13 15:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_400.pt\n",
      "2023-07-13 15:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_400.pt (epoch 1 @ 400 updates, score 4.623) (writing took 9.567869941005483 seconds)\n",
      "epoch 001:  47%|▍| 507/1072 [01:50<01:14,  7.54it/s, loss=5.074, nll_loss=3.564,2023-07-13 15:43:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:43:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 25.18it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:43:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.465 | nll_loss 2.86 | ppl 7.26 | wps 222163 | wpb 6386 | bsz 232 | num_updates 500 | best_loss 4.465\n",
      "2023-07-13 15:43:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 500 updates\n",
      "2023-07-13 15:43:13 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_500.pt\n",
      "2023-07-13 15:43:16 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_500.pt\n",
      "2023-07-13 15:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_500.pt (epoch 1 @ 500 updates, score 4.465) (writing took 9.407548408955336 seconds)\n",
      "epoch 001:  57%|▌| 607/1072 [02:13<01:00,  7.65it/s, loss=4.761, nll_loss=3.212,2023-07-13 15:43:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:43:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 18.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:43:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.374 | nll_loss 2.765 | ppl 6.8 | wps 216743 | wpb 6386 | bsz 232 | num_updates 600 | best_loss 4.374\n",
      "2023-07-13 15:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 600 updates\n",
      "2023-07-13 15:43:36 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_600.pt\n",
      "2023-07-13 15:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_600.pt\n",
      "2023-07-13 15:43:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_600.pt (epoch 1 @ 600 updates, score 4.374) (writing took 9.253689037868753 seconds)\n",
      "epoch 001:  66%|▋| 707/1072 [02:36<00:48,  7.53it/s, loss=4.583, nll_loss=3.013,2023-07-13 15:43:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:43:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:43:58 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.306 | nll_loss 2.691 | ppl 6.46 | wps 218775 | wpb 6386 | bsz 232 | num_updates 700 | best_loss 4.306\n",
      "2023-07-13 15:43:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 700 updates\n",
      "2023-07-13 15:43:58 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_700.pt\n",
      "2023-07-13 15:44:01 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_700.pt\n",
      "2023-07-13 15:44:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_700.pt (epoch 1 @ 700 updates, score 4.306) (writing took 9.022946331184357 seconds)\n",
      "epoch 001:  75%|▊| 807/1072 [02:59<00:34,  7.59it/s, loss=4.514, nll_loss=2.937,2023-07-13 15:44:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:44:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.12it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:44:21 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.307 | nll_loss 2.684 | ppl 6.42 | wps 221634 | wpb 6386 | bsz 232 | num_updates 800 | best_loss 4.306\n",
      "2023-07-13 15:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 800 updates\n",
      "2023-07-13 15:44:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_800.pt\n",
      "2023-07-13 15:44:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_800.pt\n",
      "2023-07-13 15:44:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_800.pt (epoch 1 @ 800 updates, score 4.307) (writing took 6.807921194005758 seconds)\n",
      "epoch 001:  85%|▊| 907/1072 [03:19<00:22,  7.38it/s, loss=4.388, nll_loss=2.796,2023-07-13 15:44:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:44:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.66it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:44:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.276 | nll_loss 2.653 | ppl 6.29 | wps 220898 | wpb 6386 | bsz 232 | num_updates 900 | best_loss 4.276\n",
      "2023-07-13 15:44:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 900 updates\n",
      "2023-07-13 15:44:42 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_900.pt\n",
      "2023-07-13 15:44:45 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_900.pt\n",
      "2023-07-13 15:44:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_900.pt (epoch 1 @ 900 updates, score 4.276) (writing took 9.343427451094612 seconds)\n",
      "epoch 001:  94%|▉| 1007/1072 [03:42<00:08,  7.36it/s, loss=4.352, nll_loss=2.7572023-07-13 15:45:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:45:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:45:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.273 | nll_loss 2.646 | ppl 6.26 | wps 214234 | wpb 6386 | bsz 232 | num_updates 1000 | best_loss 4.273\n",
      "2023-07-13 15:45:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1000 updates\n",
      "2023-07-13 15:45:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_1000.pt\n",
      "2023-07-13 15:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_1_1000.pt\n",
      "2023-07-13 15:45:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 4.273) (writing took 12.396106456872076 seconds)\n",
      "epoch 001: 100%|▉| 1071/1072 [04:03<00:00,  7.76it/s, loss=4.345, nll_loss=2.75,2023-07-13 15:45:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:45:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:45:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.259 | nll_loss 2.64 | ppl 6.23 | wps 202312 | wpb 6386 | bsz 232 | num_updates 1064 | best_loss 4.259\n",
      "2023-07-13 15:45:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1064 updates\n",
      "2023-07-13 15:45:26 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_best.pt\n",
      "2023-07-13 15:45:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_best.pt\n",
      "2023-07-13 15:45:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_best.pt (epoch 1 @ 1064 updates, score 4.259) (writing took 8.582635533995926 seconds)\n",
      "2023-07-13 15:45:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-07-13 15:45:34 | INFO | train | epoch 001 | loss 5.347 | nll_loss 3.878 | ppl 14.71 | wps 60971.5 | ups 4.3 | wpb 14173.9 | bsz 410.7 | num_updates 1064 | lr 0.000532 | gnorm 1.953 | clip 59.8 | loss_scale 0.5 | train_wall 140 | gb_free 18 | wall 261\n",
      "2023-07-13 15:45:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-13 15:45:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1072\n",
      "epoch 002:   0%|                                       | 0/1072 [00:00<?, ?it/s]2023-07-13 15:45:34 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-07-13 15:45:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:   3%|▉                             | 35/1072 [00:04<02:16,  7.62it/s]2023-07-13 15:45:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:45:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:45:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.258 | nll_loss 2.628 | ppl 6.18 | wps 219646 | wpb 6386 | bsz 232 | num_updates 1100 | best_loss 4.258\n",
      "2023-07-13 15:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1100 updates\n",
      "2023-07-13 15:45:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1100.pt\n",
      "2023-07-13 15:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1100.pt\n",
      "2023-07-13 15:45:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1100.pt (epoch 2 @ 1100 updates, score 4.258) (writing took 10.925688894931227 seconds)\n",
      "epoch 002:  13%|▏| 135/1072 [00:29<02:06,  7.38it/s, loss=4.271, nll_loss=2.667,2023-07-13 15:46:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:46:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.93it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:46:04 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.249 | nll_loss 2.627 | ppl 6.18 | wps 222643 | wpb 6386 | bsz 232 | num_updates 1200 | best_loss 4.249\n",
      "2023-07-13 15:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1200 updates\n",
      "2023-07-13 15:46:04 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1200.pt\n",
      "2023-07-13 15:46:07 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1200.pt\n",
      "2023-07-13 15:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1200.pt (epoch 2 @ 1200 updates, score 4.249) (writing took 9.431042747106403 seconds)\n",
      "epoch 002:  22%|▏| 235/1072 [00:52<01:49,  7.64it/s, loss=4.204, nll_loss=2.59, 2023-07-13 15:46:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:46:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 22.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:46:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.262 | nll_loss 2.631 | ppl 6.19 | wps 211290 | wpb 6386 | bsz 232 | num_updates 1300 | best_loss 4.249\n",
      "2023-07-13 15:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1300 updates\n",
      "2023-07-13 15:46:27 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1300.pt\n",
      "2023-07-13 15:46:31 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1300.pt\n",
      "2023-07-13 15:46:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1300.pt (epoch 2 @ 1300 updates, score 4.262) (writing took 10.744162183022127 seconds)\n",
      "epoch 002:  31%|▎| 335/1072 [01:16<01:34,  7.81it/s, loss=4.224, nll_loss=2.614,2023-07-13 15:46:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:46:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.87it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:46:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.292 | nll_loss 2.668 | ppl 6.36 | wps 223805 | wpb 6386 | bsz 232 | num_updates 1400 | best_loss 4.249\n",
      "2023-07-13 15:46:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1400 updates\n",
      "2023-07-13 15:46:52 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1400.pt\n",
      "2023-07-13 15:46:55 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1400.pt\n",
      "2023-07-13 15:46:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1400.pt (epoch 2 @ 1400 updates, score 4.292) (writing took 6.771097833989188 seconds)\n",
      "epoch 002:  41%|▍| 435/1072 [01:37<01:24,  7.55it/s, loss=4.237, nll_loss=2.629,2023-07-13 15:47:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:47:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:47:12 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.287 | nll_loss 2.663 | ppl 6.33 | wps 218317 | wpb 6386 | bsz 232 | num_updates 1500 | best_loss 4.249\n",
      "2023-07-13 15:47:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1500 updates\n",
      "2023-07-13 15:47:12 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1500.pt\n",
      "2023-07-13 15:47:15 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1500.pt\n",
      "2023-07-13 15:47:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1500.pt (epoch 2 @ 1500 updates, score 4.287) (writing took 7.2676571579650044 seconds)\n",
      "epoch 002:  50%|▍| 535/1072 [01:58<01:11,  7.56it/s, loss=4.257, nll_loss=2.653,2023-07-13 15:47:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:47:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.13it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:47:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.307 | nll_loss 2.683 | ppl 6.42 | wps 217165 | wpb 6386 | bsz 232 | num_updates 1600 | best_loss 4.249\n",
      "2023-07-13 15:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1600 updates\n",
      "2023-07-13 15:47:33 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1600.pt\n",
      "2023-07-13 15:47:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1600.pt\n",
      "2023-07-13 15:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1600.pt (epoch 2 @ 1600 updates, score 4.307) (writing took 8.555057897930965 seconds)\n",
      "epoch 002:  59%|▌| 635/1072 [02:20<00:58,  7.47it/s, loss=4.254, nll_loss=2.65, 2023-07-13 15:47:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:47:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.73it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:47:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.318 | nll_loss 2.698 | ppl 6.49 | wps 214852 | wpb 6386 | bsz 232 | num_updates 1700 | best_loss 4.249\n",
      "2023-07-13 15:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1700 updates\n",
      "2023-07-13 15:47:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1700.pt\n",
      "2023-07-13 15:47:59 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1700.pt\n",
      "2023-07-13 15:48:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1700.pt (epoch 2 @ 1700 updates, score 4.318) (writing took 6.757808320922777 seconds)\n",
      "epoch 002:  69%|▋| 735/1072 [02:41<00:44,  7.64it/s, loss=4.255, nll_loss=2.652,2023-07-13 15:48:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:48:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 18.40it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:48:16 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.32 | nll_loss 2.71 | ppl 6.54 | wps 225071 | wpb 6386 | bsz 232 | num_updates 1800 | best_loss 4.249\n",
      "2023-07-13 15:48:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1800 updates\n",
      "2023-07-13 15:48:16 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1800.pt\n",
      "2023-07-13 15:48:19 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1800.pt\n",
      "2023-07-13 15:48:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1800.pt (epoch 2 @ 1800 updates, score 4.32) (writing took 6.953289957949892 seconds)\n",
      "epoch 002:  78%|▊| 835/1072 [03:02<00:31,  7.51it/s, loss=4.308, nll_loss=2.711,2023-07-13 15:48:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:48:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 24.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:48:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.334 | nll_loss 2.718 | ppl 6.58 | wps 217507 | wpb 6386 | bsz 232 | num_updates 1900 | best_loss 4.249\n",
      "2023-07-13 15:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 1900 updates\n",
      "2023-07-13 15:48:37 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1900.pt\n",
      "2023-07-13 15:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_1900.pt\n",
      "2023-07-13 15:48:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_1900.pt (epoch 2 @ 1900 updates, score 4.334) (writing took 9.600633518071845 seconds)\n",
      "epoch 002:  87%|▊| 935/1072 [03:25<00:18,  7.55it/s, loss=4.313, nll_loss=2.717,2023-07-13 15:49:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:49:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 18.69it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:49:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.37 | nll_loss 2.755 | ppl 6.75 | wps 223273 | wpb 6386 | bsz 232 | num_updates 2000 | best_loss 4.249\n",
      "2023-07-13 15:49:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates\n",
      "2023-07-13 15:49:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_2000.pt\n",
      "2023-07-13 15:49:03 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_2000.pt\n",
      "2023-07-13 15:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 4.37) (writing took 7.2623368359636515 seconds)\n",
      "epoch 002:  97%|▉| 1035/1072 [03:46<00:05,  7.28it/s, loss=4.298, nll_loss=2.7012023-07-13 15:49:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:49:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:49:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.355 | nll_loss 2.746 | ppl 6.71 | wps 222715 | wpb 6386 | bsz 232 | num_updates 2100 | best_loss 4.249\n",
      "2023-07-13 15:49:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2100 updates\n",
      "2023-07-13 15:49:21 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_2100.pt\n",
      "2023-07-13 15:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_2_2100.pt\n",
      "2023-07-13 15:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_2_2100.pt (epoch 2 @ 2100 updates, score 4.355) (writing took 7.095192808890715 seconds)\n",
      "epoch 002: 100%|▉| 1071/1072 [03:58<00:00,  7.61it/s, loss=4.332, nll_loss=2.74,2023-07-13 15:49:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:49:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:49:34 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.341 | nll_loss 2.722 | ppl 6.6 | wps 216911 | wpb 6386 | bsz 232 | num_updates 2136 | best_loss 4.249\n",
      "2023-07-13 15:49:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2136 updates\n",
      "2023-07-13 15:49:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_last.pt\n",
      "2023-07-13 15:49:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_last.pt\n",
      "2023-07-13 15:49:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_last.pt (epoch 2 @ 2136 updates, score 4.341) (writing took 3.5121769960969687 seconds)\n",
      "2023-07-13 15:49:37 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-07-13 15:49:37 | INFO | train | epoch 002 | loss 4.266 | nll_loss 2.664 | ppl 6.34 | wps 62540.8 | ups 4.42 | wpb 14165.1 | bsz 410.7 | num_updates 2136 | lr 0.000967641 | gnorm 0.909 | clip 22.2 | loss_scale 0.5 | train_wall 140 | gb_free 17.7 | wall 504\n",
      "2023-07-13 15:49:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-13 15:49:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1072\n",
      "epoch 003:   0%|                                       | 0/1072 [00:00<?, ?it/s]2023-07-13 15:49:37 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-07-13 15:49:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:   6%|█▊                            | 63/1072 [00:08<02:16,  7.38it/s]2023-07-13 15:49:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:49:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.92it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:49:46 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.344 | nll_loss 2.724 | ppl 6.61 | wps 211583 | wpb 6386 | bsz 232 | num_updates 2200 | best_loss 4.249\n",
      "2023-07-13 15:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2200 updates\n",
      "2023-07-13 15:49:46 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2200.pt\n",
      "2023-07-13 15:49:52 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2200.pt\n",
      "2023-07-13 15:49:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2200.pt (epoch 3 @ 2200 updates, score 4.344) (writing took 10.518235735828057 seconds)\n",
      "epoch 003:  15%|▏| 163/1072 [00:32<02:05,  7.27it/s, loss=4.203, nll_loss=2.59, 2023-07-13 15:50:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:50:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.45it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:50:10 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.318 | nll_loss 2.695 | ppl 6.48 | wps 205060 | wpb 6386 | bsz 232 | num_updates 2300 | best_loss 4.249\n",
      "2023-07-13 15:50:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2300 updates\n",
      "2023-07-13 15:50:10 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2300.pt\n",
      "2023-07-13 15:50:13 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2300.pt\n",
      "2023-07-13 15:50:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2300.pt (epoch 3 @ 2300 updates, score 4.318) (writing took 6.772784716915339 seconds)\n",
      "epoch 003:  25%|▏| 263/1072 [00:53<01:47,  7.53it/s, loss=4.178, nll_loss=2.562,2023-07-13 15:50:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:50:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.88it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:50:31 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.316 | nll_loss 2.696 | ppl 6.48 | wps 214402 | wpb 6386 | bsz 232 | num_updates 2400 | best_loss 4.249\n",
      "2023-07-13 15:50:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2400 updates\n",
      "2023-07-13 15:50:31 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2400.pt\n",
      "2023-07-13 15:50:34 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2400.pt\n",
      "2023-07-13 15:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2400.pt (epoch 3 @ 2400 updates, score 4.316) (writing took 7.108517812099308 seconds)\n",
      "epoch 003:  34%|▎| 363/1072 [01:13<01:34,  7.50it/s, loss=4.161, nll_loss=2.544,2023-07-13 15:50:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:50:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 23.47it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:50:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.321 | nll_loss 2.696 | ppl 6.48 | wps 220562 | wpb 6386 | bsz 232 | num_updates 2500 | best_loss 4.249\n",
      "2023-07-13 15:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2500 updates\n",
      "2023-07-13 15:50:51 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2500.pt\n",
      "2023-07-13 15:50:54 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2500.pt\n",
      "2023-07-13 15:51:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2500.pt (epoch 3 @ 2500 updates, score 4.321) (writing took 9.917155890027061 seconds)\n",
      "epoch 003:  43%|▍| 463/1072 [01:37<01:20,  7.53it/s, loss=4.152, nll_loss=2.535,2023-07-13 15:51:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:51:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.81it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:51:15 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.311 | nll_loss 2.685 | ppl 6.43 | wps 221294 | wpb 6386 | bsz 232 | num_updates 2600 | best_loss 4.249\n",
      "2023-07-13 15:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2600 updates\n",
      "2023-07-13 15:51:15 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2600.pt\n",
      "2023-07-13 15:51:18 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2600.pt\n",
      "2023-07-13 15:51:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2600.pt (epoch 3 @ 2600 updates, score 4.311) (writing took 6.823814601870254 seconds)\n",
      "epoch 003:  53%|▌| 563/1072 [01:57<01:09,  7.35it/s, loss=4.173, nll_loss=2.559,2023-07-13 15:51:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:51:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 24.27it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:51:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.291 | nll_loss 2.668 | ppl 6.36 | wps 225685 | wpb 6386 | bsz 232 | num_updates 2700 | best_loss 4.249\n",
      "2023-07-13 15:51:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2700 updates\n",
      "2023-07-13 15:51:35 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2700.pt\n",
      "2023-07-13 15:51:38 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2700.pt\n",
      "2023-07-13 15:51:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2700.pt (epoch 3 @ 2700 updates, score 4.291) (writing took 6.854715294903144 seconds)\n",
      "epoch 003:  62%|▌| 663/1072 [02:18<00:52,  7.81it/s, loss=4.16, nll_loss=2.545, 2023-07-13 15:51:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:51:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.68it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:51:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.308 | nll_loss 2.684 | ppl 6.43 | wps 222998 | wpb 6386 | bsz 232 | num_updates 2800 | best_loss 4.249\n",
      "2023-07-13 15:51:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2800 updates\n",
      "2023-07-13 15:51:56 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2800.pt\n",
      "2023-07-13 15:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2800.pt\n",
      "2023-07-13 15:52:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2800.pt (epoch 3 @ 2800 updates, score 4.308) (writing took 8.133196267997846 seconds)\n",
      "epoch 003:  71%|▋| 763/1072 [02:40<00:40,  7.55it/s, loss=4.169, nll_loss=2.555,2023-07-13 15:52:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:52:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.29it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:52:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.316 | nll_loss 2.689 | ppl 6.45 | wps 220010 | wpb 6386 | bsz 232 | num_updates 2900 | best_loss 4.249\n",
      "2023-07-13 15:52:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 2900 updates\n",
      "2023-07-13 15:52:18 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2900.pt\n",
      "2023-07-13 15:52:21 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_2900.pt\n",
      "2023-07-13 15:52:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_2900.pt (epoch 3 @ 2900 updates, score 4.316) (writing took 6.914806593907997 seconds)\n",
      "epoch 003:  81%|▊| 863/1072 [03:01<00:27,  7.47it/s, loss=4.14, nll_loss=2.524, 2023-07-13 15:52:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:52:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.86it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:52:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.278 | nll_loss 2.653 | ppl 6.29 | wps 222268 | wpb 6386 | bsz 232 | num_updates 3000 | best_loss 4.249\n",
      "2023-07-13 15:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3000 updates\n",
      "2023-07-13 15:52:39 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_3000.pt\n",
      "2023-07-13 15:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_3000.pt\n",
      "2023-07-13 15:52:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_3000.pt (epoch 3 @ 3000 updates, score 4.278) (writing took 6.982334064086899 seconds)\n",
      "epoch 003:  90%|▉| 963/1072 [03:22<00:14,  7.41it/s, loss=4.145, nll_loss=2.53, 2023-07-13 15:53:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:53:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.22it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:53:00 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.255 | nll_loss 2.632 | ppl 6.2 | wps 219424 | wpb 6386 | bsz 232 | num_updates 3100 | best_loss 4.249\n",
      "2023-07-13 15:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3100 updates\n",
      "2023-07-13 15:53:00 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_3100.pt\n",
      "2023-07-13 15:53:06 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_3100.pt\n",
      "2023-07-13 15:53:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_3100.pt (epoch 3 @ 3100 updates, score 4.255) (writing took 11.151604460086673 seconds)\n",
      "epoch 003:  99%|▉| 1063/1072 [03:47<00:01,  7.68it/s, loss=4.151, nll_loss=2.5362023-07-13 15:53:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:53:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|████▊   | 3/5 [00:00<00:00, 24.33it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:53:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.274 | nll_loss 2.651 | ppl 6.28 | wps 217855 | wpb 6386 | bsz 232 | num_updates 3200 | best_loss 4.249\n",
      "2023-07-13 15:53:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3200 updates\n",
      "2023-07-13 15:53:25 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_3200.pt\n",
      "2023-07-13 15:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_3_3200.pt\n",
      "2023-07-13 15:53:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_3_3200.pt (epoch 3 @ 3200 updates, score 4.274) (writing took 6.808549141976982 seconds)\n",
      "epoch 003: 100%|▉| 1071/1072 [03:56<00:00,  3.27it/s, loss=4.13, nll_loss=2.513,2023-07-13 15:53:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-07-13 15:53:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|                | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  40%|███▏    | 2/5 [00:00<00:00, 19.16it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-07-13 15:53:34 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.268 | nll_loss 2.648 | ppl 6.27 | wps 199521 | wpb 6386 | bsz 232 | num_updates 3208 | best_loss 4.249\n",
      "2023-07-13 15:53:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3208 updates\n",
      "2023-07-13 15:53:34 | INFO | fairseq.trainer | Saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_last.pt\n",
      "2023-07-13 15:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /net/nas5/data/home/sugihara/workspace/NLP100_90~99/checkpoints-98/checkpoint_last.pt\n",
      "2023-07-13 15:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints-98/checkpoint_last.pt (epoch 3 @ 3208 updates, score 4.268) (writing took 3.3958022529259324 seconds)\n",
      "2023-07-13 15:53:37 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-07-13 15:53:37 | INFO | train | epoch 003 | loss 4.155 | nll_loss 2.538 | ppl 5.81 | wps 63308.7 | ups 4.47 | wpb 14165.1 | bsz 410.7 | num_updates 3208 | lr 0.000789583 | gnorm 0.851 | clip 13.7 | loss_scale 0.5 | train_wall 140 | gb_free 18.4 | wall 744\n",
      "2023-07-13 15:53:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-13 15:53:37 | INFO | fairseq_cli.train | done training in 735.6 seconds\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1 fairseq-train pretrain-bin --restore-file ./base/base.pretrain.pt \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
    "    --optimizer adam --adam-betas '(0.9,0.98)' \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.1 --weight-decay 1e-4 --clip-norm 1.0 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 --update-freq 1 \\\n",
    "    --save-dir checkpoints-98 \\\n",
    "    --save-interval-updates 100 --validate-interval-updates 100 \\\n",
    "    --keep-interval-updates 10 --no-epoch-checkpoints \\\n",
    "    --fp16 \\\n",
    "    --tensorboard-logdir tensorboard-98 \\\n",
    "    --max-epoch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-13 15:53:50 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints-98/checkpoint_best.pt', 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': {'_name': 'translation', 'data': 'pretrain-bin', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
      "2023-07-13 15:53:50 | INFO | fairseq.tasks.translation | [ja] dictionary: 31984 types\n",
      "2023-07-13 15:53:50 | INFO | fairseq.tasks.translation | [en] dictionary: 32024 types\n",
      "2023-07-13 15:53:50 | INFO | fairseq_cli.generate | loading model(s) from checkpoints-98/checkpoint_best.pt\n",
      "2023-07-13 15:53:53 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: pretrain-bin/test.ja-en.ja\n",
      "2023-07-13 15:53:53 | INFO | fairseq.data.data_utils | loaded 1,160 examples from: pretrain-bin/test.ja-en.en\n",
      "2023-07-13 15:53:53 | INFO | fairseq.tasks.translation | pretrain-bin test ja-en 1160 examples\n",
      "2023-07-13 15:53:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-07-13 15:53:54 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-07-13 15:53:54 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-07-13 15:53:54 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-07-13 15:54:12 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2023-07-13 15:54:12 | INFO | fairseq_cli.generate | Translated 1,160 sentences (36,206 tokens) in 11.8s (98.60 sentences/s, 3077.36 tokens/s)\n",
      "S-107\t小樽運転所より転入。\n",
      "T-107\tTransferred from the Otaru Switch Yard.\n",
      "H-107\t-0.9557791352272034\tIt was transferred to Otaru Operation Station.\n",
      "D-107\t-0.9557791352272034\tIt was transferred to Otaru Operation Station.\n",
      "P-107\t-2.2590 -0.5160 -0.9488 -1.1542 -0.2752 -1.5897 -1.3091 -0.4324 -0.1175\n",
      "S-636\t慶応2年(1866年\n",
      "T-636\t1866\n",
      "H-636\t-0.3702980875968933\t1866\n",
      "D-636\t-0.3702980875968933\t1866\n",
      "P-636\t-0.6773 -0.1832 -0.2504\n",
      "S-641\t慶応3年(1867年\n",
      "T-641\t1867\n",
      "H-641\t-0.38721489906311035\t1867\n",
      "D-641\t-0.38721489906311035\t1867\n",
      "P-641\t-0.7415 -0.1045 -0.3156\n",
      "S-0\tInfoboxBuddhist\n",
      "T-0\tInfobox Buddhist\n",
      "H-0\t-0.7619332075119019\tInfo box buddhist\n",
      "D-0\t-0.7619332075119019\tInfo box buddhist\n",
      "P-0\t-1.2307 -0.8714 -2.4684 -0.1117 -0.1103 -0.2528 -0.2881\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 fairseq-generate pretrain-bin --path checkpoints-98/checkpoint_best.pt --batch-size 128 --beam 5 > ./98-results/result.txt --remove-bpe=sentencepiece\n",
    "!head -20 ./98-results/result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info box buddhist\n",
      "Dogen was a Zen priest in the early Kamakura period.\n",
      "He was the founder of the Soto sect.\n",
      "In his later years, he also used the pseudonym of Kigen.\n",
      "According to the Soji of the same sect, he was revered as the founder of the sect.\n",
      "His shi (posthumous name) was Busshoden Togokushi (Buddhist priest) and Joyo Daishi (a priest).\n",
      "He is generally called Dogen Zenji.\n",
      "It is said that he spread to Japan how to clean his teeth by polishing his teeth, how to do with meals, and how to clean his teeth.\n",
      "There is a theory that he brought back Mosotik (Mosochiku) at the beginning.\n",
      "There are many unclear points about the birth of Dogen, but there are various theories that he was born in the direct line of Michichika TSUCHIMIKADO (MINAMOTO no Michichika or Michichika KOGA), who was Naidaijin (Minister of the Center).\n"
     ]
    }
   ],
   "source": [
    "!grep \"^H-\" ./98-results/result.txt | sort -V | cut -f3 > ./98-results/result.en.txt\n",
    "!head ./98-results/result.en.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(sys='./98-results/result.en.txt', ref='kftt-data-1.0/data/orig/kyoto-test.en', order=4, ignore_case=False, sacrebleu=False, sentence_bleu=False)\n",
      "BLEU4 = 16.62, 44.5/21.9/11.8/6.7 (BP=1.000, ratio=1.008, syslen=22238, reflen=22063)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys ./98-results/result.en.txt --ref kftt-data-1.0/data/orig/kyoto-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.29' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "/home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/sugihara/anaconda3/envs/NLP100_2/lib/python3.10/site-packages/tensorboard_data_server/bin/server)\n",
      "TensorBoard 2.13.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir tensorboard-98 --host=localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP100",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
